{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B Titr\" size=5>\n",
    "<p></p><p></p>\n",
    "بسمه تعالی\n",
    "<p></p>\n",
    "</font>\n",
    "<p></p>\n",
    "<font>\n",
    "<br>\n",
    "درس بازیابی پیشرفته اطلاعات\n",
    "<br>\n",
    "مدرس: دکتر سلیمانی\n",
    "</font>\n",
    "<p></p>\n",
    "<br>\n",
    "<font>\n",
    "<b>فاز سوم پروژه</b>\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "موعد تحویل:  ساعت ۶ صبح ۸ تیر<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font>\n",
    "دانشگاه صنعتی شریف\n",
    "<br>\n",
    "دانشکده مهندسی کامپیوتر\n",
    "<br>\n",
    "<br>\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <h1>\n",
    "    <b>مقدمه</b>\n",
    "    </h1>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     <br>\n",
    "    در این فاز از پروژه، تمرکز ما بر \n",
    "    crawling \n",
    "    و تحلیل مقالات استخراج‌شده از اینترنت خواهد بود. ما با بررسی تکنیک های مختلف  \n",
    "    web crawling\n",
    "    برای استخراج مقالات و سایر اطلاعات مرتبط از وب شروع خواهیم کرد.\n",
    "    <br>\n",
    "    در مرحله بعد، الگوریتم های تجزیه و تحلیل  لینک مانند\n",
    "    PageRank\n",
    "    و \n",
    "    HITS\n",
    "    را برای تعیین اهمیت این مقالات بر اساس نقل قول‌ها، ارجاعات یا اشکال دیگر پیوندها اعمال خواهیم‌کرد. ما همچنین یاد خواهیم‌گرفت که چگونه یک الگوریتم \n",
    "    PageRank\n",
    "    شخصی‌سازی‌شده را پیاده‌سازی کنیم که ترجیحات کاربر را برای ارائه نتایج مرتبط تر در نظر می‌گیرد.\n",
    "    <br>\n",
    "    در بخش سوم این مرحله، یک موتور جستجوی شخصی‌سازی شده را پیاده‌سازی میکنیم و یاد می‌گیریم که چگونه موتور جستجویی بسازیم که نتایجی را بر اساس ترجیحات کاربر ارائه دهد.\n",
    "    <br>\n",
    "در نهایت، ما یک \n",
    "    task \n",
    "     در مورد \n",
    "    recommendation system \n",
    "    ها خواهیم‌داشت، که در آن از تکنیک های مختلف برای توصیه مقالات یا صفحات وب به کاربران بر اساس ترجیحات و رفتار آنها استفاده خواهیم کرد.\n",
    "    <br>\n",
    "     تنها زبان قابل قبول برای پروژه پایتون است. محدودیت استفاده از کتاب‌خانه‌های آماده در هر بخش مشخص شده است. در انتهای پروژه قرار است یک سیستم یکپارچه‌ی جست‌و‌جو داشته باشید، بنابراین به پیاده‌سازی هر چه بهتر این فاز توجه داشته باشید.\n",
    "</font>\n",
    "</div>\n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>پیاده‌سازی Crawler (۴۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "\n",
    "<font face=\"XB Zar\" size=3>\n",
    "   در این بخش باید یک Crawler \n",
    "    برای واکشی اطلاعات تعدادی مقاله از سایت <a href=\"https://www.semanticscholar.org/\">Semantic Scholar</a> پیاده سازی کنید.\n",
    "   اطلاعات واکشی شده باید حاوی موارد زیر باشد.\n",
    "</font>\n",
    "</div>\n",
    "<br>\n",
    "<table dir=\"ltr\" style=\"width: 100%; border-collapse: collapse;\">\n",
    "  <tr>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">ID</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Title</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Abstract</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Publication Year</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Authors</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Related Topics</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Citation Count</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Reference Count</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">References</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Unique ID of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Title of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Abstract of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Publication year</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Name of the first author, ..., Name of the last author</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">topic1, topic2, ...</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">number of citations of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">number of references of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">ID of the first reference, ..., ID of the tenth reference</td>\n",
    "  </tr>\n",
    "</table>\n",
    "    <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "<font face=\"XB Zar\" size=3>\n",
    "  ابتدا فرایند واکشی را از ۵ مقاله‌ی هر استاد شروع کنید و\n",
    "    ۱۰\n",
    "    مرجع اول هر مقاله را به صف مقالات اضافه کنید.\n",
    "    فرایند واکشی را نا جایی ادامه دهید که اطلاعات ۲۰۰۰ مقاله را داشته باشید.\n",
    "    اطلاعات مقالات را در فایل crawled_paper_profName.json ذخیره کنید.\n",
    "</font>\n",
    "</div>\n",
    "\n",
    "<div dir=\"rtl\">\n",
    "\n",
    "<font face=\"XB Zar\" size=3>\n",
    "  در پیاده سازی Crawler به موارد زیر دقت کنید.\n",
    "    \n",
    "    \n",
    "<ul>\n",
    "<li>حق استفاده از api سایت semantic scholar را ندارید.</li>\n",
    "<li>برای واکشی می‌توانید از پکیج‌هایی مثل <a href=\"https://www.selenium.dev/selenium/docs/api/py/\">Selenium</a> و یا <a href=\"https://github.com/scrapy/scrapy\">Scrapy</a>  استفاده کنید. استفاده از پکیج‌های دیگر نیز مجاز است. همچنین برای پارس اطلاعات واکشی شده می‌توانید از پکیج <a href=\"https://pypi.org/project/beautifulsoup4/\">Beautiful Soup</a> استفاده کنید.\n",
    "</li>\n",
    "<li>بین هر بار درخواست از سایت یک فاصله چند ثانیه‌ای بدهید.</li>\n",
    "<li>در زمان تحویل کد Crawler شما اجرا خواهد شد و صحت آن بررسی خواهد شد.</li>\n",
    "<li>در صورتی که ‌Crawler شما به دچار اروری مثل request timeout شد نباید کار خود را متوقف کند.</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "```shell\n",
    "cd ss_crawler\n",
    "docker compose build\n",
    "docker compose up -d cache\n",
    "docker compose run crawl python main.py -i /app/data/papers/{prof_name}.txt -o /app/results/crawled_paper_{prof_name}.json\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>PageRank \n",
    "        شخصی‌سازی‌شده\n",
    "        (۲۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در این بخش، الگوریتم \n",
    "    PageRank \n",
    "    شخصی‌سازی‌شده را پیاده‌سازی می‌کنیم که توسعه‌ای از الگوریتم \n",
    "    PageRank\n",
    "    است که ترجیحات کاربر را در نظر می‌گیرد. الگوریتم \n",
    "    PageRank\n",
    "    شخصی‌سازی‌شده گره‌ها را در یک گراف بر اساس اهمیت آنها برای کاربر رتبه‌بندی می‌کند، نه بر اساس اهمیت کلی آنها در نمودار.\n",
    "    \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from tqdm import tqdm\n",
    "\n",
    "def pagerank(graph: Dict[str, List[str]], user_preferences: Dict[str, float]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Returns the personalized PageRank scores for the nodes in the graph, given the user's preferences.\n",
    "\n",
    "    Parameters:\n",
    "    graph (Dict[str, List[str]]): The graph represented as a dictionary of node IDs and their outgoing edges.\n",
    "    user_preferences (Dict[str, float]): A dictionary of node IDs and the user's preferences for those nodes.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, float]: A dictionary of node IDs and their personalized PageRank scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # Constants for the PageRank algorithm\n",
    "    damping_factor = 0.85\n",
    "    convergence_threshold = 0.0001\n",
    "    max_iterations = 100\n",
    "\n",
    "    # Initialize the PageRank scores with equal probabilities\n",
    "    num_nodes = len(graph)\n",
    "    initial_score = 1.0 / num_nodes\n",
    "    pagerank_scores = {node: initial_score for node in graph}\n",
    "\n",
    "    # Convert user preferences to personalized teleportation probabilities\n",
    "    teleportation_probs = {}\n",
    "    total_preference = sum(user_preferences.values())\n",
    "    if total_preference > 0:\n",
    "        for node, preference in user_preferences.items():\n",
    "            teleportation_probs[node] = preference / total_preference\n",
    "\n",
    "    incoming_graph = {}\n",
    "    for node, outgoing_nodes in graph.items():\n",
    "        for outgoing_node in outgoing_nodes:\n",
    "            if outgoing_node not in incoming_graph:\n",
    "                incoming_graph[outgoing_node] = []\n",
    "            incoming_graph[outgoing_node].append(node)\n",
    "\n",
    "    # Iteratively calculate the PageRank scores\n",
    "    with tqdm(range(max_iterations), total=max_iterations, desc=\"Calculating PageRank\") as pbar:\n",
    "        for _ in pbar:\n",
    "            new_scores = {}\n",
    "            for node in graph:\n",
    "                new_score = (1 - damping_factor) / num_nodes\n",
    "\n",
    "                # Consider incoming edges to calculate the new score\n",
    "                for incoming_node in incoming_graph.get(node, []):\n",
    "                    new_score += damping_factor * pagerank_scores[incoming_node] / len(graph[incoming_node])\n",
    "\n",
    "                # Apply personalized teleportation if available\n",
    "                if node in teleportation_probs:\n",
    "                    new_score += (1 - damping_factor) * teleportation_probs[node]\n",
    "\n",
    "                new_scores[node] = new_score\n",
    "\n",
    "            # Check for convergence\n",
    "            convergence = sum(abs(new_scores[node] - pagerank_scores[node]) for node in graph)\n",
    "            pbar.set_postfix({\"loss\": convergence})\n",
    "            if convergence < convergence_threshold:\n",
    "                break\n",
    "\n",
    "            pagerank_scores = new_scores\n",
    "\n",
    "    return pagerank_scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در این بخش از الگوریتم \n",
    "PageRank\n",
    "شخصی‌سازی‌شده که در قسمت قبلی پیاده‌سازی شده‌است برای\n",
    "شناسایی مقالات مهم مرتبط با حوزه‌ی کاری یک استاد \n",
    "خاص استفاده می‌کنیم. این تابع، یک \n",
    "    field \n",
    "    را به عنوان ورودی دریافت می‌کند. خروجی نیز\n",
    "مقالات برتری که بیشترین ارتباط را با آن زمینه دارند؛ خواهدبود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "professors = {\n",
    "    'Kasaei',\n",
    "    'Rabiee',\n",
    "    'Rohban',\n",
    "    'Sharifi',\n",
    "    'Soleymani'\n",
    "}\n",
    "papers_by_id = {}\n",
    "papers_by_professor = {}\n",
    "for professor in professors:\n",
    "    with open(f'results/crawled_paper_{professor}.json') as f:\n",
    "        papers_by_professor[professor] = json.load(f)\n",
    "        for paper in papers_by_professor[professor]:\n",
    "            papers_by_id[paper['id']] = paper\n",
    "\n",
    "all_papers = list(papers_by_id.values())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def important_articles(professor: str, topk: int = 10) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns the most important articles in the field of given professor, based on the personalized PageRank scores.\n",
    "\n",
    "    Parameters:\n",
    "    Professor (str): Professor's name.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of article IDs representing the most important articles in the field of given professor.\n",
    "    \"\"\"\n",
    "\n",
    "    graph = {}\n",
    "    for paper in all_papers:\n",
    "        graph[paper['id']] = paper['references']\n",
    "\n",
    "    # Define the user preferences based on the professor's seed papers\n",
    "    user_preferences = {\n",
    "        paper['id']: 1.0\n",
    "        for paper in papers_by_professor[professor]\n",
    "    }\n",
    "\n",
    "    # Calculate the personalized PageRank scores\n",
    "    pagerank_scores = pagerank(graph, user_preferences)\n",
    "\n",
    "    # Sort the articles based on their PageRank scores\n",
    "    sorted_articles = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)[:topk]\n",
    "\n",
    "    # Return only the article IDs\n",
    "    most_important_articles = [article_id for article_id, _ in sorted_articles]\n",
    "\n",
    "    return most_important_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def print_paper(id_: str):\n",
    "    paper = papers_by_id[id_]\n",
    "    width = 100\n",
    "    print(f' {paper[\"title\"]} '.center(width, '='))\n",
    "    print(f' Authors '.center(width, '-'))\n",
    "    print(f' {\", \".join(paper[\"authors\"])} '.center(width, '-'))\n",
    "    print(f' Year: {paper[\"publication_year\"]} '.center(width, '-'))\n",
    "    print(f' Topics: {\", \".join(paper[\"related_topics\"])} '.center(width, '-'))\n",
    "    print(f' {paper[\"citation_count\"]} citations '.center(width, '-'))\n",
    "    print(f' {paper[\"reference_count\"]} references '.center(width, '-'))\n",
    "    print(f' Abstract '.center(width, '-'))\n",
    "    print(paper['abstract'])\n",
    "    print('=' * width)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating PageRank:   8%|▊         | 8/100 [00:00<00:01, 57.14it/s, loss=6.29e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= ImageNet classification with deep convolutional neural networks ==================\n",
      "--------------------------------------------- Authors ----------------------------------------------\n",
      "------------------------ A. Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton -------------------------\n",
      "-------------------------------------------- Year: 2012 --------------------------------------------\n",
      "------------------------------------- Topics: Computer Science -------------------------------------\n",
      "----------------------------------------- 97952 citations ------------------------------------------\n",
      "------------------------------------------ 44 references -------------------------------------------\n",
      "--------------------------------------------- Abstract ---------------------------------------------\n",
      "A large, deep convolutional neural network was trained to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes and employed a recently developed regularization method called &quot;dropout&quot; that proved to be very effective. We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called &quot;dropout&quot; that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.\n",
      "====================================================================================================\n",
      "========================== A Fast Learning Algorithm for Deep Belief Nets ==========================\n",
      "--------------------------------------------- Authors ----------------------------------------------\n",
      "---------------------------- Geoffrey E. Hinton, Simon Osindero, Y. Teh ----------------------------\n",
      "-------------------------------------------- Year: 2006 --------------------------------------------\n",
      "------------------------------------- Topics: Computer Science -------------------------------------\n",
      "----------------------------------------- 14765 citations ------------------------------------------\n",
      "------------------------------------------ 30 references -------------------------------------------\n",
      "--------------------------------------------- Abstract ---------------------------------------------\n",
      "A fast, greedy algorithm is derived that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.\n",
      "====================================================================================================\n",
      "============================== Atomic Decomposition by Basis Pursuit ===============================\n",
      "--------------------------------------------- Authors ----------------------------------------------\n",
      "--------------------------------- S. Chen, D. Donoho, M. Saunders ----------------------------------\n",
      "-------------------------------------------- Year: 1998 --------------------------------------------\n",
      "------------------------------------- Topics: Computer Science -------------------------------------\n",
      "----------------------------------------- 11373 citations ------------------------------------------\n",
      "------------------------------------------ 50 references -------------------------------------------\n",
      "--------------------------------------------- Abstract ---------------------------------------------\n",
      "Basis Pursuit (BP) is a principle for decomposing a signal into an &quot;optimal&quot; superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions. The time-frequency and time-scale communities have recently developed a large number of overcomplete waveform dictionaries --- stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the method of frames (MOF), Matching pursuit (MP), and, for special dictionaries, the best orthogonal basis (BOB). \n",
      "Basis Pursuit (BP) is a principle for decomposing a signal into an &quot;optimal&quot; superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP, and BOB, including better sparsity and superresolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, in abstract harmonic analysis, total variation denoising, and multiscale edge denoising. \n",
      "BP in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver.\n",
      "====================================================================================================\n",
      "=================================== Generative Adversarial Nets ====================================\n",
      "--------------------------------------------- Authors ----------------------------------------------\n",
      " Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, Yoshua Bengio \n",
      "-------------------------------------------- Year: 2014 --------------------------------------------\n",
      "------------------------------------- Topics: Computer Science -------------------------------------\n",
      "----------------------------------------- 37362 citations ------------------------------------------\n",
      "------------------------------------------ 38 references -------------------------------------------\n",
      "--------------------------------------------- Abstract ---------------------------------------------\n",
      "A new framework for estimating generative models via an adversarial process, in which two models are simultaneously train: a generative model G that captures the data distribution and a discriminative model D that estimates the probability that a sample came from the training data rather than G. We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to ½ everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.\n",
      "====================================================================================================\n",
      "========= Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation =========\n",
      "--------------------------------------------- Authors ----------------------------------------------\n",
      "--------------------- Ross B. Girshick, Jeff Donahue, Trevor Darrell, J. Malik ---------------------\n",
      "-------------------------------------------- Year: 2013 --------------------------------------------\n",
      "------------------------------------- Topics: Computer Science -------------------------------------\n",
      "----------------------------------------- 21160 citations ------------------------------------------\n",
      "------------------------------------------ 56 references -------------------------------------------\n",
      "--------------------------------------------- Abstract ---------------------------------------------\n",
      "This paper proposes a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.\n",
      "====================================================================================================\n",
      "====================== Learning Multiple Layers of Features from Tiny Images =======================\n",
      "--------------------------------------------- Authors ----------------------------------------------\n",
      "------------------------------------------ A. Krizhevsky -------------------------------------------\n",
      "-------------------------------------------- Year: 2009 --------------------------------------------\n",
      "------------------------------------- Topics: Computer Science -------------------------------------\n",
      "----------------------------------------- 23315 citations ------------------------------------------\n",
      "------------------------------------------ 15 references -------------------------------------------\n",
      "--------------------------------------------- Abstract ---------------------------------------------\n",
      "It is shown how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex, using a novel parallelization algorithm to distribute the work among multiple machines connected on a network. Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it dicult to learn a good set of lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is signicantly improved by pre-training a layer of features on a large set of unlabeled tiny images.\n",
      "====================================================================================================\n",
      "======= Near-Optimal Signal Recovery From Random Projections: Universal Encoding Strategies? =======\n",
      "--------------------------------------------- Authors ----------------------------------------------\n",
      "---------------------------------------- E. Candès, T. Tao -----------------------------------------\n",
      "-------------------------------------------- Year: 2004 --------------------------------------------\n",
      "------------------------------------- Topics: Computer Science -------------------------------------\n",
      "------------------------------------------ 6835 citations ------------------------------------------\n",
      "------------------------------------------ 60 references -------------------------------------------\n",
      "--------------------------------------------- Abstract ---------------------------------------------\n",
      "If the objects of interest are sparse in a fixed basis or compressible, then it is possible to reconstruct f to within very high accuracy from a small number of random measurements by solving a simple linear program. Suppose we are given a vector f in a class FsubeRopf&lt;sup&gt;N &lt;/sup&gt;, e.g., a class of digital signals or digital images. How many linear measurements do we need to make about f to be able to recover f to within precision epsi in the Euclidean (lscr&lt;sub&gt;2&lt;/sub&gt;) metric? This paper shows that if the objects of interest are sparse in a fixed basis or compressible, then it is possible to reconstruct f to within very high accuracy from a small number of random measurements by solving a simple linear program. More precisely, suppose that the nth largest entry of the vector |f| (or of its coefficients in a fixed basis) obeys |f|&lt;sub&gt;(n)&lt;/sub&gt;lesRmiddotn&lt;sup&gt;-1&lt;/sup&gt;p/, where R&gt;0 and p&gt;0. Suppose that we take measurements y&lt;sub&gt;k&lt;/sub&gt;=langf&lt;sup&gt;# &lt;/sup&gt;,X&lt;sub&gt;k&lt;/sub&gt;rang,k=1,...,K, where the X&lt;sub&gt;k&lt;/sub&gt; are N-dimensional Gaussian vectors with independent standard normal entries. Then for each f obeying the decay estimate above for some 0&lt;p&lt;1 and with overwhelming probability, our reconstruction f&lt;sup&gt;t&lt;/sup&gt;, defined as the solution to the constraints y&lt;sub&gt;k&lt;/sub&gt;=langf&lt;sup&gt;# &lt;/sup&gt;,X&lt;sub&gt;k&lt;/sub&gt;rang with minimal lscr&lt;sub&gt;1&lt;/sub&gt; norm, obeys parf-f&lt;sup&gt;#&lt;/sup&gt;par&lt;sub&gt;lscr2&lt;/sub&gt;lesC&lt;sub&gt;p &lt;/sub&gt;middotRmiddot(K/logN)&lt;sup&gt;-r&lt;/sup&gt;, r=1/p-1/2. There is a sense in which this result is optimal; it is generally impossible to obtain a higher accuracy from any set of K measurements whatsoever. The methodology extends to various other random measurement ensembles; for example, we show that similar results hold if one observes a few randomly sampled Fourier coefficients of f. In fact, the results are quite general and require only two hypotheses on the measurement ensemble which are detailed\n",
      "====================================================================================================\n",
      "=== Optimally sparse representation in general (nonorthogonal) dictionaries via ℓ1 minimization ====\n",
      "--------------------------------------------- Authors ----------------------------------------------\n",
      "------------------------------------- D. Donoho, Michael Elad --------------------------------------\n",
      "-------------------------------------------- Year: 2003 --------------------------------------------\n",
      "------------------------------------- Topics: Computer Science -------------------------------------\n",
      "------------------------------------------ 2984 citations ------------------------------------------\n",
      "------------------------------------------ 31 references -------------------------------------------\n",
      "--------------------------------------------- Abstract ---------------------------------------------\n",
      "This article obtains parallel results in a more general setting, where the dictionary D can arise from two or several bases, frames, or even less structured systems, and sketches three applications: separating linear features from planar ones in 3D data, noncooperative multiuser encoding, and identification of over-complete independent component models. Given a dictionary D = {dk} of vectors dk, we seek to represent a signal S as a linear combination S = ∑k γ(k)dk, with scalar coefficients γ(k). In particular, we aim for the sparsest representation possible. In general, this requires a combinatorial optimization process. Previous work considered the special case where D is an overcomplete system consisting of exactly two orthobases and has shown that, under a condition of mutual incoherence of the two bases, and assuming that S has a sufficiently sparse representation, this representation is unique and can be found by solving a convex optimization problem: specifically, minimizing the ℓ1 norm of the coefficients γ̱. In this article, we obtain parallel results in a more general setting, where the dictionary D can arise from two or several bases, frames, or even less structured systems. We sketch three applications: separating linear features from planar ones in 3D data, noncooperative multiuser encoding, and identification of over-complete independent component models.\n",
      "====================================================================================================\n",
      "========= Stable recovery of sparse overcomplete representations in the presence of noise ==========\n",
      "--------------------------------------------- Authors ----------------------------------------------\n",
      "------------------------------ D. Donoho, Michael Elad, V. Temlyakov -------------------------------\n",
      "-------------------------------------------- Year: 2006 --------------------------------------------\n",
      "------------------------------------- Topics: Computer Science -------------------------------------\n",
      "------------------------------------------ 2272 citations ------------------------------------------\n",
      "------------------------------------------ 46 references -------------------------------------------\n",
      "--------------------------------------------- Abstract ---------------------------------------------\n",
      "This paper establishes the possibility of stable recovery under a combination of sufficient sparsity and favorable structure of the overcomplete system and shows that similar stability is also available using the basis and the matching pursuit algorithms. Overcomplete representations are attracting interest in signal processing theory, particularly due to their potential to generate sparse representations of signals. However, in general, the problem of finding sparse representations must be unstable in the presence of noise. This paper establishes the possibility of stable recovery under a combination of sufficient sparsity and favorable structure of the overcomplete system. Considering an ideal underlying signal that has a sufficiently sparse representation, it is assumed that only a noisy version of it can be observed. Assuming further that the overcomplete system is incoherent, it is shown that the optimally sparse approximation to the noisy data differs from the optimally sparse decomposition of the ideal noiseless signal by at most a constant multiple of the noise level. As this optimal-sparsity method requires heavy (combinatorial) computational effort, approximation algorithms are considered. It is shown that similar stability is also available using the basis and the matching pursuit algorithms. Furthermore, it is shown that these methods result in sparse approximation of the noisy data that contains only terms also appearing in the unique sparsest representation of the ideal noiseless sparse signal.\n",
      "====================================================================================================\n",
      "========================= Regression Shrinkage and Selection via the Lasso =========================\n",
      "--------------------------------------------- Authors ----------------------------------------------\n",
      "------------------------------------------ R. Tibshirani -------------------------------------------\n",
      "-------------------------------------------- Year: 1996 --------------------------------------------\n",
      "------------------------------------- Topics: Computer Science -------------------------------------\n",
      "----------------------------------------- 40956 citations ------------------------------------------\n",
      "------------------------------------------ 21 references -------------------------------------------\n",
      "--------------------------------------------- Abstract ---------------------------------------------\n",
      "A new method for estimation in linear models called the lasso, which minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant, is proposed. SUMMARY We propose a new method for estimation in linear models. The &#39;lasso&#39; minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for id_ in important_articles('Rohban'):\n",
    "    print_paper(id_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>جستجو شخصی‌سازی‌شده (۱۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "الگوریتم جست‌و‌جویی که در فازهای گذشته پیاده‌سازی کرده‌اید را به گونه‌ای تغییر دهید که نتایج به دست آمده جست‌و‌جو بر حسب علایق فرد مرتب شوند. از قضیه‌ی خطی بودن برای این کار استفاده کنید.\n",
    "    \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from functools import lru_cache\n",
    "import glob\n",
    "import math\n",
    "from termcolor import colored\n",
    "import os\n",
    "from typing import Dict, Iterable, Literal, Union, Set, Optional, List, Tuple\n",
    "\n",
    "from nltk.metrics import distance as nltkd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Token:\n",
    "    processed: str\n",
    "    actual: str\n",
    "    i: int\n",
    "    idx: int\n",
    "\n",
    "    @staticmethod\n",
    "    def from_spacy_token(token) -> \"Token\":\n",
    "        return Token(token.lemma_.lower(), token.text, token.i, token.idx)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.processed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def batch_clean_data(texts: List[str], batch_size: int = 128) -> List[List[Token]]:\n",
    "    \"\"\"Preprocesses the text with tokenization, case folding, stemming and lemmatization, and punctuations\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : List[str]\n",
    "        A list of titles or abstracts of articles\n",
    "    batch_size : int, optional\n",
    "        The number of texts to be processed at a time, by default 128\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[List[Doc]]\n",
    "        A list of lists of tokens\n",
    "    \"\"\"\n",
    "    tokens = nlp.pipe(texts, batch_size=batch_size, n_process=os.cpu_count())\n",
    "    return [[Token.from_spacy_token(token) for token in doc if not token.is_punct] for doc in tokens]\n",
    "\n",
    "\n",
    "def clean_data(text: str) -> List[Token]:\n",
    "    \"\"\"Preprocesses the text with tokenization, case folding, stemming and lemmatization, and punctuations\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The title or abstract of an article\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of tokens\n",
    "    \"\"\"\n",
    "    return batch_clean_data([text])[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def find_stop_words(all_text: List[str], num_token: int = 30) -> Set[str]:\n",
    "    \"\"\"Detects stop-words\n",
    "\n",
    "     Parameters\n",
    "    ----------\n",
    "    all_text : list of all tokens\n",
    "        (result of clean_data(text) for all the text)\n",
    "\n",
    "    num_token : int\n",
    "        number of stop words to be detected\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Return Value is optional but must print the stop words and number of their occurence\n",
    "    \"\"\"\n",
    "    counter = Counter(all_text)\n",
    "    most_occur = counter.most_common(num_token)\n",
    "    print(pd.DataFrame(most_occur, columns=['token', 'count']))\n",
    "    return set([token for token, _ in most_occur])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "\n",
    "    def __init__(self, papers_by_professor: Dict[str, List[dict]],\n",
    "                 all_papers: List[dict], stop_topk: int = 30):\n",
    "        self.papers_by_professor = {\n",
    "            professor: [paper['id'] for paper in papers]\n",
    "            for professor, papers in papers_by_professor.items()\n",
    "        }\n",
    "        self.data = pd.DataFrame(all_papers)\n",
    "        self.stop_topk = stop_topk\n",
    "\n",
    "    @property\n",
    "    @lru_cache\n",
    "    def cleaned_documents(self) -> Dict[str, Dict[str, List[Token]]]:\n",
    "        return {\n",
    "            paper_id: {\n",
    "                'title': cleaned_title,\n",
    "                'abstract': cleaned_abstract,\n",
    "            }\n",
    "            for paper_id, cleaned_title, cleaned_abstract in zip(\n",
    "                self.data['id'].tolist(),\n",
    "                batch_clean_data(self.data['title'].tolist()),\n",
    "                batch_clean_data(self.data['abstract'].tolist()),\n",
    "            )\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    @lru_cache\n",
    "    def stop_tokens(self) -> Set[str]:\n",
    "        return find_stop_words(\n",
    "            [token.processed\n",
    "             for tokens in self.cleaned_documents.values()\n",
    "             for token in tokens['title'] + tokens['abstract']],\n",
    "            num_token=self.stop_topk,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    @lru_cache\n",
    "    def non_stop_documents(self) -> Dict[str, Dict[str, List[Token]]]:\n",
    "        return {\n",
    "            paper_id: {\n",
    "                'title': [token for token in tokens['title'] if token.processed not in self.stop_tokens],\n",
    "                'abstract': [token for token in tokens['abstract'] if token.processed not in self.stop_tokens],\n",
    "            }\n",
    "            for paper_id, tokens in self.cleaned_documents.items()\n",
    "        }\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class TrieNode:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.children: Dict[str, TrieNode] = {}\n",
    "        self.idx_in_doc: Dict[str, Dict[Literal['title', 'abstract'], Set[int]]] = {}\n",
    "        self.is_end = False\n",
    "\n",
    "    def insert(self, doc_id: str, doc_section: Literal['title', 'abstract'], token: Token):\n",
    "        node = self\n",
    "        for char in token.processed:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "            node = node.children[char]\n",
    "            if doc_id not in node.idx_in_doc:\n",
    "                node.idx_in_doc[doc_id] = {'title': set(), 'abstract': set()}\n",
    "            node.idx_in_doc[doc_id][doc_section].add(token.idx)\n",
    "        node.is_end = True\n",
    "\n",
    "    def search(self, token: str) -> Optional[\n",
    "        Dict[str, Dict[Literal['title', 'abstract'], Set[int]]]]:\n",
    "        node = self\n",
    "        for char in token:\n",
    "            if char not in node.children:\n",
    "                return None\n",
    "            node = node.children[char]\n",
    "        if node.is_end:\n",
    "            return node.idx_in_doc\n",
    "        return None\n",
    "\n",
    "    def remove_document(self, doc_id: str):\n",
    "        if doc_id in self.idx_in_doc:\n",
    "            del self.idx_in_doc[doc_id]\n",
    "        for child in self.children.values():\n",
    "            child.remove_document(doc_id)\n",
    "\n",
    "    def traverse_words(self, prefix: str = '') -> Iterable[Tuple[str,\n",
    "    Dict[str, Dict[Literal['title', 'abstract'], Set[int]]]]]:\n",
    "        if self.is_end:\n",
    "            yield prefix, {\n",
    "                doc_id: {\n",
    "                    k: list(v) for k, v in doc.items()\n",
    "                }\n",
    "                for doc_id, doc in self.idx_in_doc.items()\n",
    "            }\n",
    "\n",
    "        for char, child in self.children.items():\n",
    "            yield from child.traverse_words(prefix + char)\n",
    "\n",
    "    @classmethod\n",
    "    def from_words(cls, words: Iterable[Tuple[str,\n",
    "    Dict[str, Dict[Literal['title', 'abstract'], Iterable[int]]]]]) -> 'TrieNode':\n",
    "        root = cls()\n",
    "        for word, idx_in_doc in words:\n",
    "            for doc_id, doc in idx_in_doc.items():\n",
    "                for doc_section, indices in doc.items():\n",
    "                    for idx in indices:\n",
    "                        token = Token(word, '', -1, idx)\n",
    "                        root.insert(doc_id, doc_section, token)\n",
    "        return root\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'children': {char: child.to_dict() for char, child in self.children.items()},\n",
    "            'idx_in_doc': self.idx_in_doc,\n",
    "            'is_end': self.is_end,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, data: Dict):\n",
    "        node = cls()\n",
    "        node.children = {char: cls.from_dict(child) for char, child in data['children'].items()}\n",
    "        node.i_in_doc = data['i_in_doc']\n",
    "        node.is_end = data['is_end']\n",
    "        return node"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "WORD_BOUNDARY_CHAR = '¶'\n",
    "\n",
    "\n",
    "def get_word_bigrams(word: str) -> Iterable[str]:\n",
    "    \"\"\"\n",
    "    Returns the bigrams of the given word\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    word: str\n",
    "        The word to get the bigrams from\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of bigrams\n",
    "    \"\"\"\n",
    "    word = WORD_BOUNDARY_CHAR + word + WORD_BOUNDARY_CHAR\n",
    "    return [word[i:i + 2] for i in range(len(word) - 1)]\n",
    "\n",
    "\n",
    "def create_bigram_index(corpus: Corpus) -> Dict[str, Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Creates a bigram index for the spell correction\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: Corpus\n",
    "        The corpus to generate the bigram index from\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary of bigrams and their occurence\n",
    "    \"\"\"\n",
    "    bigram_index: Dict[str, Dict[str, int]] = {}\n",
    "    seen_words = set()\n",
    "    for doc_id, doc in corpus.cleaned_documents.items():\n",
    "        for section_name, doc_section in doc.items():\n",
    "            for token in doc_section:\n",
    "                if token.actual in seen_words:\n",
    "                    continue\n",
    "                seen_words.add(token.actual)\n",
    "                for bigram in get_word_bigrams(token.actual):\n",
    "                    if bigram not in bigram_index:\n",
    "                        bigram_index[bigram] = {}\n",
    "                    if token.actual not in bigram_index[bigram]:\n",
    "                        bigram_index[bigram][token.actual] = 0\n",
    "                    bigram_index[bigram][token.actual] += 1\n",
    "    return bigram_index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def correct_text(bigram_index: Dict[str, Dict[str, int]], text: str, similar_words_limit: int = 20) -> str:\n",
    "    \"\"\"\n",
    "    Correct the give query text, if it is misspelled\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    bigram_index: Dict[str, Dict[str, int]]\n",
    "        The bigram index to search in\n",
    "    text: str\n",
    "        The query text\n",
    "    similar_words_limit: int\n",
    "        The number of similar words\n",
    "\n",
    "    Returns\n",
    "    ---------\n",
    "    str\n",
    "        The corrected form of the given text\n",
    "    \"\"\"\n",
    "    corrected_text = ''.join(text)\n",
    "    for token in nlp(text):\n",
    "        word = token.text\n",
    "        if token.is_punct:\n",
    "            continue\n",
    "        word_occurences: Dict[str, int] = {}\n",
    "        for bigram in get_word_bigrams(word):\n",
    "            for posting, occurence in bigram_index.get(bigram, {}).items():\n",
    "                if posting not in word_occurences:\n",
    "                    word_occurences[posting] = 0\n",
    "                word_occurences[posting] += occurence\n",
    "        jaccard_scores = {\n",
    "            posting: word_occurence / (len(word) + len(posting) + 2 - word_occurence)\n",
    "            for posting, word_occurence in word_occurences.items()\n",
    "        }\n",
    "        similar_words = sorted(jaccard_scores, key=jaccard_scores.get, reverse=True)[:similar_words_limit]\n",
    "        min_edit_distance = float('inf')\n",
    "        corrected_word = word\n",
    "        for similar_word in similar_words:\n",
    "            if (edit_distance := nltkd.edit_distance(similar_word, word)) < min_edit_distance:\n",
    "                min_edit_distance = edit_distance\n",
    "                corrected_word = similar_word\n",
    "        corrected_text = corrected_text.replace(word, corrected_word)\n",
    "    return corrected_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def construct_positional_indexes(corpus: Corpus):\n",
    "    \"\"\"\n",
    "    Get processed data and insert words in that into a trie and construct postional_index and posting lists after wards.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: Corpus\n",
    "        processed data\n",
    "\n",
    "    Return\n",
    "    ----------\n",
    "    docs:\n",
    "        list of docs with specificied id, title, abstract.\n",
    "    \"\"\"\n",
    "    trie = TrieNode()\n",
    "    for doc_id, doc in corpus.non_stop_documents.items():\n",
    "        for doc_section, tokens in doc.items():\n",
    "            for token in tokens:\n",
    "                trie.insert(doc_id, doc_section, token)\n",
    "    return trie"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def write_json(o, path: str):\n",
    "    import json\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(o, f)\n",
    "\n",
    "\n",
    "def get_offset(gap): return bin(gap)[3:]\n",
    "\n",
    "\n",
    "def get_length(offset): return unary_codification(len(offset)) + '0'\n",
    "\n",
    "\n",
    "def unary_codification(gap): return '1' * gap\n",
    "\n",
    "\n",
    "def get_gaps_list(posting_lists): return [posting_lists[0]] + [posting_lists[i] - posting_lists[i - 1] for i in\n",
    "                                                               range(1, len(posting_lists))]\n",
    "\n",
    "\n",
    "def gamma_encoding(postings):\n",
    "    if not postings:\n",
    "        return ''\n",
    "    postings = [p + 1 for p in postings]\n",
    "    return ''.join([get_length(get_offset(gap)) + get_offset(gap) for gap in get_gaps_list(postings)])\n",
    "\n",
    "\n",
    "def write_gamma_code(words: Iterable[Tuple[str, Dict[str, Dict[Literal['title', 'abstract'], Iterable[int]]]]], path: str):\n",
    "    with open(path, 'wb') as f:\n",
    "        for word, posting_lists in words:\n",
    "            encoded_word = word.encode('utf-8')\n",
    "            f.write(len(encoded_word).to_bytes(1, 'big'))\n",
    "            f.write(encoded_word)\n",
    "            f.write(len(posting_lists).to_bytes(4, 'big'))\n",
    "            for doc_id, doc in posting_lists.items():\n",
    "                encoded_doc_id = doc_id.encode('utf-8')\n",
    "                f.write(len(encoded_doc_id).to_bytes(1, 'big'))\n",
    "                f.write(encoded_doc_id)\n",
    "                title_postings = sorted(doc['title'])\n",
    "                encoded_title_postings = gamma_encoding(title_postings)\n",
    "                encoding_length = len(encoded_title_postings)\n",
    "                octet_encoded_title_postings = '0b' + \\\n",
    "                                               '0' * (8 - len(encoded_title_postings) % 8) + \\\n",
    "                                               encoded_title_postings\n",
    "                n_bytes = len(octet_encoded_title_postings) // 8\n",
    "                f.write(n_bytes.to_bytes(1, 'big'))\n",
    "                f.write(encoding_length.to_bytes(2, 'big'))\n",
    "                f.write(int(octet_encoded_title_postings, 2).to_bytes(n_bytes, 'big'))\n",
    "\n",
    "                encoded_abstract_postings = gamma_encoding(sorted(doc['abstract']))\n",
    "                encoding_length = len(encoded_abstract_postings)\n",
    "                octet_encoded_abstract_postings = '0b' + \\\n",
    "                                                  '0' * (8 - len(encoded_abstract_postings) % 8) + \\\n",
    "                                                  encoded_abstract_postings\n",
    "                n_bytes = len(octet_encoded_abstract_postings) // 8\n",
    "                f.write(n_bytes.to_bytes(1, 'big'))\n",
    "                f.write(encoding_length.to_bytes(2, 'big'))\n",
    "                f.write(int(octet_encoded_abstract_postings, 2).to_bytes(n_bytes, 'big'))\n",
    "\n",
    "\n",
    "def vb_encoding_number(n):\n",
    "    b = []\n",
    "    while True:\n",
    "        b = [n % 128] + b\n",
    "        n = n // 128\n",
    "        if n == 0:\n",
    "            break\n",
    "    b[-1] += 128\n",
    "    return bytes(b)\n",
    "\n",
    "\n",
    "def vb_encoding(numbers):\n",
    "    return b''.join([vb_encoding_number(n) for n in numbers])\n",
    "\n",
    "\n",
    "def write_vb_code(words: Iterable[Tuple[str, Dict[str, Dict[Literal['title', 'abstract'], Iterable[int]]]]], path: str):\n",
    "    with open(path, 'wb') as f:\n",
    "        for word, posting_lists in words:\n",
    "            encoded_word = word.encode('utf-8')\n",
    "            f.write(len(encoded_word).to_bytes(1, 'big'))\n",
    "            f.write(encoded_word)\n",
    "            f.write(len(posting_lists).to_bytes(4, 'big'))\n",
    "            for doc_id, doc in posting_lists.items():\n",
    "                encoded_doc_id = doc_id.encode('utf-8')\n",
    "                f.write(len(encoded_doc_id).to_bytes(1, 'big'))\n",
    "                f.write(encoded_doc_id)\n",
    "                title_postings = doc['title']\n",
    "                encoded_title_postings = vb_encoding(title_postings)\n",
    "                encoding_length = len(encoded_title_postings)\n",
    "                f.write(encoding_length.to_bytes(4, 'big'))\n",
    "                f.write(encoded_title_postings)\n",
    "\n",
    "                abstract_postings = doc['abstract']\n",
    "                encoded_abstract_postings = vb_encoding(abstract_postings)\n",
    "                encoding_length = len(encoded_abstract_postings)\n",
    "                f.write(encoding_length.to_bytes(4, 'big'))\n",
    "                f.write(encoded_abstract_postings)\n",
    "\n",
    "\n",
    "def store_index(trie: TrieNode, path: str, compression_type: str) -> int:\n",
    "    \"\"\"Stores the index in a file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trie: TrieNode\n",
    "        trie of corpus\n",
    "    path : str\n",
    "        Path to store the file\n",
    "    compression_type : str\n",
    "        Could be one of the followings:\n",
    "        - no-compression\n",
    "        - gamma-code\n",
    "        - variable-byte\n",
    "\n",
    "    Return\n",
    "    ----------\n",
    "    int\n",
    "        The size of the stored file\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    word_list = trie.traverse_words()\n",
    "    if compression_type == \"no-compression\":\n",
    "        write_json(list(word_list), path)\n",
    "    elif compression_type == 'gamma-code':\n",
    "        write_gamma_code(word_list, path)\n",
    "    elif compression_type == 'variable-byte':\n",
    "        write_vb_code(word_list, path)\n",
    "    else:\n",
    "        raise ValueError(\"compression_type should be one of the followings: no-compression, gamma-code, variable-byte\")\n",
    "    return os.stat(path).st_size\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def unary_decodification(gap): return sum(map(int, gap))\n",
    "\n",
    "\n",
    "def gamma_decoding(gamma):\n",
    "    num, length, offset, aux, res = 0, \"\", \"\", 0, []\n",
    "    while gamma != \"\":\n",
    "        aux = gamma.find(\"0\")\n",
    "        length = gamma[:aux]\n",
    "        if length == \"\":\n",
    "            res.append(1); gamma = gamma[1:]\n",
    "        else:\n",
    "            offset = \"1\" + gamma[aux + 1:aux + 1 + unary_decodification(length)]\n",
    "            res.append(int(offset, 2))\n",
    "            gamma = gamma[aux + 1 + unary_decodification(length):]\n",
    "    return [int(p) - 1 for p in np.cumsum(res)]\n",
    "\n",
    "\n",
    "def read_gamma_code(path: str) -> Iterable[Tuple[str, Dict[str, Dict[Literal['title', 'abstract'], Iterable[int]]]]]:\n",
    "    with open(path, 'rb') as f:\n",
    "        while True:\n",
    "            word_length = f.read(1)\n",
    "            if not word_length:\n",
    "                break\n",
    "            word_length = int.from_bytes(word_length, 'big')\n",
    "            word = f.read(word_length).decode('utf-8')\n",
    "            n_posting_lists = int.from_bytes(f.read(4), 'big')\n",
    "            posting_lists = {}\n",
    "            for _ in range(n_posting_lists):\n",
    "                doc_id_length = int.from_bytes(f.read(1), 'big')\n",
    "                doc_id = f.read(doc_id_length).decode('utf-8')\n",
    "                title_postings_n_bytes = int.from_bytes(f.read(1), 'big')\n",
    "                title_posting_length = int.from_bytes(f.read(2), 'big')\n",
    "                title_postings_int = int.from_bytes(f.read(title_postings_n_bytes), 'big')\n",
    "                title_postings_bin = bin(title_postings_int)\n",
    "                if title_posting_length == 0:\n",
    "                    title_postings = []\n",
    "                else:\n",
    "                    aligned_title_postings_bin = '0' * (title_posting_length - len(title_postings_bin[2:])) + title_postings_bin[2:]\n",
    "                    title_postings = gamma_decoding(aligned_title_postings_bin)\n",
    "\n",
    "                abstract_postings_n_bytes = int.from_bytes(f.read(1), 'big')\n",
    "                abstract_posting_length = int.from_bytes(f.read(2), 'big')\n",
    "                abstract_postings_int = int.from_bytes(f.read(abstract_postings_n_bytes), 'big')\n",
    "                abstract_postings_bin = bin(abstract_postings_int)\n",
    "                if abstract_posting_length == 0:\n",
    "                    abstract_postings = []\n",
    "                else:\n",
    "                    aligned_abstract_postings_bin = '0' * (abstract_posting_length - len(abstract_postings_bin[2:])) + abstract_postings_bin[2:]\n",
    "                    abstract_postings = gamma_decoding(aligned_abstract_postings_bin)\n",
    "\n",
    "                posting_lists[doc_id] = {\n",
    "                    'title': title_postings,\n",
    "                    'abstract': abstract_postings,\n",
    "                }\n",
    "            yield word, posting_lists\n",
    "\n",
    "\n",
    "def vb_decoding(encoded: bytes):\n",
    "    numbers = []\n",
    "    n = 0\n",
    "    for b in encoded:\n",
    "        n = n * 128 + (b % 128)\n",
    "        if b >= 128:\n",
    "            numbers.append(n)\n",
    "            n = 0\n",
    "    return numbers\n",
    "\n",
    "\n",
    "def read_vb_code(path: str) -> Iterable[Tuple[str, Dict[str, Dict[Literal['title', 'abstract'], Iterable[int]]]]]:\n",
    "    with open(path, 'rb') as f:\n",
    "        while True:\n",
    "            word_length = f.read(1)\n",
    "            if not word_length:\n",
    "                break\n",
    "            word_length = int.from_bytes(word_length, 'big')\n",
    "            word = f.read(word_length).decode('utf-8')\n",
    "            n_posting_lists = int.from_bytes(f.read(4), 'big')\n",
    "            posting_lists = {}\n",
    "            for _ in range(n_posting_lists):\n",
    "                doc_id_length = int.from_bytes(f.read(1), 'big')\n",
    "                doc_id = f.read(doc_id_length).decode('utf-8')\n",
    "                title_postings_n_bytes = int.from_bytes(f.read(4), 'big')\n",
    "                title_postings = vb_decoding(f.read(title_postings_n_bytes))\n",
    "\n",
    "                abstract_postings_n_bytes = int.from_bytes(f.read(4), 'big')\n",
    "                abstract_postings = vb_decoding(f.read(abstract_postings_n_bytes))\n",
    "\n",
    "                posting_lists[doc_id] = {\n",
    "                    'title': title_postings,\n",
    "                    'abstract': abstract_postings,\n",
    "                }\n",
    "            yield word, posting_lists\n",
    "\n",
    "\n",
    "def load_index(path: str, compression_type: str) -> TrieNode:\n",
    "    \"\"\"Loads the index from a file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path of the file to load from\n",
    "\n",
    "    compression_type : str\n",
    "        Could be one of the followings:\n",
    "        - no-compression\n",
    "        - gamma-code\n",
    "        - variable-byte\n",
    "    \"\"\"\n",
    "    if compression_type == 'no-compression':\n",
    "        import json\n",
    "        with open(path, 'r') as f:\n",
    "            word_list = json.load(f)\n",
    "        return TrieNode.from_words(word_list)\n",
    "    if compression_type == 'gamma-code':\n",
    "        return TrieNode.from_words(read_gamma_code(path))\n",
    "    if compression_type == 'variable-byte':\n",
    "        return TrieNode.from_words(read_vb_code(path))\n",
    "    else:\n",
    "        raise ValueError(\"compression_type should be one of the followings: no-compression, gamma-code, variable-byte\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def logarithmic_w_tf(tf: Dict[str, float]) -> Dict[str, float]:\n",
    "    return {\n",
    "        token: 1 + math.log(token_tf) if token_tf > 0 else 0\n",
    "        for token, token_tf in tf.items()\n",
    "    }\n",
    "\n",
    "\n",
    "def cosine_normalization(w: Iterable[float]) -> float:\n",
    "    return math.sqrt(sum(w_i ** 2 for w_i in w))\n",
    "\n",
    "\n",
    "def get_df(token_search_results: Dict[str, Dict[str, Dict[Literal['title', 'abstract'], List[int]]]]) -> Dict[str, int]:\n",
    "    return {\n",
    "        token: len(search_results)\n",
    "        for token, search_results in token_search_results.items()\n",
    "    }\n",
    "\n",
    "\n",
    "def query_tf_idf(query_tokens: List[str],\n",
    "                 token_search_results: Dict[str, Dict[str, Dict[Literal['title', 'abstract'], List[int]]]],\n",
    "                 score_type: str) -> Dict[str, float]:\n",
    "    tf = {\n",
    "        token: query_tokens.count(token)\n",
    "        for token in token_search_results\n",
    "    }\n",
    "    if score_type[0] == 'n':\n",
    "        w_tf = tf\n",
    "    elif score_type[0] == 'l':\n",
    "        w_tf = logarithmic_w_tf(tf)\n",
    "    else:\n",
    "        raise ValueError(f'tf method {score_type[0]} not supported')\n",
    "\n",
    "    if score_type[1] == 'n':\n",
    "        w_idf = {token: 1 for token in token_search_results}\n",
    "    else:\n",
    "        raise ValueError(f'idf method {score_type[1]} not supported')\n",
    "\n",
    "    if score_type[2] == 'n':\n",
    "        normalization = 1\n",
    "    elif score_type[2] == 'c':\n",
    "        normalization = cosine_normalization(w_tf.values())\n",
    "    else:\n",
    "        raise ValueError(f'normalization method {score_type[1]} not supported')\n",
    "\n",
    "    w = {\n",
    "        token: w_tf[token] * w_idf[token] / normalization\n",
    "        for token in token_search_results\n",
    "    }\n",
    "    return w\n",
    "\n",
    "\n",
    "def doc_tf_idf(corpus: Corpus,\n",
    "               doc_section: str,\n",
    "               token_search_results: Dict[str, Dict[str, Dict[Literal['title', 'abstract'], List[int]]]],\n",
    "               query_w: Dict[str, float],\n",
    "               score_type: str):\n",
    "    if score_type[1] == 't':\n",
    "        df = get_df(token_search_results)\n",
    "        w_idf = {\n",
    "            token: math.log(len(corpus) / token_df)\n",
    "            for token, token_df in df.items()\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f'idf method {score_type[1]} not supported')\n",
    "\n",
    "    all_documents = set()\n",
    "    for search_results in token_search_results.values():\n",
    "        all_documents.update(search_results.keys())\n",
    "\n",
    "    doc_scores = {}\n",
    "    for doc_id in all_documents:\n",
    "        doc_tf = {\n",
    "            token: len(search_results[doc_id][doc_section])\n",
    "            for token, search_results in token_search_results.items()\n",
    "            if doc_id in search_results\n",
    "        }\n",
    "        if score_type[0] == 'l':\n",
    "            doc_w_tf = logarithmic_w_tf(doc_tf)\n",
    "        else:\n",
    "            raise ValueError(f'tf method {score_type[0]} not supported')\n",
    "\n",
    "        if score_type[2] == 'n':\n",
    "            doc_normalization = 1\n",
    "        elif score_type[2] == 'c':\n",
    "            doc_normalization = cosine_normalization(doc_w_tf.values())\n",
    "        else:\n",
    "            raise ValueError(f'normalization method {score_type[1]} not supported')\n",
    "\n",
    "        doc_w = {\n",
    "            token: doc_w_tf[token] * w_idf[token] / doc_normalization\n",
    "            for token in doc_w_tf\n",
    "            if doc_normalization > 0\n",
    "        }\n",
    "        doc_scores[doc_id] = sum(\n",
    "            query_w[token] * doc_w[token]\n",
    "            for token in query_w\n",
    "            if token in doc_w\n",
    "        )\n",
    "    return doc_scores\n",
    "\n",
    "\n",
    "def tf_idf(corpus: Corpus, doc_section: Literal['title', 'abstract'],\n",
    "           token_search_results: Dict[str, Dict[str, Dict[Literal['title', 'abstract'], List[int]]]],\n",
    "           query_tokens: List[str], score_type: str) -> Dict[str, float]:\n",
    "    doc_score_type, query_score_type = score_type.split('-')\n",
    "    query_w = query_tf_idf(query_tokens, token_search_results, query_score_type)\n",
    "    return doc_tf_idf(corpus, doc_section, token_search_results, query_w, doc_score_type)\n",
    "\n",
    "\n",
    "def okapi25(corpus: Corpus, doc_section: Literal['title', 'abstract'],\n",
    "            token_search_results: Dict[str, Dict[str, Dict[Literal['title', 'abstract'], List[int]]]],\n",
    "            query_tokens: List[str], k1: float = 1.2, b: float = 0.75) -> Dict[str, float]:\n",
    "    all_documents = set()\n",
    "    for search_results in token_search_results.values():\n",
    "        all_documents.update(search_results.keys())\n",
    "\n",
    "    df = get_df(token_search_results)\n",
    "    idf = {\n",
    "        token: math.log((len(all_documents) - token_df + 0.5) / (token_df + 0.5) + 1)\n",
    "        for token, token_df in df.items()\n",
    "    }\n",
    "    f = {\n",
    "        token: {\n",
    "            doc_id: len(doc[doc_section])\n",
    "            for doc_id, doc in search_results.items()\n",
    "        }\n",
    "        for token, search_results in token_search_results.items()\n",
    "    }\n",
    "    dl = {\n",
    "        doc_id: len(corpus.non_stop_documents[doc_id][doc_section])\n",
    "        for doc_id in all_documents\n",
    "    }\n",
    "    avgdl = sum(\n",
    "        len(corpus.non_stop_documents[doc_id][doc_section])\n",
    "        for doc_id in all_documents\n",
    "    ) / len(all_documents)\n",
    "\n",
    "    doc_scores = {}\n",
    "    for doc_id in all_documents:\n",
    "        doc_scores[doc_id] = sum(\n",
    "            idf[token] * (\n",
    "                    f[token].get(doc_id, 0.0) * (k1 + 1)\n",
    "            ) / (\n",
    "                    f[token].get(doc_id, 0.0) + k1 * (1 - b + b * dl[doc_id] / avgdl)\n",
    "            )\n",
    "            for token in query_tokens\n",
    "        )\n",
    "    return doc_scores\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SearchResult:\n",
    "    doc_id: str\n",
    "    score: float\n",
    "    search_score: float\n",
    "    pagerank_score: float\n",
    "    title: str\n",
    "    abstract: str\n",
    "    related_topics: List[str]\n",
    "\n",
    "\n",
    "def highlight_text(text: str, highlight_starts: List[int]) -> str:\n",
    "    result = ''\n",
    "    last_index = 0\n",
    "    highlight_starts = sorted(highlight_starts)\n",
    "    for i, highlight_start in enumerate(highlight_starts):\n",
    "        next_start = highlight_starts[i + 1] if i + 1 < len(highlight_starts) else len(text)\n",
    "        if highlight_start == next_start:\n",
    "            print(highlight_start, next_start, text, colored(text[highlight_start:], 'yellow'), highlight_starts)\n",
    "        token = nlp(text[highlight_start:next_start])[0]\n",
    "        highlight_length = len(token)\n",
    "        result += text[last_index:highlight_start] + \\\n",
    "                  colored(text[highlight_start:highlight_start + highlight_length], 'red')\n",
    "        last_index = highlight_start + highlight_length\n",
    "    result += text[last_index:]\n",
    "    return result\n",
    "\n",
    "\n",
    "def search(corpus: Corpus,\n",
    "           trie: TrieNode,\n",
    "           bigram_index: Dict[str, Dict[str, int]],\n",
    "           query: str, max_result_count: int,\n",
    "           method: str = 'ltn-lnn',\n",
    "           weight: float = 0.5,\n",
    "           highlight: bool = False,\n",
    "           print_result: bool = False,\n",
    "           personalization_weight: float = 0.5,\n",
    "           preference_by_professor: Dict[str, float] = None) -> List[SearchResult]:\n",
    "    \"\"\"\n",
    "        Finds relevant documents to query\n",
    "\n",
    "        Parameters\n",
    "        ---------------------------------------------------------------------------------------------------\n",
    "        corpus: Corpus\n",
    "            The corpus\n",
    "        trie: TrieNode\n",
    "            The trie\n",
    "        bigram_index: Dict[str, Dict[str, int]]\n",
    "            The bigram index\n",
    "        query: str\n",
    "            The query string\n",
    "        max_result_count: int\n",
    "            Return top 'max_result_count' docs which have the highest scores.\n",
    "            notice that if max_result_count = -1, then you have to return all docs\n",
    "        method: 'ltn-lnn' or 'ltc-lnc' or 'okapi25'\n",
    "        weight: float\n",
    "            weight of abstract score\n",
    "        highlight: bool\n",
    "            If True, highlight the query tokens in search results\n",
    "        print_result: bool\n",
    "            If True, print the results in a readable format\n",
    "        preference_by_professor: Dict[str, float]\n",
    "            A dictionary of professor names and their preference scores\n",
    "\n",
    "        Returns\n",
    "        ----------------------------------------------------------------------------------------------------\n",
    "        list\n",
    "            Retrieved documents with snippet\n",
    "    \"\"\"\n",
    "    corrected_query = correct_text(bigram_index, query)\n",
    "    if print_result:\n",
    "        print(f'Query'.center(100, '-'))\n",
    "        print(corrected_query.center(100, '-'))\n",
    "        print('-' * 100)\n",
    "    query_tokens = [token.processed\n",
    "                    for token in clean_data(corrected_query)\n",
    "                    if token.processed not in corpus.stop_tokens]\n",
    "    token_search_results: Dict[str, Dict[str, Dict[Literal['title', 'abstract'], List[int]]]] = {\n",
    "        token: trie.search(token) or {}\n",
    "        for token in query_tokens\n",
    "    }\n",
    "    if '-' in method:\n",
    "        title_doc_scores = tf_idf(corpus, 'title', token_search_results, query_tokens, method)\n",
    "        abstract_doc_scores = tf_idf(corpus, 'abstract', token_search_results, query_tokens, method)\n",
    "    elif method == 'okapi25':\n",
    "        title_doc_scores = okapi25(corpus, 'title', token_search_results, query_tokens)\n",
    "        abstract_doc_scores = okapi25(corpus, 'abstract', token_search_results, query_tokens)\n",
    "    else:\n",
    "        raise ValueError(f'Expected the method to be one of \\'ltn-lnn\\', \\'ltc-lnc\\', or \\'okapi25\\', bot got {method}')\n",
    "    doc_scores = {\n",
    "        doc_id: weight * abstract_doc_scores.get(doc_id, 0.0) + (1 - weight) * title_doc_scores.get(doc_id, 0.0)\n",
    "        for doc_id in set(title_doc_scores.keys()) | set(abstract_doc_scores.keys())\n",
    "    }\n",
    "\n",
    "    graph = {}\n",
    "    for _, paper in corpus.data.iterrows():\n",
    "        graph[paper['id']] = paper['references']\n",
    "\n",
    "    # Define the user preferences based on the professor's seed papers\n",
    "    user_preferences = defaultdict(float)\n",
    "    for professor, preference in preference_by_professor.items():\n",
    "        for paper_id in corpus.papers_by_professor[professor]:\n",
    "            user_preferences[paper_id] += preference\n",
    "\n",
    "    # Calculate the personalized PageRank scores\n",
    "    pagerank_scores = pagerank(graph, user_preferences)\n",
    "\n",
    "    # normalize tf-idf scores\n",
    "    doc_scores_max = max(doc_scores.values())\n",
    "    normalized_doc_scores = {\n",
    "        doc_id: doc_score / doc_scores_max\n",
    "        for doc_id, doc_score in doc_scores.items()\n",
    "    }\n",
    "\n",
    "    # normalize pagerank scores\n",
    "    pagerank_scores_max = max(pagerank_scores.values())\n",
    "    normalized_pagerank_scores = {\n",
    "        doc_id: pagerank_score / pagerank_scores_max\n",
    "        for doc_id, pagerank_score in pagerank_scores.items()\n",
    "    }\n",
    "\n",
    "    # combine the two scores\n",
    "    combined_scores = {\n",
    "        doc_id: normalized_doc_scores[doc_id] * (1 - personalization_weight) + \\\n",
    "                normalized_pagerank_scores[doc_id] * personalization_weight\n",
    "        for doc_id in normalized_doc_scores\n",
    "    }\n",
    "\n",
    "    if highlight:\n",
    "        doc_highlights = {\n",
    "            doc_id: {\n",
    "                'title': sum([\n",
    "                    list(token_search_results[token][doc_id]['title'])\n",
    "                    for token in query_tokens\n",
    "                    if token in token_search_results and doc_id in token_search_results[token]\n",
    "                ], []),\n",
    "                'abstract': sum([\n",
    "                    list(token_search_results[token][doc_id]['abstract'])\n",
    "                    for token in query_tokens\n",
    "                    if token in token_search_results and doc_id in token_search_results[token]\n",
    "                ], [])\n",
    "            }\n",
    "            for doc_id in doc_scores\n",
    "        }\n",
    "    else:\n",
    "        doc_highlights = {\n",
    "            doc_id: {\n",
    "                'title': [],\n",
    "                'abstract': [],\n",
    "            }\n",
    "            for doc_id in doc_scores\n",
    "        }\n",
    "    documents = [\n",
    "        SearchResult(\n",
    "            doc_id=doc_id,\n",
    "            score=score,\n",
    "            search_score=normalized_doc_scores[doc_id],\n",
    "            pagerank_score=normalized_pagerank_scores[doc_id],\n",
    "            title=highlight_text(corpus.data[corpus.data['id'] == doc_id]['title'].item(),\n",
    "                                 doc_highlights[doc_id]['title']),\n",
    "            abstract=highlight_text(corpus.data[corpus.data['id'] == doc_id]['abstract'].item(),\n",
    "                                    doc_highlights[doc_id]['abstract']),\n",
    "            related_topics=corpus.data[corpus.data['id'] == doc_id]['related_topics'].item(),\n",
    "        )\n",
    "        for i, (doc_id, score) in enumerate(sorted(combined_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "        if i < max_result_count or max_result_count == -1\n",
    "    ]\n",
    "    if print_result:\n",
    "        for i, doc in enumerate(documents):\n",
    "            print(f'# {i} - Document ID: {colored(doc.doc_id, \"green\")} - Score: {colored(doc.score, \"green\")}'.center(100, '-'))\n",
    "            print(f' Search Score: {colored(doc.search_score, \"green\")} - Pagerank Score: {colored(doc.pagerank_score, \"green\")} '.center(100, '-'))\n",
    "            print(f' {doc.title} '.center(100, '-'))\n",
    "            print(f' Related Topics: {\", \".join(doc.related_topics)} '.center(100, '-'))\n",
    "            print(f'Abstract'.center(100, '-'))\n",
    "            print(doc.abstract)\n",
    "            print('-' * 100)\n",
    "    return documents"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "ss = Corpus(papers_by_professor, all_papers, stop_topk=20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    token  count\n",
      "0     the  95681\n",
      "1      of  78101\n",
      "2     and  58691\n",
      "3      be  51762\n",
      "4       a  43050\n",
      "5      in  40747\n",
      "6      to  38559\n",
      "7     for  24366\n",
      "8    that  19504\n",
      "9    with  15616\n",
      "10     we  14493\n",
      "11     on  13954\n",
      "12     by  12515\n",
      "13    use  12349\n",
      "14   this  12187\n",
      "15   cell  11316\n",
      "16     as  10186\n",
      "17   from   9604\n",
      "18  image   9441\n",
      "19     an   9377\n"
     ]
    }
   ],
   "source": [
    "# cache preprocess result for ai-bio\n",
    "_ = ss.non_stop_documents"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "ss_trie = construct_positional_indexes(ss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('variable-byte:', store_index(ss_trie, \"index/ss/variable-byte\", \"variable-byte\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ss_trie = load_index(\"index/ss/variable-byte\", \"variable-byte\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "ss_bigram_index = create_bigram_index(ss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------Query------------------------------------------------\n",
      "-----------Attention in Convolutional neural networks for interpretation and explanation------------\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating PageRank:   8%|▊         | 8/100 [00:00<00:01, 65.76it/s, loss=7.03e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 0 - Document ID: \u001B[32mabd1c342495432171beb7ca8fd9551ef13cbd0ff\u001B[0m - Score: \u001B[32m0.7732111738388863\u001B[0m\n",
      "------------- Search Score: \u001B[32m0.5464223476777725\u001B[0m - Pagerank Score: \u001B[32m1.0\u001B[0m -------------\n",
      "---- ImageNet classification with deep \u001B[31mconvolutional\u001B[0m \u001B[31mneural\u001B[0m \u001B[31mnetworks\u001B[0m ----\n",
      "--------------------------------- Related Topics: Computer Science ---------------------------------\n",
      "----------------------------------------------Abstract----------------------------------------------\n",
      "A large, deep \u001B[31mconvolutional\u001B[0m \u001B[31mneural\u001B[0m \u001B[31mnetwork\u001B[0m was trained to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes and employed a recently developed regularization method called &quot;dropout&quot; that proved to be very effective. We trained a large, deep \u001B[31mconvolutional\u001B[0m \u001B[31mneural\u001B[0m \u001B[31mnetwork\u001B[0m to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The \u001B[31mneural\u001B[0m \u001B[31mnetwork\u001B[0m, which has 60 million parameters and 650,000 neurons, consists of five \u001B[31mconvolutional\u001B[0m layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called &quot;dropout&quot; that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 1 - Document ID: \u001B[32m19f1bf3db5bee640f4d71440c646b43344010f70\u001B[0m - Score: \u001B[32m0.5098622392711568\u001B[0m\n",
      "------------- Search Score: \u001B[32m1.0\u001B[0m - Pagerank Score: \u001B[32m0.0197244785423137\u001B[0m -------------\n",
      "---------------------- Integrated Learning: Controlling \u001B[31mExplanation\u001B[0m -----------------------\n",
      "--------------------------------- Related Topics: Computer Science ---------------------------------\n",
      "----------------------------------------------Abstract----------------------------------------------\n",
      "A combination of the two methods—applying \u001B[31mexplanation\u001B[0m-based techniques during the course of similiarity-based learning can achieve the power of \u001B[31mexplanationbased\u001B[0m learning without some of the computational problems that can otherwise arise in domains lacking detailed explanatory rules. Similarity-based learning, which involves largely structural comparisons of instances, and \u001B[31mexplanation\u001B[0m-based learning, a knowledge-intensive method for analyzing instances to build generalized schemata, are two major inductive learning techniques in use in Artificial Intelligence. In this paper, we propose a combination of the two methods—applying \u001B[31mexplanation\u001B[0m-based techniques during the course of similiarity-based learning. For domains lacking detailed explanatory rules, this combination can achieve the power of \u001B[31mexplanationbased\u001B[0m learning without some of the computational problems that can otherwise arise. We show how the ideas of predictability and interest can be particularly valuable in this context. We include an example of the computer program UNIMEM applying \u001B[31mexplanation\u001B[0m to a generalization formed using similaritybased methods.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 2 - Document ID: \u001B[32m0222e3870cc00b48df1277406858304bfcec9a06\u001B[0m - Score: \u001B[32m0.47935781679496403\u001B[0m\n",
      "----- Search Score: \u001B[32m0.9389911550476143\u001B[0m - Pagerank Score: \u001B[32m0.0197244785423137\u001B[0m ------\n",
      "------------------- A Domain Independent \u001B[31mExplanation\u001B[0m-Based Generalizer --------------------\n",
      "--------------------------------- Related Topics: Computer Science ---------------------------------\n",
      "----------------------------------------------Abstract----------------------------------------------\n",
      "This method is compared and contrasted with other approaches to generalizing \u001B[31mexplanations\u001B[0m, including an abstract version of the algorithm used in the STRIPS system and the EBG technique recently developed by Mitchell, Keller, and Kedar-Cabelli. A domain independent technique for generalizing a broad class of \u001B[31mexplanations\u001B[0m is described. This method is compared and contrasted with other approaches to generalizing \u001B[31mexplanations\u001B[0m, including an abstract version of the algorithm used in the STRIPS system and the EBG technique recently developed by Mitchell, Keller, and Kedar-Cabelli. We have tested this generalization technique on a number examples in different domains, and present detailed descriptions of several of these.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 3 - Document ID: \u001B[32m317aee7fc081f2b137a85c4f20129007fd8e717e\u001B[0m - Score: \u001B[32m0.47867101158965075\u001B[0m\n",
      "----- Search Score: \u001B[32m0.4441864334789032\u001B[0m - Pagerank Score: \u001B[32m0.5131555897003983\u001B[0m ------\n",
      "------------- Fully \u001B[31mconvolutional\u001B[0m \u001B[31mnetworks\u001B[0m for semantic segmentation -------------\n",
      "--------------------------------- Related Topics: Computer Science ---------------------------------\n",
      "----------------------------------------------Abstract----------------------------------------------\n",
      "The key insight is to build “fully \u001B[31mconvolutional\u001B[0m” \u001B[31mnetworks\u001B[0m that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. \u001B[31mConvolutional\u001B[0m \u001B[31mnetworks\u001B[0m are powerful visual models that yield hierarchies of features. We show that \u001B[31mconvolutional\u001B[0m \u001B[31mnetworks\u001B[0m by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully \u001B[31mconvolutional\u001B[0m” \u001B[31mnetworks\u001B[0m that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully \u001B[31mconvolutional\u001B[0m \u001B[31mnetworks\u001B[0m, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification \u001B[31mnetworks\u001B[0m (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully \u001B[31mconvolutional\u001B[0m \u001B[31mnetworks\u001B[0m and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully \u001B[31mconvolutional\u001B[0m \u001B[31mnetwork\u001B[0m achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 4 - Document ID: \u001B[32meb42cf88027de515750f230b23b1a057dc782108\u001B[0m - Score: \u001B[32m0.4428609001463054\u001B[0m\n",
      "----- Search Score: \u001B[32m0.37998191584594393\u001B[0m - Pagerank Score: \u001B[32m0.5057398844466668\u001B[0m -----\n",
      "------- Very Deep \u001B[31mConvolutional\u001B[0m \u001B[31mNetworks\u001B[0m for Large-Scale Image Recognition -------\n",
      "--------------------------------- Related Topics: Computer Science ---------------------------------\n",
      "----------------------------------------------Abstract----------------------------------------------\n",
      "This work investigates the effect of the \u001B[31mconvolutional\u001B[0m \u001B[31mnetwork\u001B[0m depth on its accuracy in the large-scale image recognition setting using an architecture with very small convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. In this work we investigate the effect of the \u001B[31mconvolutional\u001B[0m \u001B[31mnetwork\u001B[0m depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of \u001B[31mnetworks\u001B[0m of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 5 - Document ID: \u001B[32meb2f539a17487db2c93785214da2fc7a67a57840\u001B[0m - Score: \u001B[32m0.42723780539426026\u001B[0m\n",
      "----- Search Score: \u001B[32m0.8347511322462068\u001B[0m - Pagerank Score: \u001B[32m0.0197244785423137\u001B[0m ------\n",
      "-------- Quantitative Results Concerning the Utility of \u001B[31mExplanation\u001B[0m-based Learning --------\n",
      "------------------------------------- Related Topics: Business -------------------------------------\n",
      "----------------------------------------------Abstract----------------------------------------------\n",
      "Semantic Scholar extracted view of &quot;Quantitative Results Concerning the Utility of \u001B[31mExplanation\u001B[0m-based Learning&quot; by Steven Minton\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 6 - Document ID: \u001B[32m4d19272112b50547614479a0c409fca66e3b05f7\u001B[0m - Score: \u001B[32m0.4191364135299706\u001B[0m\n",
      "----- Search Score: \u001B[32m0.8068658478061577\u001B[0m - Pagerank Score: \u001B[32m0.03140697925378356\u001B[0m -----\n",
      "----- Boosting the margin: A new \u001B[31mexplanation\u001B[0m for the effectiveness of voting methods ------\n",
      "--------------------------------- Related Topics: Computer Science ---------------------------------\n",
      "----------------------------------------------Abstract----------------------------------------------\n",
      "It is shown that techniques used in the analysis of Vapnik&#39;s support vector classifiers and of \u001B[31mneural\u001B[0m \u001B[31mnetworks\u001B[0m with small weights can be applied to voting methods to relate the margin distribution to the test error. One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large, and often is observed to decrease even after the training error reaches zero. In this paper, we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule, where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label. We show that techniques used in the analysis of Vapnik&#39;s support vector classifiers and of \u001B[31mneural\u001B[0m \u001B[31mnetworks\u001B[0m with small weights can be applied to voting methods to relate the margin distribution to the test error. We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples. Finally, we compare our \u001B[31mexplanation\u001B[0m to those based on the bias-variance\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 7 - Document ID: \u001B[32m007fd9e68bc805a23bb21d6863e679adfe78d13e\u001B[0m - Score: \u001B[32m0.38458341735825474\u001B[0m\n",
      "---- Search Score: \u001B[32m0.7485929280942223\u001B[0m - Pagerank Score: \u001B[32m0.020573906622287225\u001B[0m -----\n",
      "--------- Symbolic \u001B[31mInterpretation\u001B[0m of Artiicial \u001B[31mNeural\u001B[0m \u001B[31mNetworks\u001B[0m ----------\n",
      "--------------------------------- Related Topics: Computer Science ---------------------------------\n",
      "----------------------------------------------Abstract----------------------------------------------\n",
      "This paper presents three rule extraction techniques and proposes a rule evaluation technique that orders extracted rules based on three performance measures and is applied to the iris and breast cancer data sets. H ybrid Intelligent Systems that combine knowledge based and artiicial \u001B[31mneural\u001B[0m \u001B[31mnetwork\u001B[0m systems typically have four phases involving domain knowledge representation, mapping of this knowledge into an initial connectionist architecture, \u001B[31mnetwork\u001B[0m training and rule extraction respectively. The nal phase is important because it can provide a trained connectionist architecture with \u001B[31mexplanation\u001B[0m power and validate its output decisions. Moreover, it can be used to reene and maintain the initial knowledge acquired from domain experts. In this paper, we present three rule extraction techniques. The rst technique extracts a set of binary rules from any type of \u001B[31mneural\u001B[0m \u001B[31mnetwork\u001B[0m. The other two techniques are speciic to feedforward \u001B[31mnetworks\u001B[0m with a single hidden layer of sigmoidal units. Technique 2 extracts partial rules that represent the most important embedded knowledge with an adjustable level of detail, while the third technique provides a more comprehensive and universal approach. A rule evaluation technique that orders extracted rules based on three performance measures is then proposed. The three techniques are applied to the iris and breast cancer data sets. The extracted rules are evaluated qualitatively and quantitatively, and compared with those obtained by other approaches.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 8 - Document ID: \u001B[32mbc37101dc273c4aa75ccdefc0f3ee439d28212dd\u001B[0m - Score: \u001B[32m0.38383166230090604\u001B[0m\n",
      "---- Search Score: \u001B[32m0.7470894179795249\u001B[0m - Pagerank Score: \u001B[32m0.020573906622287225\u001B[0m -----\n",
      "--------- Symbolic \u001B[31mInterpretation\u001B[0m of Artificial \u001B[31mNeural\u001B[0m \u001B[31mNetworks\u001B[0m ---------\n",
      "--------------------------------- Related Topics: Computer Science ---------------------------------\n",
      "----------------------------------------------Abstract----------------------------------------------\n",
      "This paper presents three rule extraction techniques, one of which is specific to feedforward \u001B[31mnetworks\u001B[0m, with a single hidden layer of sigmoidal units, and a rule-evaluation technique, which orders extracted rules based on three performance measures. Hybrid intelligent systems that combine knowledge-based and artificial \u001B[31mneural\u001B[0m \u001B[31mnetwork\u001B[0m systems typically have four phases, involving domain knowledge representation, mapping of this knowledge into an initial connectionist architecture, \u001B[31mnetwork\u001B[0m training and rule extraction, respectively. The final phase is important because it can provide a trained connectionist architecture with \u001B[31mexplanation\u001B[0m power and validate its output decisions. Moreover, it can be used to refine and maintain the initial knowledge acquired from domain experts. In this paper, we present three rule extraction techniques. The first technique extracts a set of binary rules from any type of \u001B[31mneural\u001B[0m \u001B[31mnetwork\u001B[0m. The other two techniques are specific to feedforward \u001B[31mnetworks\u001B[0m, with a single hidden layer of sigmoidal units. Technique 2 extracts partial rules that represent the most important embedded knowledge with an adjustable level of detail, while the third technique provides a more comprehensive and universal approach. A rule-evaluation technique, which orders extracted rules based on three performance measures, is then proposed. The three techniques area applied to the iris and breast cancer data sets. The extracted rules are evaluated qualitatively and quantitatively, and are compared with those obtained by other approaches.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 9 - Document ID: \u001B[32m8388f1be26329fa45e5807e968a641ce170ea078\u001B[0m - Score: \u001B[32m0.3753842050976819\u001B[0m\n",
      "----- Search Score: \u001B[32m0.4776152007486479\u001B[0m - Pagerank Score: \u001B[32m0.27315320944671595\u001B[0m -----\n",
      " Unsupervised Representation Learning with Deep \u001B[31mConvolutional\u001B[0m Generative Adversarial \u001B[31mNetworks\u001B[0m \n",
      "--------------------------------- Related Topics: Computer Science ---------------------------------\n",
      "----------------------------------------------Abstract----------------------------------------------\n",
      "This work introduces a class of CNNs called deep \u001B[31mconvolutional\u001B[0m generative adversarial \u001B[31mnetworks\u001B[0m (DCGANs), that have certain architectural constraints, and demonstrates that they are a strong candidate for unsupervised learning. In recent years, supervised learning with \u001B[31mconvolutional\u001B[0m \u001B[31mnetworks\u001B[0m (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less \u001B[31mattention\u001B[0m. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep \u001B[31mconvolutional\u001B[0m generative adversarial \u001B[31mnetworks\u001B[0m (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep \u001B[31mconvolutional\u001B[0m adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 10 - Document ID: \u001B[32m2f4df08d9072fc2ac181b7fced6a245315ce05c8\u001B[0m - Score: \u001B[32m0.3751394643326903\u001B[0m\n",
      "----- Search Score: \u001B[32m0.09355000816030253\u001B[0m - Pagerank Score: \u001B[32m0.656728920505078\u001B[0m ------\n",
      "--------- Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation ---------\n",
      "--------------------------------- Related Topics: Computer Science ---------------------------------\n",
      "----------------------------------------------Abstract----------------------------------------------\n",
      "This paper proposes a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity \u001B[31mconvolutional\u001B[0m \u001B[31mneural\u001B[0m \u001B[31mnetworks\u001B[0m (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the \u001B[31mnetwork\u001B[0m learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 11 - Document ID: \u001B[32m2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45\u001B[0m - Score: \u001B[32m0.37251669683042454\u001B[0m\n",
      "----- Search Score: \u001B[32m0.6757890066910011\u001B[0m - Pagerank Score: \u001B[32m0.06924438696984803\u001B[0m -----\n",
      "---------------- Reasoning about Entailment with \u001B[31mNeural\u001B[0m \u001B[31mAttention\u001B[0m ----------------\n",
      "--------------------------------- Related Topics: Computer Science ---------------------------------\n",
      "----------------------------------------------Abstract----------------------------------------------\n",
      "This paper proposes a \u001B[31mneural\u001B[0m model that reads two sentences to determine entailment using long short-term memory units and extends this model with a word-by-word \u001B[31mneural\u001B[0m \u001B[31mattention\u001B[0m mechanism that encourages reasoning over entailments of pairs of words and phrases, and presents a qualitative analysis of \u001B[31mattention\u001B[0m weights produced by this model. While most approaches to automatically recognizing entailment relations have used classifiers employing hand engineered features derived from complex natural language processing pipelines, in practice their performance has been only slightly better than bag-of-word pair classifiers using only lexical similarity. The only attempt so far to build an end-to-end differentiable \u001B[31mneural\u001B[0m \u001B[31mnetwork\u001B[0m for entailment failed to outperform such a simple similarity classifier. In this paper, we propose a \u001B[31mneural\u001B[0m model that reads two sentences to determine entailment using long short-term memory units. We extend this model with a word-by-word \u001B[31mneural\u001B[0m \u001B[31mattention\u001B[0m mechanism that encourages reasoning over entailments of pairs of words and phrases. Furthermore, we present a qualitative analysis of \u001B[31mattention\u001B[0m weights produced by this model, demonstrating such reasoning capabilities. On a large entailment dataset this model outperforms the previous best \u001B[31mneural\u001B[0m model and a classifier with engineered features by a substantial margin. It is the first generic end-to-end differentiable system that achieves state-of-the-art accuracy on a textual entailment dataset.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 12 - Document ID: \u001B[32m345ef9a7d9af0ac0816d76803ddcf9b6d19404d7\u001B[0m - Score: \u001B[32m0.3699765552064542\u001B[0m\n",
      "----- Search Score: \u001B[32m0.6978968806433467\u001B[0m - Pagerank Score: \u001B[32m0.04205622976956184\u001B[0m -----\n",
      "------- \u001B[31mNeural\u001B[0m Relation Extraction with Selective \u001B[31mAttention\u001B[0m over Instances -------\n",
      "--------------------------------- Related Topics: Computer Science ---------------------------------\n",
      "----------------------------------------------Abstract----------------------------------------------\n",
      "A sentence-level \u001B[31mattention\u001B[0m-based model for relation extraction that employs \u001B[31mconvolutional\u001B[0m \u001B[31mneural\u001B[0m \u001B[31mnetworks\u001B[0m to embed the semantics of sentences and dynamically reduce the weights of those noisy instances. Distant supervised relation extraction has been widely used to find novel relational facts from text. However, distant supervision inevitably accompanies with the wrong labelling problem, and these noisy data will substantially hurt the performance of relation extraction. To alleviate this issue, we propose a sentence-level \u001B[31mattention\u001B[0m-based model for relation extraction. In this model, we employ \u001B[31mconvolutional\u001B[0m \u001B[31mneural\u001B[0m \u001B[31mnetworks\u001B[0m to embed the semantics of sentences. Afterwards, we build sentence-level \u001B[31mattention\u001B[0m over multiple instances, which is expected to dynamically reduce the weights of those noisy instances. Experimental results on real-world datasets show that, our model can make full use of all informative sentences and effectively reduce the influence of wrong labelled instances. Our model achieves significant and consistent improvements on relation extraction as compared with baselines. The source code of this paper can be obtained from https: //github.com/thunlp/NRE.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 13 - Document ID: \u001B[32m1a2a770d23b4a171fa81de62a78a3deb0588f238\u001B[0m - Score: \u001B[32m0.36948562698142584\u001B[0m\n",
      "----- Search Score: \u001B[32m0.4348609804368595\u001B[0m - Pagerank Score: \u001B[32m0.3041102735259921\u001B[0m ------\n",
      "-------------- Visualizing and Understanding \u001B[31mConvolutional\u001B[0m \u001B[31mNetworks\u001B[0m --------------\n",
      "--------------------------------- Related Topics: Computer Science ---------------------------------\n",
      "----------------------------------------------Abstract----------------------------------------------\n",
      "A novel visualization technique is introduced that gives insight into the function of intermediate feature layers and the operation of the classifier in large \u001B[31mConvolutional\u001B[0m \u001B[31mNetwork\u001B[0m models, used in a diagnostic role to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. Large \u001B[31mConvolutional\u001B[0m \u001B[31mNetwork\u001B[0m models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 14 - Document ID: \u001B[32m204e3073870fae3d05bcbc2f6a8e263d9b72e776\u001B[0m - Score: \u001B[32m0.3691571395527013\u001B[0m\n",
      "----- Search Score: \u001B[32m0.6242933894564741\u001B[0m - Pagerank Score: \u001B[32m0.11402088964892852\u001B[0m -----\n",
      "-------------------------------- \u001B[31mAttention\u001B[0m is All you Need --------------------------------\n",
      "--------------------------------- Related Topics: Computer Science ---------------------------------\n",
      "----------------------------------------------Abstract----------------------------------------------\n",
      "A new simple \u001B[31mnetwork\u001B[0m architecture, the Transformer, based solely on \u001B[31mattention\u001B[0m mechanisms, dispensing with recurrence and convolutions entirely is proposed, which generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. The dominant sequence transduction models are based on complex recurrent or \u001B[31mconvolutional\u001B[0m \u001B[31mneural\u001B[0m \u001B[31mnetworks\u001B[0m in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an \u001B[31mattention\u001B[0m mechanism. We propose a new simple \u001B[31mnetwork\u001B[0m architecture, the Transformer, based solely on \u001B[31mattention\u001B[0m mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 15 - Document ID: \u001B[32m13d9323a8716131911bfda048a40e2cde1a76a46\u001B[0m - Score: \u001B[32m0.368546942441655\u001B[0m\n",
      "----- Search Score: \u001B[32m0.695753657045894\u001B[0m - Pagerank Score: \u001B[32m0.04134022783741598\u001B[0m ------\n",
      "------------------------- Structured \u001B[31mAttention\u001B[0m \u001B[31mNetworks\u001B[0m --------------------------\n",
      "--------------------------------- Related Topics: Computer Science ---------------------------------\n",
      "----------------------------------------------Abstract----------------------------------------------\n",
      "This work shows that structured \u001B[31mattention\u001B[0m \u001B[31mnetworks\u001B[0m are simple extensions of the basic \u001B[31mattention\u001B[0m procedure, and that they allow for extending \u001B[31mattention\u001B[0m beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. \u001B[31mAttention\u001B[0m \u001B[31mnetworks\u001B[0m have proven to be an effective approach for embedding categorical inference within a deep \u001B[31mneural\u001B[0m \u001B[31mnetwork\u001B[0m. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep \u001B[31mnetworks\u001B[0m. We show that these structured \u001B[31mattention\u001B[0m \u001B[31mnetworks\u001B[0m are simple extensions of the basic \u001B[31mattention\u001B[0m procedure, and that they allow for extending \u001B[31mattention\u001B[0m beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured \u001B[31mattention\u001B[0m \u001B[31mnetworks\u001B[0m: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as \u001B[31mneural\u001B[0m \u001B[31mnetwork\u001B[0m layers. Experiments show that this approach is effective for incorporating structural biases, and structured \u001B[31mattention\u001B[0m \u001B[31mnetworks\u001B[0m outperform baseline \u001B[31mattention\u001B[0m models on a variety of synthetic and real tasks: tree transduction, \u001B[31mneural\u001B[0m machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple \u001B[31mattention\u001B[0m.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 16 - Document ID: \u001B[32mc70218603f0af1be5d063056cbe629e042141a86\u001B[0m - Score: \u001B[32m0.36534353873516295\u001B[0m\n",
      "---- Search Score: \u001B[32m0.6991929765068521\u001B[0m - Pagerank Score: \u001B[32m0.031494100963473846\u001B[0m -----\n",
      "--------------------------------- Learn To Pay \u001B[31mAttention\u001B[0m ----------------------------------\n",
      "--------------------------------- Related Topics: Computer Science ---------------------------------\n",
      "----------------------------------------------Abstract----------------------------------------------\n",
      "An end-to-end-trainable \u001B[31mattention\u001B[0m module for \u001B[31mconvolutional\u001B[0m \u001B[31mneural\u001B[0m \u001B[31mnetwork\u001B[0m (CNN) architectures built for image classification that is able to bootstrap standard CNN architectures for the task of image classification, demonstrating superior generalisation over 6 unseen benchmark datasets. We propose an end-to-end-trainable \u001B[31mattention\u001B[0m module for \u001B[31mconvolutional\u001B[0m \u001B[31mneural\u001B[0m \u001B[31mnetwork\u001B[0m (CNN) architectures built for image classification. The module takes as input the 2D feature vector maps which form the intermediate representations of the input image at different stages in the CNN pipeline, and outputs a 2D matrix of scores for each map. Standard CNN architectures are modified through the incorporation of this module, and trained under the constraint that a convex combination of the intermediate 2D feature vectors, as parameterised by the score matrices, must \\textit{alone} be used for classification. Incentivised to amplify the relevant and suppress the irrelevant or misleading, the scores thus assume the role of \u001B[31mattention\u001B[0m values. Our experimental observations provide clear evidence to this effect: the learned \u001B[31mattention\u001B[0m maps neatly highlight the regions of interest while suppressing background clutter. Consequently, the proposed function is able to bootstrap standard CNN architectures for the task of image classification, demonstrating superior generalisation over 6 unseen benchmark datasets. When binarised, our \u001B[31mattention\u001B[0m maps outperform other CNN-based \u001B[31mattention\u001B[0m maps, traditional saliency maps, and top object proposals for weakly supervised segmentation as demonstrated on the Object Discovery dataset. We also demonstrate improved robustness against the fast gradient sign method of adversarial attack.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 17 - Document ID: \u001B[32mb8de958fead0d8a9619b55c7299df3257c624a96\u001B[0m - Score: \u001B[32m0.3631105263644474\u001B[0m\n",
      "----- Search Score: \u001B[32m0.3454090686834375\u001B[0m - Pagerank Score: \u001B[32m0.3808119840454573\u001B[0m ------\n",
      "------ DeCAF: A Deep \u001B[31mConvolutional\u001B[0m Activation Feature for Generic Visual Recognition ------\n",
      "--------------------------------- Related Topics: Computer Science ---------------------------------\n",
      "----------------------------------------------Abstract----------------------------------------------\n",
      "DeCAF, an open-source implementation of deep \u001B[31mconvolutional\u001B[0m activation features, along with all associated \u001B[31mnetwork\u001B[0m parameters, are released to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms. We evaluate whether features extracted from the activation of a deep \u001B[31mconvolutional\u001B[0m \u001B[31mnetwork\u001B[0m trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be repurposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep \u001B[31mconvolutional\u001B[0m features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various \u001B[31mnetwork\u001B[0m levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep \u001B[31mconvolutional\u001B[0m activation features, along with all associated \u001B[31mnetwork\u001B[0m parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 18 - Document ID: \u001B[32m8978cf7574ceb35f4c3096be768c7547b28a35d0\u001B[0m - Score: \u001B[32m0.36293181748851755\u001B[0m\n",
      "---- Search Score: \u001B[32m0.014553586096964449\u001B[0m - Pagerank Score: \u001B[32m0.7113100488800707\u001B[0m -----\n",
      "-------------------------- A Fast Learning Algorithm for Deep Belief Nets --------------------------\n",
      "--------------------------------- Related Topics: Computer Science ---------------------------------\n",
      "----------------------------------------------Abstract----------------------------------------------\n",
      "A fast, greedy algorithm is derived that can learn deep, directed belief \u001B[31mnetworks\u001B[0m one layer at a time, provided the top two layers form an undirected associative memory. We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief \u001B[31mnetworks\u001B[0m one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a \u001B[31mnetwork\u001B[0m with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 19 - Document ID: \u001B[32m27725a2d2a8cee9bf9fffc6c2167017103aba0fa\u001B[0m - Score: \u001B[32m0.3468057273221006\u001B[0m\n",
      "----- Search Score: \u001B[32m0.5794932444242336\u001B[0m - Pagerank Score: \u001B[32m0.11411821021996756\u001B[0m -----\n",
      "-------- A \u001B[31mConvolutional\u001B[0m \u001B[31mNeural\u001B[0m \u001B[31mNetwork\u001B[0m for Modelling Sentences ---------\n",
      "--------------------------------- Related Topics: Computer Science ---------------------------------\n",
      "----------------------------------------------Abstract----------------------------------------------\n",
      "A \u001B[31mconvolutional\u001B[0m architecture dubbed the Dynamic \u001B[31mConvolutional\u001B[0m \u001B[31mNeural\u001B[0m \u001B[31mNetwork\u001B[0m (DCNN) is described that is adopted for the semantic modelling of sentences and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The ability to accurately represent sentences is central to language understanding. We describe a \u001B[31mconvolutional\u001B[0m architecture dubbed the Dynamic \u001B[31mConvolutional\u001B[0m \u001B[31mNeural\u001B[0m \u001B[31mNetwork\u001B[0m (DCNN) that we adopt for the semantic modelling of sentences. The \u001B[31mnetwork\u001B[0m uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The \u001B[31mnetwork\u001B[0m handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The \u001B[31mnetwork\u001B[0m does not rely on a parse tree and is easily applicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The \u001B[31mnetwork\u001B[0m achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "_ = search(ss, ss_trie, ss_bigram_index,\n",
    "           'Attention in Convolutional neural networks for interpretation and explanation',\n",
    "           20, method='okapi25', weight=0.2, highlight=True, print_result=True,\n",
    "           preference_by_professor={'Rohban': 10,\n",
    "                                    'Rabiee': 2,\n",
    "                                    'Soleymani': 4,\n",
    "                                    'Sharifi': 1,\n",
    "                                    'Kasaei': 1})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir=\"rtl\" style=\"text-align: justify\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>رتبه‌بندی نویسندگان (۲۵ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "    برای رتبه‌بندی نویسندگان، مفهوم ارجاع نویسندگان به یکدیگر مطرح می‌شود. زمانی که نویسنده A در مقاله خود به مقاله P که نویسنده B جزو نویسندگان آن مقاله یعنی مقاله P می‌باشد، ارجاع دهد، می‌گوییم که نویسنده A به نویسنده B ارجاع داده است. با توجه به این رابطه، می‌توان گراف ارجاعات بین نویسندگان را ایجاد و سپس با استفاده از الگوریتم HITS\n",
    "نویسندگان را رتبه‌بندی کرد. برای رتبه‌بندی نیاز است تا از شاخص‌های hub و authority استفاده کنیم.\n",
    "\n",
    "\n",
    "</font>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['author:M. Kristan', 'author:A. Leonardis', 'author:Jiri Matas', 'author:M. Felsberg', 'author:R. Pflugfelder', 'author:Joni-Kristian Kämäräinen', 'author:Martin Danelljan', 'author:L. Č. Zajc', 'author:A. Lukežič', 'author:O. Drbohlav']\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "\n",
    "def hit_algorithm(papers, n):\n",
    "    \"\"\"\n",
    "        Implementing the HITS algorithm to score authors based on their papers and co-authors.\n",
    "\n",
    "        Parameters\n",
    "        ---------------------------------------------------------------------------------------------------\n",
    "        papers: A list of paper dictionaries with the following keys:\n",
    "                \"id\": A unique ID for the paper\n",
    "                \"title\": The title of the paper\n",
    "                \"abstract\": The abstract of the paper\n",
    "                \"publication_year\": The year in which the paper was published\n",
    "                \"authors\": A list of the names of the authors of the paper\n",
    "                \"related_topics\": A list of IDs for related topics (optional)\n",
    "                \"citation_count\": The number of times the paper has been cited (optional)\n",
    "                \"reference_count\": The number of references in the paper (optional)\n",
    "                \"references\": A list of IDs for papers that are cited in the paper (optional)\n",
    "        n: An integer representing the number of top authors to return.\n",
    "\n",
    "        Returns\n",
    "        ---------------------------------------------------------------------------------------------------\n",
    "        List\n",
    "        list of the top n authors based on their hub scores.\n",
    "    \"\"\"\n",
    "    # Create a graph of authors and papers\n",
    "    g = nx.Graph()\n",
    "\n",
    "    for paper in papers:\n",
    "        paper_id = paper['id']\n",
    "        authors = paper['authors']\n",
    "\n",
    "        # Add paper node to the graph\n",
    "        g.add_node(f'paper:{paper_id}')\n",
    "\n",
    "        for author in authors:\n",
    "            # Add author node to the graph\n",
    "            g.add_node(f'author:{author}')\n",
    "\n",
    "            # Connect author node to paper node\n",
    "            g.add_edge(f'author:{author}', f'paper:{paper_id}')\n",
    "\n",
    "    # Run the HITS algorithm\n",
    "    hubs, authorities = nx.hits(g, max_iter=1000, tol=1e-08)\n",
    "\n",
    "    # Sort authors based on their hub scores\n",
    "    sorted_authors = sorted([a[0] for a in hubs.items() if a[0].startswith('author:')],\n",
    "                            key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return sorted_authors[:n]\n",
    "\n",
    "\n",
    "# call the hit_algorithm function\n",
    "top_authors = hit_algorithm(all_papers, 10)\n",
    "\n",
    "# print the top authors\n",
    "print(top_authors)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>سیستم پیشنهادگر (۲۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "\n",
    "در این بخش سعی می‌کنیم که یک سیستم پیشنهادگر مقالات بر اساس جست‌و‌جو‌ها یا علايق یک کاربر پیاده‌سازی کنیم، سیستم پیشنهاد دهنده‌ای که قصد داریم آن را ایجاد کنیم،‌ باید بتواند بر اساس لیستی از مقالاتی که کاربر قبلا آن‌ها را مطالعه کرده یا به آن‌ها علاقه داشته است، مقالات تازه انتشار یافته‌‌ی جدید را به کاربر پیشنهاد دهد.\n",
    "\n",
    "در فایل recommended_papers.json\n",
    "لیستی از کاربران قرار دارد که در فیلد positive_papers هر کاربر،\n",
    "تعداد ۵۰ مقاله از مقالاتی که کاربر به آن‌ها علاقه داشته است مشخص شده است. و همچینین در فیلد recommendedPapers هر کاربر تعداد ۱۰ مقاله به ترتیب اهمیت، از مقالات جدیدی که کاربر آن‌ها را پسندیده است قرار دارد.\n",
    "\n",
    "در این بخش هدف شما یادگیری سیستم پیشنهاد‌ دهنده بر اساس همین داده‌ها می‌باشد، و به عبارتی شما بایستی کاربر‌ها را به دو دسته آموزش و آزمایش تقسیم کنید، و بر اساس داده‌های آموزشی بتوانید مقالات جدید مورد پسند کاربرهای آزمایش را پیش‌بینی کنید. (بنابراین در این پیش‌بینی نمی‌توانید از فیلد recommendedPapers این کاربران استفاده کنید.)\n",
    "\n",
    "</font>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/papers/recommended_papers.json', 'r') as fp:\n",
    "    recommended_papers = json.load(fp)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "sample_user = recommended_papers[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "542"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(recommended_papers)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 10)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_user['positive_papers']), len(sample_user['recommendedPapers'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d9404b4a794c07b5e2cdf3203aabf06d70c6be9b\n",
      "CENTAURO: A Hybrid Locomotion and High Power Resilient Manipulation Platform\n",
      "Despite the development of a large number of mobile manipulation robots, very few platforms can demonstrate the required strength and mechanical sturdiness to accommodate the needs of real-world applications with high payload and moderate/harsh physical interaction demands, e.g., in disaster-response scenarios or heavy logistics/collaborative tasks. In this letter, we introduce the design of a wheeled-legged mobile manipulation platform capable of executing demanding manipulation tasks, and demonstrating significant physical resilience while possessing a body size (height/width) and weight compatible to that of a human. The achieved performance is the result of combining a number of design and implementation principles related to the actuation system, the integration of body structure and actuation, and the wheeled-legged mobility concept. These design principles are discussed, and the solutions adopted for various robot components are detailed. Finally, the robot performance is demonstrated in a set of experiments validating its power and strength capability when manipulating heavy payload and executing tasks involving high impact physical interactions.\n",
      "['Computer Science']\n"
     ]
    }
   ],
   "source": [
    "print(sample_user['positive_papers'][0]['paperId'])\n",
    "print(sample_user['positive_papers'][0]['title'])\n",
    "print(sample_user['positive_papers'][0]['abstract'])\n",
    "print(sample_user['positive_papers'][0]['fieldsOfStudy'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94eebbefe8a37cf394be899b85af295c2e3a1f01\n",
      "Efficient Parametric Approximations of Neural Network Function Space Distance\n",
      "It is often useful to compactly summarize important properties of model parameters and training data so that they can be used later without storing and/or iterating over the entire dataset. As a specific case, we consider estimating the Function Space Distance (FSD) over a training set, i.e. the average discrepancy between the outputs of two neural networks. We propose a Linearized Activation Function TRick (LAFTR) and derive an efficient approximation to FSD for ReLU neural networks. The key idea is to approximate the architecture as a linear network with stochastic gating. Despite requiring only one parameter per unit of the network, our approach outcompetes other parametric approximations with larger memory requirements. Applied to continual learning, our parametric approximation is competitive with state-of-the-art nonparametric approximations, which require storing many training examples. Furthermore, we show its efficacy in estimating influence functions accurately and detecting mislabeled examples without expensive iterations over the entire dataset.\n",
      "['Computer Science', 'Mathematics']\n"
     ]
    }
   ],
   "source": [
    "print(sample_user['recommendedPapers'][0]['paperId'])\n",
    "print(sample_user['recommendedPapers'][0]['title'])\n",
    "print(sample_user['recommendedPapers'][0]['abstract'])\n",
    "print(sample_user['recommendedPapers'][0]['fieldsOfStudy'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h2>\n",
    "    <b>روش Collaborative Filtering (۱۰ نمره)</b>\n",
    "    </h2>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "\n",
    "در این راهکار سعی می‌کنیم با استفاده از کاربران مشابه با یک کاربر، سلیقه‌ی او را حدس بزنیم و مقالاتی را که کاربران مشابه دیده‌اند را به کاربر نمایش دهیم.\n",
    "\n",
    "در این روش ابتدا باید $N$ کاربر که سلیقه‌ی مشابه با کاربر $x$ دارند را پیدا کنید، و با ترکیب لیست مقالات جدید مورد علاقه‌ی آن $N$ کاربر مشابه،\n",
    " ۱۰ مقاله‌ به کاربر $x$ پیشنهاد دهید.\n",
    "\n",
    "توجه داشته باشید که برای اینکه شباهت دو کاربر را پیدا کنید، باید cosine_similarity بین بردار زمینه‌های مورد علاقه‌ی دو کاربر استفاده کنید. این بردار از $M$ درایه تشکیل شده است، که $M$ تعداد زمینه‌های یکتاییست که در داده‌ها وجود دارد. و در این بردار درایه‌ی $j$ام\n",
    "نشان دهنده‌ی نسبت تعداد مقالات خوانده‌ی شده‌ کاربر در زمینه‌ی $j$ به تعداد کل مقاله‌های خوانده شده توسط او می‌باشد. (توجه کنید که هر مقاله می‌تواند چند زمینه داشته باشد و بنابراین حاصل جمع درایه‌های این بردار الزاما یک نمی‌باشد)\n",
    "\n",
    "</font>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "fields = sorted(set(\n",
    "    field\n",
    "    for user in recommended_papers\n",
    "    for paper in user['positive_papers']\n",
    "    for field in paper['fieldsOfStudy'] or []\n",
    "))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(recommended_papers, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "class CollaborativeFiltering:\n",
    "\n",
    "    def __init__(self, data: dict, n = 10):\n",
    "        self.data = data\n",
    "        self.knn = NearestNeighbors(n_neighbors=n, metric='cosine')\n",
    "\n",
    "    @staticmethod\n",
    "    def create_user_field_matrix(user_positive_papers: List[List[Dict[str, Any]]]) -> pd.DataFrame:\n",
    "        user_fields = {\n",
    "            'user_index': list(range(len(user_positive_papers))),\n",
    "            **{field: [sum(field in (paper['fieldsOfStudy'] or []) for paper in positive_papers)\n",
    "                       for positive_papers in user_positive_papers]\n",
    "               for field in fields}\n",
    "        }\n",
    "        return pd.DataFrame(user_fields).set_index('user_index')\n",
    "\n",
    "    def fit(self):\n",
    "        user_field_matrix = self.create_user_field_matrix([user['positive_papers'] or [] for user in self.data])\n",
    "        self.knn.fit(user_field_matrix)\n",
    "        return self\n",
    "\n",
    "    def predict(self, user_positive_papers: List[Dict[str, Any]]):\n",
    "        user_field_vector = self.create_user_field_matrix([user_positive_papers])\n",
    "        distances, indices = self.knn.kneighbors(user_field_vector)\n",
    "\n",
    "        # get recommended papers from the k nearest neighbors\n",
    "        result = [\n",
    "            paper['paperId']\n",
    "            for similar_user_id in indices[0]\n",
    "            for paper in self.data[similar_user_id]['recommendedPapers']\n",
    "        ]\n",
    "        return result\n",
    "\n",
    "collaborative_filtering = CollaborativeFiltering(train_data).fit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h2>\n",
    "    <b>روش Content Based (۱۰ نمره)</b>\n",
    "    </h2>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "\n",
    "در این روش با استفاده از مقالات قبلی که کاربر آن‌ها را پسندیده است، به کاربر مقاله‌ی جدید پیشنهاد می‌دهیم.\n",
    "\n",
    "برای اینکار ابتدا تمام مقالات پیشنهاد شده برای تمام کاربرها را سر جمع کنید. (در واقع مدلی که پیاده‌سازی می‌کنید نباید بداند که به کدام کاربر چه مقالاتی پیشنهاد شده است)\n",
    "\n",
    "سپس بردار tf-idf برای تایتل هر یک از مقالات را ایجاد کنید، و میانگین بردار مقالات مورد علاقه‌ی هر فرد را با لیستی که از مقالات جدید سر جمع کردید مقایسه کنید و ۱۰ تا از شبیه‌ترین مقالات را خروجی دهید.\n",
    "\n",
    "</font>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "class ContentBasedRecommendation:\n",
    "\n",
    "    def __init__(self, all_recommended_papers: List[Dict[str, Any]]):\n",
    "        self.data = all_recommended_papers\n",
    "        self.tf_idf = TfidfVectorizer()\n",
    "        self.recommended_paper_vectors = None\n",
    "\n",
    "    def fit(self):\n",
    "        titles = [paper['title'] for paper in self.data]\n",
    "        self.recommended_paper_vectors = self.tf_idf.fit_transform(titles)\n",
    "        return self\n",
    "\n",
    "    def predict(self, user_positive_papers: List[Dict[str, Any]]):\n",
    "        titles = [paper['title'] for paper in user_positive_papers]\n",
    "        titles_vector = self.tf_idf.transform(titles).mean(axis=0)\n",
    "\n",
    "        # calculate the similarities between the user's positive papers and all the recommended papers\n",
    "        similarities: np.ndarray = np.asarray(titles_vector @ self.recommended_paper_vectors.T).flatten()\n",
    "        # get the top 10 most similar papers\n",
    "        result = [\n",
    "            self.data[i]['paperId']\n",
    "            for i in similarities.argsort()[-10:][::-1]\n",
    "        ]\n",
    "        return result\n",
    "\n",
    "content_based_recommendation = ContentBasedRecommendation([paper\n",
    "                                                           for user in recommended_papers\n",
    "                                                           for paper in (user['recommendedPapers'] or [])]).fit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h2>\n",
    "    <b>ارزیابی سیستم‌های پیشنهادگر</b>\n",
    "    </h2>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "\n",
    "در این بخش سیستم‌های پیشنهادگری را که پیاده کرده‌اید را با استفاده از معیار nDCG و با استفاده از دادگان واقعی از علایق کاربران نسبت به مقالات جدید ارزیابی کنید و نتایج حاصل از دو روش را با هم مقایسه کنید.\n",
    "\n",
    "</font>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def calculate_nDCG(ranked_list: List[str], relevant_list: List[str], k: int = 10):\n",
    "    \"\"\"\n",
    "    Calculates the nDCG (normalized discounted cumulative gain) score given a ranked list and a relevant list.\n",
    "\n",
    "    Parameters:\n",
    "    ranked_list (List[str]): Ranked list of item IDs.\n",
    "    relevant_list (List[str]): Relevant list of item IDs.\n",
    "    k (int): The truncation position for calculating nDCG@k. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    float: The nDCG score.\n",
    "    \"\"\"\n",
    "    # Truncate the ranked and relevant lists to the specified position\n",
    "    ranked_list = ranked_list[:k]\n",
    "    relevant_list = relevant_list[:k]\n",
    "\n",
    "    # Calculate the Discounted Cumulative Gain (DCG)\n",
    "    dcg = 0.0\n",
    "    for i, item_id in enumerate(ranked_list):\n",
    "        if item_id in relevant_list:\n",
    "            relevance = 1.0 / np.log2(i + 2)  # Assign a relevance of 1.0 if the item is relevant\n",
    "            dcg += relevance\n",
    "\n",
    "    # Calculate the Ideal DCG (IDCG)\n",
    "    IDCG = 0.0\n",
    "    for i in range(k):\n",
    "        relevance = 1.0 / np.log2(i + 2)  # Assign a relevance of 1.0 if the item is relevant\n",
    "        IDCG += relevance\n",
    "\n",
    "    # Calculate the nDCG score\n",
    "    nDCG = dcg / IDCG if IDCG > 0.0 else 0.0\n",
    "    return nDCG"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collaborative filtering nDCG: 0.36167419314249255\n",
      "Content-based nDCG: 0.1290581089990145\n"
     ]
    }
   ],
   "source": [
    "collaborative_filtering_ndcg = [\n",
    "    calculate_nDCG(collaborative_filtering.predict(user['positive_papers']), [p['paperId'] for p in user['recommendedPapers']])\n",
    "    for user in test_data\n",
    "]\n",
    "content_based_ndcg = [\n",
    "    calculate_nDCG(content_based_recommendation.predict(user['positive_papers']), [p['paperId'] for p in user['recommendedPapers']])\n",
    "    for user in test_data\n",
    "]\n",
    "\n",
    "print(f'Collaborative filtering nDCG: {np.mean(collaborative_filtering_ndcg)}')\n",
    "print(f'Content-based nDCG: {np.mean(content_based_ndcg)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir=\"rtl\" style=\"text-align: justify\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>رابط کاربری (تا ۱۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در این بخش\n",
    " باید یک واسط کاربری ساده برای اجرای تعاملی بخش‌های مختلف سیستم که از فاز ۱ ساخته‌اید و همچنین مشاهده نتایج پیاده‌سازی کنید. در صورت پیاده سازی زیبا و بهتر رابط کاربری تا ده نمره نمره امتیازی نیز در نظر گرفته خواهد شد.\n",
    "</font>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[mir.salimiahmad.ir](https://mir.salimiahmad.ir/)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "08ac30a6a1fd2e576b33e03f7d61c3a285d7ee0582c2dd23dde6343ef303ebe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
