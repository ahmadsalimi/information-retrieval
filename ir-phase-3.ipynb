{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B Titr\" size=5>\n",
    "<p></p><p></p>\n",
    "بسمه تعالی\n",
    "<p></p>\n",
    "</font>\n",
    "<p></p>\n",
    "<font>\n",
    "<br>\n",
    "درس بازیابی پیشرفته اطلاعات\n",
    "<br>\n",
    "مدرس: دکتر سلیمانی\n",
    "</font>\n",
    "<p></p>\n",
    "<br>\n",
    "<font>\n",
    "<b>فاز سوم پروژه</b>\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "موعد تحویل:  ساعت ۶ صبح ۸ تیر<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font>\n",
    "دانشگاه صنعتی شریف\n",
    "<br>\n",
    "دانشکده مهندسی کامپیوتر\n",
    "<br>\n",
    "<br>\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <h1>\n",
    "    <b>مقدمه</b>\n",
    "    </h1>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     <br>\n",
    "    در این فاز از پروژه، تمرکز ما بر \n",
    "    crawling \n",
    "    و تحلیل مقالات استخراج‌شده از اینترنت خواهد بود. ما با بررسی تکنیک های مختلف  \n",
    "    web crawling\n",
    "    برای استخراج مقالات و سایر اطلاعات مرتبط از وب شروع خواهیم کرد.\n",
    "    <br>\n",
    "    در مرحله بعد، الگوریتم های تجزیه و تحلیل  لینک مانند\n",
    "    PageRank\n",
    "    و \n",
    "    HITS\n",
    "    را برای تعیین اهمیت این مقالات بر اساس نقل قول‌ها، ارجاعات یا اشکال دیگر پیوندها اعمال خواهیم‌کرد. ما همچنین یاد خواهیم‌گرفت که چگونه یک الگوریتم \n",
    "    PageRank\n",
    "    شخصی‌سازی‌شده را پیاده‌سازی کنیم که ترجیحات کاربر را برای ارائه نتایج مرتبط تر در نظر می‌گیرد.\n",
    "    <br>\n",
    "    در بخش سوم این مرحله، یک موتور جستجوی شخصی‌سازی شده را پیاده‌سازی میکنیم و یاد می‌گیریم که چگونه موتور جستجویی بسازیم که نتایجی را بر اساس ترجیحات کاربر ارائه دهد.\n",
    "    <br>\n",
    "در نهایت، ما یک \n",
    "    task \n",
    "     در مورد \n",
    "    recommendation system \n",
    "    ها خواهیم‌داشت، که در آن از تکنیک های مختلف برای توصیه مقالات یا صفحات وب به کاربران بر اساس ترجیحات و رفتار آنها استفاده خواهیم کرد.\n",
    "    <br>\n",
    "     تنها زبان قابل قبول برای پروژه پایتون است. محدودیت استفاده از کتاب‌خانه‌های آماده در هر بخش مشخص شده است. در انتهای پروژه قرار است یک سیستم یکپارچه‌ی جست‌و‌جو داشته باشید، بنابراین به پیاده‌سازی هر چه بهتر این فاز توجه داشته باشید.\n",
    "</font>\n",
    "</div>\n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>پیاده‌سازی Crawler (۴۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "\n",
    "<font face=\"XB Zar\" size=3>\n",
    "   در این بخش باید یک Crawler \n",
    "    برای واکشی اطلاعات تعدادی مقاله از سایت <a href=\"https://www.semanticscholar.org/\">Semantic Scholar</a> پیاده سازی کنید.\n",
    "   اطلاعات واکشی شده باید حاوی موارد زیر باشد.\n",
    "</font>\n",
    "</div>\n",
    "<br>\n",
    "<table dir=\"ltr\" style=\"width: 100%; border-collapse: collapse;\">\n",
    "  <tr>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">ID</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Title</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Abstract</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Publication Year</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Authors</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Related Topics</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Citation Count</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Reference Count</th>\n",
    "    <th style=\"padding: 8px; text-align: justify; border: 1px solid black;\">References</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Unique ID of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Title of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Abstract of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Publication year</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">Name of the first author, ..., Name of the last author</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">topic1, topic2, ...</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">number of citations of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">number of references of the paper</td>\n",
    "    <td style=\"padding: 8px; text-align: justify; border: 1px solid black;\">ID of the first reference, ..., ID of the tenth reference</td>\n",
    "  </tr>\n",
    "</table>\n",
    "    <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "\n",
    "<font face=\"XB Zar\" size=3>\n",
    "  ابتدا فرایند واکشی را از ۵ مقاله‌ی هر استاد شروع کنید و\n",
    "    ۱۰\n",
    "    مرجع اول هر مقاله را به صف مقالات اضافه کنید.\n",
    "    فرایند واکشی را نا جایی ادامه دهید که اطلاعات ۲۰۰۰ مقاله را داشته باشید.\n",
    "    اطلاعات مقالات را در فایل crawled_paper_profName.json ذخیره کنید.\n",
    "</font>\n",
    "</div>\n",
    "\n",
    "<div dir=\"rtl\">\n",
    "\n",
    "<font face=\"XB Zar\" size=3>\n",
    "  در پیاده سازی Crawler به موارد زیر دقت کنید.\n",
    "    \n",
    "    \n",
    "<ul>\n",
    "<li>حق استفاده از api سایت semantic scholar را ندارید.</li>\n",
    "<li>برای واکشی می‌توانید از پکیج‌هایی مثل <a href=\"https://www.selenium.dev/selenium/docs/api/py/\">Selenium</a> و یا <a href=\"https://github.com/scrapy/scrapy\">Scrapy</a>  استفاده کنید. استفاده از پکیج‌های دیگر نیز مجاز است. همچنین برای پارس اطلاعات واکشی شده می‌توانید از پکیج <a href=\"https://pypi.org/project/beautifulsoup4/\">Beautiful Soup</a> استفاده کنید.\n",
    "</li>\n",
    "<li>بین هر بار درخواست از سایت یک فاصله چند ثانیه‌ای بدهید.</li>\n",
    "<li>در زمان تحویل کد Crawler شما اجرا خواهد شد و صحت آن بررسی خواهد شد.</li>\n",
    "<li>در صورتی که ‌Crawler شما به دچار اروری مثل request timeout شد نباید کار خود را متوقف کند.</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>PageRank \n",
    "        شخصی‌سازی‌شده\n",
    "        (۲۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در این بخش، الگوریتم \n",
    "    PageRank \n",
    "    شخصی‌سازی‌شده را پیاده‌سازی می‌کنیم که توسعه‌ای از الگوریتم \n",
    "    PageRank\n",
    "    است که ترجیحات کاربر را در نظر می‌گیرد. الگوریتم \n",
    "    PageRank\n",
    "    شخصی‌سازی‌شده گره‌ها را در یک گراف بر اساس اهمیت آنها برای کاربر رتبه‌بندی می‌کند، نه بر اساس اهمیت کلی آنها در نمودار.\n",
    "    \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from tqdm import tqdm\n",
    "\n",
    "def pagerank(graph: Dict[str, List[str]], user_preferences: Dict[str, float]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Returns the personalized PageRank scores for the nodes in the graph, given the user's preferences.\n",
    "\n",
    "    Parameters:\n",
    "    graph (Dict[str, List[str]]): The graph represented as a dictionary of node IDs and their outgoing edges.\n",
    "    user_preferences (Dict[str, float]): A dictionary of node IDs and the user's preferences for those nodes.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, float]: A dictionary of node IDs and their personalized PageRank scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # Constants for the PageRank algorithm\n",
    "    damping_factor = 0.85\n",
    "    convergence_threshold = 0.0001\n",
    "    max_iterations = 100\n",
    "\n",
    "    # Initialize the PageRank scores with equal probabilities\n",
    "    num_nodes = len(graph)\n",
    "    initial_score = 1.0 / num_nodes\n",
    "    pagerank_scores = {node: initial_score for node in graph}\n",
    "\n",
    "    # Convert user preferences to personalized teleportation probabilities\n",
    "    teleportation_probs = {}\n",
    "    total_preference = sum(user_preferences.values())\n",
    "    if total_preference > 0:\n",
    "        for node, preference in user_preferences.items():\n",
    "            teleportation_probs[node] = preference / total_preference\n",
    "\n",
    "    incoming_graph = {}\n",
    "    for node, outgoing_nodes in graph.items():\n",
    "        for outgoing_node in outgoing_nodes:\n",
    "            if outgoing_node not in incoming_graph:\n",
    "                incoming_graph[outgoing_node] = []\n",
    "            incoming_graph[outgoing_node].append(node)\n",
    "\n",
    "    # Iteratively calculate the PageRank scores\n",
    "    with tqdm(range(max_iterations), total=max_iterations, desc=\"Calculating PageRank\") as pbar:\n",
    "        for _ in pbar:\n",
    "            new_scores = {}\n",
    "            for node in graph:\n",
    "                new_score = (1 - damping_factor) / num_nodes\n",
    "\n",
    "                # Consider incoming edges to calculate the new score\n",
    "                for incoming_node in incoming_graph.get(node, []):\n",
    "                    new_score += damping_factor * pagerank_scores[incoming_node] / len(graph[incoming_node])\n",
    "\n",
    "                # Apply personalized teleportation if available\n",
    "                if node in teleportation_probs:\n",
    "                    new_score += (1 - damping_factor) * teleportation_probs[node]\n",
    "\n",
    "                new_scores[node] = new_score\n",
    "\n",
    "            # Check for convergence\n",
    "            convergence = sum(abs(new_scores[node] - pagerank_scores[node]) for node in graph)\n",
    "            pbar.set_postfix({\"loss\": convergence})\n",
    "            if convergence < convergence_threshold:\n",
    "                break\n",
    "\n",
    "            pagerank_scores = new_scores\n",
    "\n",
    "    return pagerank_scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در این بخش از الگوریتم \n",
    "PageRank\n",
    "شخصی‌سازی‌شده که در قسمت قبلی پیاده‌سازی شده‌است برای\n",
    "شناسایی مقالات مهم مرتبط با حوزه‌ی کاری یک استاد \n",
    "خاص استفاده می‌کنیم. این تابع، یک \n",
    "    field \n",
    "    را به عنوان ورودی دریافت می‌کند. خروجی نیز\n",
    "مقالات برتری که بیشترین ارتباط را با آن زمینه دارند؛ خواهدبود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "professors = {\n",
    "    'Kasaei',\n",
    "    'Rabiee',\n",
    "    'Rohban',\n",
    "    'Sharifi',\n",
    "    'Soleymani'\n",
    "}\n",
    "papers_by_id = {}\n",
    "papers_by_professor = {}\n",
    "for professor in professors:\n",
    "    with open(f'results/crawled_paper_{professor}.json') as f:\n",
    "        papers_by_professor[professor] = json.load(f)\n",
    "        for paper in papers_by_professor[professor]:\n",
    "            papers_by_id[paper['id']] = paper\n",
    "\n",
    "all_papers = list(papers_by_id.values())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def important_articles(professor: str, topk: int = 10) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns the most important articles in the field of given professor, based on the personalized PageRank scores.\n",
    "\n",
    "    Parameters:\n",
    "    Professor (str): Professor's name.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of article IDs representing the most important articles in the field of given professor.\n",
    "    \"\"\"\n",
    "\n",
    "    graph = {}\n",
    "    for paper in all_papers:\n",
    "        graph[paper['id']] = paper['references']\n",
    "\n",
    "    # Define the user preferences based on the professor's seed papers\n",
    "    user_preferences = {\n",
    "        paper['id']: 1.0\n",
    "        for paper in papers_by_professor[professor]\n",
    "    }\n",
    "\n",
    "    # Calculate the personalized PageRank scores\n",
    "    pagerank_scores = pagerank(graph, user_preferences)\n",
    "\n",
    "    # Sort the articles based on their PageRank scores\n",
    "    sorted_articles = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)[:topk]\n",
    "\n",
    "    # Return only the article IDs\n",
    "    most_important_articles = [article_id for article_id, _ in sorted_articles]\n",
    "\n",
    "    return most_important_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def print_paper(id_: str):\n",
    "    paper = papers_by_id[id_]\n",
    "    width = 100\n",
    "    print(f' {paper[\"title\"]} '.center(width, '='))\n",
    "    print(f' Authors '.center(width, '-'))\n",
    "    print(f' {\", \".join(paper[\"authors\"])} '.center(width, '-'))\n",
    "    print(f' Year: {paper[\"publication_year\"]} '.center(width, '-'))\n",
    "    print(f' Topics: {\", \".join(paper[\"related_topics\"])} '.center(width, '-'))\n",
    "    print(f' {paper[\"citation_count\"]} citations '.center(width, '-'))\n",
    "    print(f' {paper[\"reference_count\"]} references '.center(width, '-'))\n",
    "    print(f' Abstract '.center(width, '-'))\n",
    "    print(paper['abstract'])\n",
    "    print('=' * width)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating PageRank:   8%|▊         | 8/100 [00:00<00:01, 70.85it/s, loss=6.29e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= ImageNet classification with deep convolutional neural networks ==================\n",
      "--------------------------------------------- Authors ----------------------------------------------\n",
      "------------------------ A. Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton -------------------------\n",
      "-------------------------------------------- Year: 2012 --------------------------------------------\n",
      "------------------------------------- Topics: Computer Science -------------------------------------\n",
      "----------------------------------------- 97952 citations ------------------------------------------\n",
      "------------------------------------------ 44 references -------------------------------------------\n",
      "--------------------------------------------- Abstract ---------------------------------------------\n",
      "A large, deep convolutional neural network was trained to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes and employed a recently developed regularization method called &quot;dropout&quot; that proved to be very effective. We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called &quot;dropout&quot; that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.\n",
      "====================================================================================================\n",
      "========================== A Fast Learning Algorithm for Deep Belief Nets ==========================\n",
      "--------------------------------------------- Authors ----------------------------------------------\n",
      "---------------------------- Geoffrey E. Hinton, Simon Osindero, Y. Teh ----------------------------\n",
      "-------------------------------------------- Year: 2006 --------------------------------------------\n",
      "------------------------------------- Topics: Computer Science -------------------------------------\n",
      "----------------------------------------- 14765 citations ------------------------------------------\n",
      "------------------------------------------ 30 references -------------------------------------------\n",
      "--------------------------------------------- Abstract ---------------------------------------------\n",
      "A fast, greedy algorithm is derived that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.\n",
      "====================================================================================================\n",
      "============================== Atomic Decomposition by Basis Pursuit ===============================\n",
      "--------------------------------------------- Authors ----------------------------------------------\n",
      "--------------------------------- S. Chen, D. Donoho, M. Saunders ----------------------------------\n",
      "-------------------------------------------- Year: 1998 --------------------------------------------\n",
      "------------------------------------- Topics: Computer Science -------------------------------------\n",
      "----------------------------------------- 11373 citations ------------------------------------------\n",
      "------------------------------------------ 50 references -------------------------------------------\n",
      "--------------------------------------------- Abstract ---------------------------------------------\n",
      "Basis Pursuit (BP) is a principle for decomposing a signal into an &quot;optimal&quot; superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions. The time-frequency and time-scale communities have recently developed a large number of overcomplete waveform dictionaries --- stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the method of frames (MOF), Matching pursuit (MP), and, for special dictionaries, the best orthogonal basis (BOB). \n",
      "Basis Pursuit (BP) is a principle for decomposing a signal into an &quot;optimal&quot; superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP, and BOB, including better sparsity and superresolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, in abstract harmonic analysis, total variation denoising, and multiscale edge denoising. \n",
      "BP in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver.\n",
      "====================================================================================================\n",
      "=================================== Generative Adversarial Nets ====================================\n",
      "--------------------------------------------- Authors ----------------------------------------------\n",
      " Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, Yoshua Bengio \n",
      "-------------------------------------------- Year: 2014 --------------------------------------------\n",
      "------------------------------------- Topics: Computer Science -------------------------------------\n",
      "----------------------------------------- 37362 citations ------------------------------------------\n",
      "------------------------------------------ 38 references -------------------------------------------\n",
      "--------------------------------------------- Abstract ---------------------------------------------\n",
      "A new framework for estimating generative models via an adversarial process, in which two models are simultaneously train: a generative model G that captures the data distribution and a discriminative model D that estimates the probability that a sample came from the training data rather than G. We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to ½ everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.\n",
      "====================================================================================================\n",
      "========= Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation =========\n",
      "--------------------------------------------- Authors ----------------------------------------------\n",
      "--------------------- Ross B. Girshick, Jeff Donahue, Trevor Darrell, J. Malik ---------------------\n",
      "-------------------------------------------- Year: 2013 --------------------------------------------\n",
      "------------------------------------- Topics: Computer Science -------------------------------------\n",
      "----------------------------------------- 21160 citations ------------------------------------------\n",
      "------------------------------------------ 56 references -------------------------------------------\n",
      "--------------------------------------------- Abstract ---------------------------------------------\n",
      "This paper proposes a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.\n",
      "====================================================================================================\n",
      "====================== Learning Multiple Layers of Features from Tiny Images =======================\n",
      "--------------------------------------------- Authors ----------------------------------------------\n",
      "------------------------------------------ A. Krizhevsky -------------------------------------------\n",
      "-------------------------------------------- Year: 2009 --------------------------------------------\n",
      "------------------------------------- Topics: Computer Science -------------------------------------\n",
      "----------------------------------------- 23315 citations ------------------------------------------\n",
      "------------------------------------------ 15 references -------------------------------------------\n",
      "--------------------------------------------- Abstract ---------------------------------------------\n",
      "It is shown how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex, using a novel parallelization algorithm to distribute the work among multiple machines connected on a network. Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it dicult to learn a good set of lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is signicantly improved by pre-training a layer of features on a large set of unlabeled tiny images.\n",
      "====================================================================================================\n",
      "======= Near-Optimal Signal Recovery From Random Projections: Universal Encoding Strategies? =======\n",
      "--------------------------------------------- Authors ----------------------------------------------\n",
      "---------------------------------------- E. Candès, T. Tao -----------------------------------------\n",
      "-------------------------------------------- Year: 2004 --------------------------------------------\n",
      "------------------------------------- Topics: Computer Science -------------------------------------\n",
      "------------------------------------------ 6835 citations ------------------------------------------\n",
      "------------------------------------------ 60 references -------------------------------------------\n",
      "--------------------------------------------- Abstract ---------------------------------------------\n",
      "If the objects of interest are sparse in a fixed basis or compressible, then it is possible to reconstruct f to within very high accuracy from a small number of random measurements by solving a simple linear program. Suppose we are given a vector f in a class FsubeRopf&lt;sup&gt;N &lt;/sup&gt;, e.g., a class of digital signals or digital images. How many linear measurements do we need to make about f to be able to recover f to within precision epsi in the Euclidean (lscr&lt;sub&gt;2&lt;/sub&gt;) metric? This paper shows that if the objects of interest are sparse in a fixed basis or compressible, then it is possible to reconstruct f to within very high accuracy from a small number of random measurements by solving a simple linear program. More precisely, suppose that the nth largest entry of the vector |f| (or of its coefficients in a fixed basis) obeys |f|&lt;sub&gt;(n)&lt;/sub&gt;lesRmiddotn&lt;sup&gt;-1&lt;/sup&gt;p/, where R&gt;0 and p&gt;0. Suppose that we take measurements y&lt;sub&gt;k&lt;/sub&gt;=langf&lt;sup&gt;# &lt;/sup&gt;,X&lt;sub&gt;k&lt;/sub&gt;rang,k=1,...,K, where the X&lt;sub&gt;k&lt;/sub&gt; are N-dimensional Gaussian vectors with independent standard normal entries. Then for each f obeying the decay estimate above for some 0&lt;p&lt;1 and with overwhelming probability, our reconstruction f&lt;sup&gt;t&lt;/sup&gt;, defined as the solution to the constraints y&lt;sub&gt;k&lt;/sub&gt;=langf&lt;sup&gt;# &lt;/sup&gt;,X&lt;sub&gt;k&lt;/sub&gt;rang with minimal lscr&lt;sub&gt;1&lt;/sub&gt; norm, obeys parf-f&lt;sup&gt;#&lt;/sup&gt;par&lt;sub&gt;lscr2&lt;/sub&gt;lesC&lt;sub&gt;p &lt;/sub&gt;middotRmiddot(K/logN)&lt;sup&gt;-r&lt;/sup&gt;, r=1/p-1/2. There is a sense in which this result is optimal; it is generally impossible to obtain a higher accuracy from any set of K measurements whatsoever. The methodology extends to various other random measurement ensembles; for example, we show that similar results hold if one observes a few randomly sampled Fourier coefficients of f. In fact, the results are quite general and require only two hypotheses on the measurement ensemble which are detailed\n",
      "====================================================================================================\n",
      "=== Optimally sparse representation in general (nonorthogonal) dictionaries via ℓ1 minimization ====\n",
      "--------------------------------------------- Authors ----------------------------------------------\n",
      "------------------------------------- D. Donoho, Michael Elad --------------------------------------\n",
      "-------------------------------------------- Year: 2003 --------------------------------------------\n",
      "------------------------------------- Topics: Computer Science -------------------------------------\n",
      "------------------------------------------ 2984 citations ------------------------------------------\n",
      "------------------------------------------ 31 references -------------------------------------------\n",
      "--------------------------------------------- Abstract ---------------------------------------------\n",
      "This article obtains parallel results in a more general setting, where the dictionary D can arise from two or several bases, frames, or even less structured systems, and sketches three applications: separating linear features from planar ones in 3D data, noncooperative multiuser encoding, and identification of over-complete independent component models. Given a dictionary D = {dk} of vectors dk, we seek to represent a signal S as a linear combination S = ∑k γ(k)dk, with scalar coefficients γ(k). In particular, we aim for the sparsest representation possible. In general, this requires a combinatorial optimization process. Previous work considered the special case where D is an overcomplete system consisting of exactly two orthobases and has shown that, under a condition of mutual incoherence of the two bases, and assuming that S has a sufficiently sparse representation, this representation is unique and can be found by solving a convex optimization problem: specifically, minimizing the ℓ1 norm of the coefficients γ̱. In this article, we obtain parallel results in a more general setting, where the dictionary D can arise from two or several bases, frames, or even less structured systems. We sketch three applications: separating linear features from planar ones in 3D data, noncooperative multiuser encoding, and identification of over-complete independent component models.\n",
      "====================================================================================================\n",
      "========= Stable recovery of sparse overcomplete representations in the presence of noise ==========\n",
      "--------------------------------------------- Authors ----------------------------------------------\n",
      "------------------------------ D. Donoho, Michael Elad, V. Temlyakov -------------------------------\n",
      "-------------------------------------------- Year: 2006 --------------------------------------------\n",
      "------------------------------------- Topics: Computer Science -------------------------------------\n",
      "------------------------------------------ 2272 citations ------------------------------------------\n",
      "------------------------------------------ 46 references -------------------------------------------\n",
      "--------------------------------------------- Abstract ---------------------------------------------\n",
      "This paper establishes the possibility of stable recovery under a combination of sufficient sparsity and favorable structure of the overcomplete system and shows that similar stability is also available using the basis and the matching pursuit algorithms. Overcomplete representations are attracting interest in signal processing theory, particularly due to their potential to generate sparse representations of signals. However, in general, the problem of finding sparse representations must be unstable in the presence of noise. This paper establishes the possibility of stable recovery under a combination of sufficient sparsity and favorable structure of the overcomplete system. Considering an ideal underlying signal that has a sufficiently sparse representation, it is assumed that only a noisy version of it can be observed. Assuming further that the overcomplete system is incoherent, it is shown that the optimally sparse approximation to the noisy data differs from the optimally sparse decomposition of the ideal noiseless signal by at most a constant multiple of the noise level. As this optimal-sparsity method requires heavy (combinatorial) computational effort, approximation algorithms are considered. It is shown that similar stability is also available using the basis and the matching pursuit algorithms. Furthermore, it is shown that these methods result in sparse approximation of the noisy data that contains only terms also appearing in the unique sparsest representation of the ideal noiseless sparse signal.\n",
      "====================================================================================================\n",
      "========================= Regression Shrinkage and Selection via the Lasso =========================\n",
      "--------------------------------------------- Authors ----------------------------------------------\n",
      "------------------------------------------ R. Tibshirani -------------------------------------------\n",
      "-------------------------------------------- Year: 1996 --------------------------------------------\n",
      "------------------------------------- Topics: Computer Science -------------------------------------\n",
      "----------------------------------------- 40956 citations ------------------------------------------\n",
      "------------------------------------------ 21 references -------------------------------------------\n",
      "--------------------------------------------- Abstract ---------------------------------------------\n",
      "A new method for estimation in linear models called the lasso, which minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant, is proposed. SUMMARY We propose a new method for estimation in linear models. The &#39;lasso&#39; minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for id_ in important_articles('Rohban'):\n",
    "    print_paper(id_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>جستجو شخصی‌سازی‌شده (۱۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "الگوریتم جست‌و‌جویی که در فازهای گذشته پیاده‌سازی کرده‌اید را به گونه‌ای تغییر دهید که نتایج به دست آمده جست‌و‌جو بر حسب علایق فرد مرتب شوند. از قضیه‌ی خطی بودن برای این کار استفاده کنید.\n",
    "    \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from functools import lru_cache\n",
    "import glob\n",
    "import math\n",
    "from termcolor import colored\n",
    "import os\n",
    "from typing import Dict, Iterable, Literal, Union, Set, Optional, List, Tuple\n",
    "\n",
    "from nltk.metrics import distance as nltkd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Token:\n",
    "    processed: str\n",
    "    actual: str\n",
    "    i: int\n",
    "    idx: int\n",
    "\n",
    "    @staticmethod\n",
    "    def from_spacy_token(token) -> \"Token\":\n",
    "        return Token(token.lemma_.lower(), token.text, token.i, token.idx)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.processed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def batch_clean_data(texts: List[str], batch_size: int = 128) -> List[List[Token]]:\n",
    "    \"\"\"Preprocesses the text with tokenization, case folding, stemming and lemmatization, and punctuations\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : List[str]\n",
    "        A list of titles or abstracts of articles\n",
    "    batch_size : int, optional\n",
    "        The number of texts to be processed at a time, by default 128\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[List[Doc]]\n",
    "        A list of lists of tokens\n",
    "    \"\"\"\n",
    "    tokens = nlp.pipe(texts, batch_size=batch_size, n_process=os.cpu_count())\n",
    "    return [[Token.from_spacy_token(token) for token in doc if not token.is_punct] for doc in tokens]\n",
    "\n",
    "\n",
    "def clean_data(text: str) -> List[Token]:\n",
    "    \"\"\"Preprocesses the text with tokenization, case folding, stemming and lemmatization, and punctuations\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The title or abstract of an article\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of tokens\n",
    "    \"\"\"\n",
    "    return batch_clean_data([text])[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def find_stop_words(all_text: List[str], num_token: int = 30) -> Set[str]:\n",
    "    \"\"\"Detects stop-words\n",
    "\n",
    "     Parameters\n",
    "    ----------\n",
    "    all_text : list of all tokens\n",
    "        (result of clean_data(text) for all the text)\n",
    "\n",
    "    num_token : int\n",
    "        number of stop words to be detected\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Return Value is optional but must print the stop words and number of their occurence\n",
    "    \"\"\"\n",
    "    counter = Counter(all_text)\n",
    "    most_occur = counter.most_common(num_token)\n",
    "    print(pd.DataFrame(most_occur, columns=['token', 'count']))\n",
    "    return set([token for token, _ in most_occur])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "\n",
    "    def __init__(self, papers_by_professor: Dict[str, List[dict]],\n",
    "                 all_papers: List[dict], stop_topk: int = 30):\n",
    "        self.papers_by_professor = {\n",
    "            professor: [paper['id'] for paper in papers]\n",
    "            for professor, papers in papers_by_professor.items()\n",
    "        }\n",
    "        self.data = pd.DataFrame(all_papers)\n",
    "        self.stop_topk = stop_topk\n",
    "\n",
    "    @property\n",
    "    @lru_cache\n",
    "    def cleaned_documents(self) -> Dict[str, Dict[str, List[Token]]]:\n",
    "        return {\n",
    "            paper_id: {\n",
    "                'title': cleaned_title,\n",
    "                'abstract': cleaned_abstract,\n",
    "            }\n",
    "            for paper_id, cleaned_title, cleaned_abstract in zip(\n",
    "                self.data['id'].tolist(),\n",
    "                batch_clean_data(self.data['title'].tolist()),\n",
    "                batch_clean_data(self.data['abstract'].tolist()),\n",
    "            )\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    @lru_cache\n",
    "    def stop_tokens(self) -> Set[str]:\n",
    "        return find_stop_words(\n",
    "            [token.processed\n",
    "             for tokens in self.cleaned_documents.values()\n",
    "             for token in tokens['title'] + tokens['abstract']],\n",
    "            num_token=self.stop_topk,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    @lru_cache\n",
    "    def non_stop_documents(self) -> Dict[str, Dict[str, List[Token]]]:\n",
    "        return {\n",
    "            paper_id: {\n",
    "                'title': [token for token in tokens['title'] if token.processed not in self.stop_tokens],\n",
    "                'abstract': [token for token in tokens['abstract'] if token.processed not in self.stop_tokens],\n",
    "            }\n",
    "            for paper_id, tokens in self.cleaned_documents.items()\n",
    "        }\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "class TrieNode:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.children: Dict[str, TrieNode] = {}\n",
    "        self.idx_in_doc: Dict[str, Dict[Literal['title', 'abstract'], Set[int]]] = {}\n",
    "        self.is_end = False\n",
    "\n",
    "    def insert(self, doc_id: str, doc_section: Literal['title', 'abstract'], token: Token):\n",
    "        node = self\n",
    "        for char in token.processed:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "            node = node.children[char]\n",
    "            if doc_id not in node.idx_in_doc:\n",
    "                node.idx_in_doc[doc_id] = {'title': set(), 'abstract': set()}\n",
    "            node.idx_in_doc[doc_id][doc_section].add(token.idx)\n",
    "        node.is_end = True\n",
    "\n",
    "    def search(self, token: str) -> Optional[\n",
    "        Dict[str, Dict[Literal['title', 'abstract'], Set[int]]]]:\n",
    "        node = self\n",
    "        for char in token:\n",
    "            if char not in node.children:\n",
    "                return None\n",
    "            node = node.children[char]\n",
    "        if node.is_end:\n",
    "            return node.idx_in_doc\n",
    "        return None\n",
    "\n",
    "    def remove_document(self, doc_id: str):\n",
    "        if doc_id in self.idx_in_doc:\n",
    "            del self.idx_in_doc[doc_id]\n",
    "        for child in self.children.values():\n",
    "            child.remove_document(doc_id)\n",
    "\n",
    "    def traverse_words(self, prefix: str = '') -> Iterable[Tuple[str,\n",
    "    Dict[str, Dict[Literal['title', 'abstract'], Set[int]]]]]:\n",
    "        if self.is_end:\n",
    "            yield prefix, {\n",
    "                doc_id: {\n",
    "                    k: list(v) for k, v in doc.items()\n",
    "                }\n",
    "                for doc_id, doc in self.idx_in_doc.items()\n",
    "            }\n",
    "\n",
    "        for char, child in self.children.items():\n",
    "            yield from child.traverse_words(prefix + char)\n",
    "\n",
    "    @classmethod\n",
    "    def from_words(cls, words: Iterable[Tuple[str,\n",
    "    Dict[str, Dict[Literal['title', 'abstract'], Iterable[int]]]]]) -> 'TrieNode':\n",
    "        root = cls()\n",
    "        for word, idx_in_doc in words:\n",
    "            for doc_id, doc in idx_in_doc.items():\n",
    "                for doc_section, indices in doc.items():\n",
    "                    for idx in indices:\n",
    "                        token = Token(word, '', -1, idx)\n",
    "                        root.insert(doc_id, doc_section, token)\n",
    "        return root\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'children': {char: child.to_dict() for char, child in self.children.items()},\n",
    "            'idx_in_doc': self.idx_in_doc,\n",
    "            'is_end': self.is_end,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, data: Dict):\n",
    "        node = cls()\n",
    "        node.children = {char: cls.from_dict(child) for char, child in data['children'].items()}\n",
    "        node.i_in_doc = data['i_in_doc']\n",
    "        node.is_end = data['is_end']\n",
    "        return node"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "WORD_BOUNDARY_CHAR = '¶'\n",
    "\n",
    "\n",
    "def get_word_bigrams(word: str) -> Iterable[str]:\n",
    "    \"\"\"\n",
    "    Returns the bigrams of the given word\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    word: str\n",
    "        The word to get the bigrams from\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of bigrams\n",
    "    \"\"\"\n",
    "    word = WORD_BOUNDARY_CHAR + word + WORD_BOUNDARY_CHAR\n",
    "    return [word[i:i + 2] for i in range(len(word) - 1)]\n",
    "\n",
    "\n",
    "def create_bigram_index(corpus: Corpus) -> Dict[str, Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Creates a bigram index for the spell correction\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: Corpus\n",
    "        The corpus to generate the bigram index from\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary of bigrams and their occurence\n",
    "    \"\"\"\n",
    "    bigram_index: Dict[str, Dict[str, int]] = {}\n",
    "    seen_words = set()\n",
    "    for doc_id, doc in corpus.cleaned_documents.items():\n",
    "        for section_name, doc_section in doc.items():\n",
    "            for token in doc_section:\n",
    "                if token.actual in seen_words:\n",
    "                    continue\n",
    "                seen_words.add(token.actual)\n",
    "                for bigram in get_word_bigrams(token.actual):\n",
    "                    if bigram not in bigram_index:\n",
    "                        bigram_index[bigram] = {}\n",
    "                    if token.actual not in bigram_index[bigram]:\n",
    "                        bigram_index[bigram][token.actual] = 0\n",
    "                    bigram_index[bigram][token.actual] += 1\n",
    "    return bigram_index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def correct_text(bigram_index: Dict[str, Dict[str, int]], text: str, similar_words_limit: int = 20) -> str:\n",
    "    \"\"\"\n",
    "    Correct the give query text, if it is misspelled\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    bigram_index: Dict[str, Dict[str, int]]\n",
    "        The bigram index to search in\n",
    "    text: str\n",
    "        The query text\n",
    "    similar_words_limit: int\n",
    "        The number of similar words\n",
    "\n",
    "    Returns\n",
    "    ---------\n",
    "    str\n",
    "        The corrected form of the given text\n",
    "    \"\"\"\n",
    "    corrected_text = ''.join(text)\n",
    "    for token in nlp(text):\n",
    "        word = token.text\n",
    "        if token.is_punct:\n",
    "            continue\n",
    "        word_occurences: Dict[str, int] = {}\n",
    "        for bigram in get_word_bigrams(word):\n",
    "            for posting, occurence in bigram_index.get(bigram, {}).items():\n",
    "                if posting not in word_occurences:\n",
    "                    word_occurences[posting] = 0\n",
    "                word_occurences[posting] += occurence\n",
    "        jaccard_scores = {\n",
    "            posting: word_occurence / (len(word) + len(posting) + 2 - word_occurence)\n",
    "            for posting, word_occurence in word_occurences.items()\n",
    "        }\n",
    "        similar_words = sorted(jaccard_scores, key=jaccard_scores.get, reverse=True)[:similar_words_limit]\n",
    "        min_edit_distance = float('inf')\n",
    "        corrected_word = word\n",
    "        for similar_word in similar_words:\n",
    "            if (edit_distance := nltkd.edit_distance(similar_word, word)) < min_edit_distance:\n",
    "                min_edit_distance = edit_distance\n",
    "                corrected_word = similar_word\n",
    "        corrected_text = corrected_text.replace(word, corrected_word)\n",
    "    return corrected_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "def construct_positional_indexes(corpus: Corpus):\n",
    "    \"\"\"\n",
    "    Get processed data and insert words in that into a trie and construct postional_index and posting lists after wards.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: Corpus\n",
    "        processed data\n",
    "\n",
    "    Return\n",
    "    ----------\n",
    "    docs:\n",
    "        list of docs with specificied id, title, abstract.\n",
    "    \"\"\"\n",
    "    trie = TrieNode()\n",
    "    for doc_id, doc in corpus.non_stop_documents.items():\n",
    "        for doc_section, tokens in doc.items():\n",
    "            for token in tokens:\n",
    "                trie.insert(doc_id, doc_section, token)\n",
    "    return trie"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def write_json(o, path: str):\n",
    "    import json\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(o, f)\n",
    "\n",
    "\n",
    "def get_offset(gap): return bin(gap)[3:]\n",
    "\n",
    "\n",
    "def get_length(offset): return unary_codification(len(offset)) + '0'\n",
    "\n",
    "\n",
    "def unary_codification(gap): return '1' * gap\n",
    "\n",
    "\n",
    "def get_gaps_list(posting_lists): return [posting_lists[0]] + [posting_lists[i] - posting_lists[i - 1] for i in\n",
    "                                                               range(1, len(posting_lists))]\n",
    "\n",
    "\n",
    "def gamma_encoding(postings):\n",
    "    if not postings:\n",
    "        return ''\n",
    "    postings = [p + 1 for p in postings]\n",
    "    return ''.join([get_length(get_offset(gap)) + get_offset(gap) for gap in get_gaps_list(postings)])\n",
    "\n",
    "\n",
    "def write_gamma_code(words: Iterable[Tuple[str, Dict[str, Dict[Literal['title', 'abstract'], Iterable[int]]]]], path: str):\n",
    "    with open(path, 'wb') as f:\n",
    "        for word, posting_lists in words:\n",
    "            encoded_word = word.encode('utf-8')\n",
    "            f.write(len(encoded_word).to_bytes(1, 'big'))\n",
    "            f.write(encoded_word)\n",
    "            f.write(len(posting_lists).to_bytes(4, 'big'))\n",
    "            for doc_id, doc in posting_lists.items():\n",
    "                encoded_doc_id = doc_id.encode('utf-8')\n",
    "                f.write(len(encoded_doc_id).to_bytes(1, 'big'))\n",
    "                f.write(encoded_doc_id)\n",
    "                title_postings = sorted(doc['title'])\n",
    "                encoded_title_postings = gamma_encoding(title_postings)\n",
    "                encoding_length = len(encoded_title_postings)\n",
    "                octet_encoded_title_postings = '0b' + \\\n",
    "                                               '0' * (8 - len(encoded_title_postings) % 8) + \\\n",
    "                                               encoded_title_postings\n",
    "                n_bytes = len(octet_encoded_title_postings) // 8\n",
    "                f.write(n_bytes.to_bytes(1, 'big'))\n",
    "                f.write(encoding_length.to_bytes(2, 'big'))\n",
    "                f.write(int(octet_encoded_title_postings, 2).to_bytes(n_bytes, 'big'))\n",
    "\n",
    "                encoded_abstract_postings = gamma_encoding(sorted(doc['abstract']))\n",
    "                encoding_length = len(encoded_abstract_postings)\n",
    "                octet_encoded_abstract_postings = '0b' + \\\n",
    "                                                  '0' * (8 - len(encoded_abstract_postings) % 8) + \\\n",
    "                                                  encoded_abstract_postings\n",
    "                n_bytes = len(octet_encoded_abstract_postings) // 8\n",
    "                f.write(n_bytes.to_bytes(1, 'big'))\n",
    "                f.write(encoding_length.to_bytes(2, 'big'))\n",
    "                f.write(int(octet_encoded_abstract_postings, 2).to_bytes(n_bytes, 'big'))\n",
    "\n",
    "\n",
    "def vb_encoding_number(n):\n",
    "    b = []\n",
    "    while True:\n",
    "        b = [n % 128] + b\n",
    "        n = n // 128\n",
    "        if n == 0:\n",
    "            break\n",
    "    b[-1] += 128\n",
    "    return bytes(b)\n",
    "\n",
    "\n",
    "def vb_encoding(numbers):\n",
    "    return b''.join([vb_encoding_number(n) for n in numbers])\n",
    "\n",
    "\n",
    "def write_vb_code(words: Iterable[Tuple[str, Dict[str, Dict[Literal['title', 'abstract'], Iterable[int]]]]], path: str):\n",
    "    with open(path, 'wb') as f:\n",
    "        for word, posting_lists in words:\n",
    "            encoded_word = word.encode('utf-8')\n",
    "            f.write(len(encoded_word).to_bytes(1, 'big'))\n",
    "            f.write(encoded_word)\n",
    "            f.write(len(posting_lists).to_bytes(4, 'big'))\n",
    "            for doc_id, doc in posting_lists.items():\n",
    "                encoded_doc_id = doc_id.encode('utf-8')\n",
    "                f.write(len(encoded_doc_id).to_bytes(1, 'big'))\n",
    "                f.write(encoded_doc_id)\n",
    "                title_postings = doc['title']\n",
    "                encoded_title_postings = vb_encoding(title_postings)\n",
    "                encoding_length = len(encoded_title_postings)\n",
    "                f.write(encoding_length.to_bytes(4, 'big'))\n",
    "                f.write(encoded_title_postings)\n",
    "\n",
    "                abstract_postings = doc['abstract']\n",
    "                encoded_abstract_postings = vb_encoding(abstract_postings)\n",
    "                encoding_length = len(encoded_abstract_postings)\n",
    "                f.write(encoding_length.to_bytes(4, 'big'))\n",
    "                f.write(encoded_abstract_postings)\n",
    "\n",
    "\n",
    "def store_index(trie: TrieNode, path: str, compression_type: str) -> int:\n",
    "    \"\"\"Stores the index in a file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trie: TrieNode\n",
    "        trie of corpus\n",
    "    path : str\n",
    "        Path to store the file\n",
    "    compression_type : str\n",
    "        Could be one of the followings:\n",
    "        - no-compression\n",
    "        - gamma-code\n",
    "        - variable-byte\n",
    "\n",
    "    Return\n",
    "    ----------\n",
    "    int\n",
    "        The size of the stored file\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    word_list = trie.traverse_words()\n",
    "    if compression_type == \"no-compression\":\n",
    "        write_json(list(word_list), path)\n",
    "    elif compression_type == 'gamma-code':\n",
    "        write_gamma_code(word_list, path)\n",
    "    elif compression_type == 'variable-byte':\n",
    "        write_vb_code(word_list, path)\n",
    "    else:\n",
    "        raise ValueError(\"compression_type should be one of the followings: no-compression, gamma-code, variable-byte\")\n",
    "    return os.stat(path).st_size\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "def unary_decodification(gap): return sum(map(int, gap))\n",
    "\n",
    "\n",
    "def gamma_decoding(gamma):\n",
    "    num, length, offset, aux, res = 0, \"\", \"\", 0, []\n",
    "    while gamma != \"\":\n",
    "        aux = gamma.find(\"0\")\n",
    "        length = gamma[:aux]\n",
    "        if length == \"\":\n",
    "            res.append(1); gamma = gamma[1:]\n",
    "        else:\n",
    "            offset = \"1\" + gamma[aux + 1:aux + 1 + unary_decodification(length)]\n",
    "            res.append(int(offset, 2))\n",
    "            gamma = gamma[aux + 1 + unary_decodification(length):]\n",
    "    return [int(p) - 1 for p in np.cumsum(res)]\n",
    "\n",
    "\n",
    "def read_gamma_code(path: str) -> Iterable[Tuple[str, Dict[str, Dict[Literal['title', 'abstract'], Iterable[int]]]]]:\n",
    "    with open(path, 'rb') as f:\n",
    "        while True:\n",
    "            word_length = f.read(1)\n",
    "            if not word_length:\n",
    "                break\n",
    "            word_length = int.from_bytes(word_length, 'big')\n",
    "            word = f.read(word_length).decode('utf-8')\n",
    "            n_posting_lists = int.from_bytes(f.read(4), 'big')\n",
    "            posting_lists = {}\n",
    "            for _ in range(n_posting_lists):\n",
    "                doc_id_length = int.from_bytes(f.read(1), 'big')\n",
    "                doc_id = f.read(doc_id_length).decode('utf-8')\n",
    "                title_postings_n_bytes = int.from_bytes(f.read(1), 'big')\n",
    "                title_posting_length = int.from_bytes(f.read(2), 'big')\n",
    "                title_postings_int = int.from_bytes(f.read(title_postings_n_bytes), 'big')\n",
    "                title_postings_bin = bin(title_postings_int)\n",
    "                if title_posting_length == 0:\n",
    "                    title_postings = []\n",
    "                else:\n",
    "                    aligned_title_postings_bin = '0' * (title_posting_length - len(title_postings_bin[2:])) + title_postings_bin[2:]\n",
    "                    title_postings = gamma_decoding(aligned_title_postings_bin)\n",
    "\n",
    "                abstract_postings_n_bytes = int.from_bytes(f.read(1), 'big')\n",
    "                abstract_posting_length = int.from_bytes(f.read(2), 'big')\n",
    "                abstract_postings_int = int.from_bytes(f.read(abstract_postings_n_bytes), 'big')\n",
    "                abstract_postings_bin = bin(abstract_postings_int)\n",
    "                if abstract_posting_length == 0:\n",
    "                    abstract_postings = []\n",
    "                else:\n",
    "                    aligned_abstract_postings_bin = '0' * (abstract_posting_length - len(abstract_postings_bin[2:])) + abstract_postings_bin[2:]\n",
    "                    abstract_postings = gamma_decoding(aligned_abstract_postings_bin)\n",
    "\n",
    "                posting_lists[doc_id] = {\n",
    "                    'title': title_postings,\n",
    "                    'abstract': abstract_postings,\n",
    "                }\n",
    "            yield word, posting_lists\n",
    "\n",
    "\n",
    "def vb_decoding(encoded: bytes):\n",
    "    numbers = []\n",
    "    n = 0\n",
    "    for b in encoded:\n",
    "        n = n * 128 + (b % 128)\n",
    "        if b >= 128:\n",
    "            numbers.append(n)\n",
    "            n = 0\n",
    "    return numbers\n",
    "\n",
    "\n",
    "def read_vb_code(path: str) -> Iterable[Tuple[str, Dict[str, Dict[Literal['title', 'abstract'], Iterable[int]]]]]:\n",
    "    with open(path, 'rb') as f:\n",
    "        while True:\n",
    "            word_length = f.read(1)\n",
    "            if not word_length:\n",
    "                break\n",
    "            word_length = int.from_bytes(word_length, 'big')\n",
    "            word = f.read(word_length).decode('utf-8')\n",
    "            n_posting_lists = int.from_bytes(f.read(4), 'big')\n",
    "            posting_lists = {}\n",
    "            for _ in range(n_posting_lists):\n",
    "                doc_id_length = int.from_bytes(f.read(1), 'big')\n",
    "                doc_id = f.read(doc_id_length).decode('utf-8')\n",
    "                title_postings_n_bytes = int.from_bytes(f.read(4), 'big')\n",
    "                title_postings = vb_decoding(f.read(title_postings_n_bytes))\n",
    "\n",
    "                abstract_postings_n_bytes = int.from_bytes(f.read(4), 'big')\n",
    "                abstract_postings = vb_decoding(f.read(abstract_postings_n_bytes))\n",
    "\n",
    "                posting_lists[doc_id] = {\n",
    "                    'title': title_postings,\n",
    "                    'abstract': abstract_postings,\n",
    "                }\n",
    "            yield word, posting_lists\n",
    "\n",
    "\n",
    "def load_index(path: str, compression_type: str) -> TrieNode:\n",
    "    \"\"\"Loads the index from a file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path of the file to load from\n",
    "\n",
    "    compression_type : str\n",
    "        Could be one of the followings:\n",
    "        - no-compression\n",
    "        - gamma-code\n",
    "        - variable-byte\n",
    "    \"\"\"\n",
    "    if compression_type == 'no-compression':\n",
    "        import json\n",
    "        with open(path, 'r') as f:\n",
    "            word_list = json.load(f)\n",
    "        return TrieNode.from_words(word_list)\n",
    "    if compression_type == 'gamma-code':\n",
    "        return TrieNode.from_words(read_gamma_code(path))\n",
    "    if compression_type == 'variable-byte':\n",
    "        return TrieNode.from_words(read_vb_code(path))\n",
    "    else:\n",
    "        raise ValueError(\"compression_type should be one of the followings: no-compression, gamma-code, variable-byte\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def logarithmic_w_tf(tf: Dict[str, float]) -> Dict[str, float]:\n",
    "    return {\n",
    "        token: 1 + math.log(token_tf) if token_tf > 0 else 0\n",
    "        for token, token_tf in tf.items()\n",
    "    }\n",
    "\n",
    "\n",
    "def cosine_normalization(w: Iterable[float]) -> float:\n",
    "    return math.sqrt(sum(w_i ** 2 for w_i in w))\n",
    "\n",
    "\n",
    "def get_df(token_search_results: Dict[str, Dict[str, Dict[Literal['title', 'abstract'], List[int]]]]) -> Dict[str, int]:\n",
    "    return {\n",
    "        token: len(search_results)\n",
    "        for token, search_results in token_search_results.items()\n",
    "    }\n",
    "\n",
    "\n",
    "def query_tf_idf(query_tokens: List[str],\n",
    "                 token_search_results: Dict[str, Dict[str, Dict[Literal['title', 'abstract'], List[int]]]],\n",
    "                 score_type: str) -> Dict[str, float]:\n",
    "    tf = {\n",
    "        token: query_tokens.count(token)\n",
    "        for token in token_search_results\n",
    "    }\n",
    "    if score_type[0] == 'n':\n",
    "        w_tf = tf\n",
    "    elif score_type[0] == 'l':\n",
    "        w_tf = logarithmic_w_tf(tf)\n",
    "    else:\n",
    "        raise ValueError(f'tf method {score_type[0]} not supported')\n",
    "\n",
    "    if score_type[1] == 'n':\n",
    "        w_idf = {token: 1 for token in token_search_results}\n",
    "    else:\n",
    "        raise ValueError(f'idf method {score_type[1]} not supported')\n",
    "\n",
    "    if score_type[2] == 'n':\n",
    "        normalization = 1\n",
    "    elif score_type[2] == 'c':\n",
    "        normalization = cosine_normalization(w_tf.values())\n",
    "    else:\n",
    "        raise ValueError(f'normalization method {score_type[1]} not supported')\n",
    "\n",
    "    w = {\n",
    "        token: w_tf[token] * w_idf[token] / normalization\n",
    "        for token in token_search_results\n",
    "    }\n",
    "    return w\n",
    "\n",
    "\n",
    "def doc_tf_idf(corpus: Corpus,\n",
    "               doc_section: str,\n",
    "               token_search_results: Dict[str, Dict[str, Dict[Literal['title', 'abstract'], List[int]]]],\n",
    "               query_w: Dict[str, float],\n",
    "               score_type: str):\n",
    "    if score_type[1] == 't':\n",
    "        df = get_df(token_search_results)\n",
    "        w_idf = {\n",
    "            token: math.log(len(corpus) / token_df)\n",
    "            for token, token_df in df.items()\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f'idf method {score_type[1]} not supported')\n",
    "\n",
    "    all_documents = set()\n",
    "    for search_results in token_search_results.values():\n",
    "        all_documents.update(search_results.keys())\n",
    "\n",
    "    doc_scores = {}\n",
    "    for doc_id in all_documents:\n",
    "        doc_tf = {\n",
    "            token: len(search_results[doc_id][doc_section])\n",
    "            for token, search_results in token_search_results.items()\n",
    "            if doc_id in search_results\n",
    "        }\n",
    "        if score_type[0] == 'l':\n",
    "            doc_w_tf = logarithmic_w_tf(doc_tf)\n",
    "        else:\n",
    "            raise ValueError(f'tf method {score_type[0]} not supported')\n",
    "\n",
    "        if score_type[2] == 'n':\n",
    "            doc_normalization = 1\n",
    "        elif score_type[2] == 'c':\n",
    "            doc_normalization = cosine_normalization(doc_w_tf.values())\n",
    "        else:\n",
    "            raise ValueError(f'normalization method {score_type[1]} not supported')\n",
    "\n",
    "        doc_w = {\n",
    "            token: doc_w_tf[token] * w_idf[token] / doc_normalization\n",
    "            for token in doc_w_tf\n",
    "            if doc_normalization > 0\n",
    "        }\n",
    "        doc_scores[doc_id] = sum(\n",
    "            query_w[token] * doc_w[token]\n",
    "            for token in query_w\n",
    "            if token in doc_w\n",
    "        )\n",
    "    return doc_scores\n",
    "\n",
    "\n",
    "def tf_idf(corpus: Corpus, doc_section: Literal['title', 'abstract'],\n",
    "           token_search_results: Dict[str, Dict[str, Dict[Literal['title', 'abstract'], List[int]]]],\n",
    "           query_tokens: List[str], score_type: str) -> Dict[str, float]:\n",
    "    doc_score_type, query_score_type = score_type.split('-')\n",
    "    query_w = query_tf_idf(query_tokens, token_search_results, query_score_type)\n",
    "    return doc_tf_idf(corpus, doc_section, token_search_results, query_w, doc_score_type)\n",
    "\n",
    "\n",
    "def okapi25(corpus: Corpus, doc_section: Literal['title', 'abstract'],\n",
    "            token_search_results: Dict[str, Dict[str, Dict[Literal['title', 'abstract'], List[int]]]],\n",
    "            query_tokens: List[str], k1: float = 1.2, b: float = 0.75) -> Dict[str, float]:\n",
    "    all_documents = set()\n",
    "    for search_results in token_search_results.values():\n",
    "        all_documents.update(search_results.keys())\n",
    "\n",
    "    df = get_df(token_search_results)\n",
    "    idf = {\n",
    "        token: math.log((len(all_documents) - token_df + 0.5) / (token_df + 0.5) + 1)\n",
    "        for token, token_df in df.items()\n",
    "    }\n",
    "    f = {\n",
    "        token: {\n",
    "            doc_id: len(doc[doc_section])\n",
    "            for doc_id, doc in search_results.items()\n",
    "        }\n",
    "        for token, search_results in token_search_results.items()\n",
    "    }\n",
    "    dl = {\n",
    "        doc_id: len(corpus.non_stop_documents[doc_id][doc_section])\n",
    "        for doc_id in all_documents\n",
    "    }\n",
    "    avgdl = sum(\n",
    "        len(corpus.non_stop_documents[doc_id][doc_section])\n",
    "        for doc_id in all_documents\n",
    "    ) / len(all_documents)\n",
    "\n",
    "    doc_scores = {}\n",
    "    for doc_id in all_documents:\n",
    "        doc_scores[doc_id] = sum(\n",
    "            idf[token] * (\n",
    "                    f[token].get(doc_id, 0.0) * (k1 + 1)\n",
    "            ) / (\n",
    "                    f[token].get(doc_id, 0.0) + k1 * (1 - b + b * dl[doc_id] / avgdl)\n",
    "            )\n",
    "            for token in query_tokens\n",
    "        )\n",
    "    return doc_scores\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SearchResult:\n",
    "    doc_id: str\n",
    "    score: float\n",
    "    search_score: float\n",
    "    pagerank_score: float\n",
    "    title: str\n",
    "    abstract: str\n",
    "    related_topics: List[str]\n",
    "\n",
    "\n",
    "def highlight_text(text: str, highlight_starts: List[int]) -> str:\n",
    "    result = ''\n",
    "    last_index = 0\n",
    "    highlight_starts = sorted(highlight_starts)\n",
    "    for i, highlight_start in enumerate(highlight_starts):\n",
    "        next_start = highlight_starts[i + 1] if i + 1 < len(highlight_starts) else len(text)\n",
    "        if highlight_start == next_start:\n",
    "            print(highlight_start, next_start, text, colored(text[highlight_start:], 'yellow'), highlight_starts)\n",
    "        token = nlp(text[highlight_start:next_start])[0]\n",
    "        highlight_length = len(token)\n",
    "        result += text[last_index:highlight_start] + \\\n",
    "                  colored(text[highlight_start:highlight_start + highlight_length], 'red')\n",
    "        last_index = highlight_start + highlight_length\n",
    "    result += text[last_index:]\n",
    "    return result\n",
    "\n",
    "\n",
    "def search(corpus: Corpus,\n",
    "           trie: TrieNode,\n",
    "           bigram_index: Dict[str, Dict[str, int]],\n",
    "           query: str, max_result_count: int,\n",
    "           method: str = 'ltn-lnn',\n",
    "           weight: float = 0.5,\n",
    "           highlight: bool = False,\n",
    "           print_result: bool = False,\n",
    "           personalization_weight: float = 0.5,\n",
    "           preference_by_professor: Dict[str, float] = None) -> List[SearchResult]:\n",
    "    \"\"\"\n",
    "        Finds relevant documents to query\n",
    "\n",
    "        Parameters\n",
    "        ---------------------------------------------------------------------------------------------------\n",
    "        corpus: Corpus\n",
    "            The corpus\n",
    "        trie: TrieNode\n",
    "            The trie\n",
    "        bigram_index: Dict[str, Dict[str, int]]\n",
    "            The bigram index\n",
    "        query: str\n",
    "            The query string\n",
    "        max_result_count: int\n",
    "            Return top 'max_result_count' docs which have the highest scores.\n",
    "            notice that if max_result_count = -1, then you have to return all docs\n",
    "        method: 'ltn-lnn' or 'ltc-lnc' or 'okapi25'\n",
    "        weight: float\n",
    "            weight of abstract score\n",
    "        highlight: bool\n",
    "            If True, highlight the query tokens in search results\n",
    "        print_result: bool\n",
    "            If True, print the results in a readable format\n",
    "        preference_by_professor: Dict[str, float]\n",
    "            A dictionary of professor names and their preference scores\n",
    "\n",
    "        Returns\n",
    "        ----------------------------------------------------------------------------------------------------\n",
    "        list\n",
    "            Retrieved documents with snippet\n",
    "    \"\"\"\n",
    "    corrected_query = correct_text(bigram_index, query)\n",
    "    if print_result:\n",
    "        print(f'Query'.center(100, '-'))\n",
    "        print(corrected_query.center(100, '-'))\n",
    "        print('-' * 100)\n",
    "    query_tokens = [token.processed\n",
    "                    for token in clean_data(corrected_query)\n",
    "                    if token.processed not in corpus.stop_tokens]\n",
    "    token_search_results: Dict[str, Dict[str, Dict[Literal['title', 'abstract'], List[int]]]] = {\n",
    "        token: trie.search(token) or {}\n",
    "        for token in query_tokens\n",
    "    }\n",
    "    if '-' in method:\n",
    "        title_doc_scores = tf_idf(corpus, 'title', token_search_results, query_tokens, method)\n",
    "        abstract_doc_scores = tf_idf(corpus, 'abstract', token_search_results, query_tokens, method)\n",
    "    elif method == 'okapi25':\n",
    "        title_doc_scores = okapi25(corpus, 'title', token_search_results, query_tokens)\n",
    "        abstract_doc_scores = okapi25(corpus, 'abstract', token_search_results, query_tokens)\n",
    "    else:\n",
    "        raise ValueError(f'Expected the method to be one of \\'ltn-lnn\\', \\'ltc-lnc\\', or \\'okapi25\\', bot got {method}')\n",
    "    doc_scores = {\n",
    "        doc_id: weight * abstract_doc_scores.get(doc_id, 0.0) + (1 - weight) * title_doc_scores.get(doc_id, 0.0)\n",
    "        for doc_id in set(title_doc_scores.keys()) | set(abstract_doc_scores.keys())\n",
    "    }\n",
    "\n",
    "    graph = {}\n",
    "    for _, paper in corpus.data.iterrows():\n",
    "        graph[paper['id']] = paper['references']\n",
    "\n",
    "    # Define the user preferences based on the professor's seed papers\n",
    "    user_preferences = defaultdict(float)\n",
    "    for professor, preference in preference_by_professor.items():\n",
    "        for paper_id in corpus.papers_by_professor[professor]:\n",
    "            user_preferences[paper_id] += preference\n",
    "\n",
    "    # Calculate the personalized PageRank scores\n",
    "    pagerank_scores = pagerank(graph, user_preferences)\n",
    "\n",
    "    # normalize tf-idf scores\n",
    "    doc_scores_max = max(doc_scores.values())\n",
    "    normalized_doc_scores = {\n",
    "        doc_id: doc_score / doc_scores_max\n",
    "        for doc_id, doc_score in doc_scores.items()\n",
    "    }\n",
    "\n",
    "    # normalize pagerank scores\n",
    "    pagerank_scores_max = max(pagerank_scores.values())\n",
    "    normalized_pagerank_scores = {\n",
    "        doc_id: pagerank_score / pagerank_scores_max\n",
    "        for doc_id, pagerank_score in pagerank_scores.items()\n",
    "    }\n",
    "\n",
    "    # combine the two scores\n",
    "    combined_scores = {\n",
    "        doc_id: normalized_doc_scores[doc_id] * (1 - personalization_weight) + \\\n",
    "                normalized_pagerank_scores[doc_id] * personalization_weight\n",
    "        for doc_id in normalized_doc_scores\n",
    "    }\n",
    "\n",
    "    if highlight:\n",
    "        doc_highlights = {\n",
    "            doc_id: {\n",
    "                'title': sum([\n",
    "                    list(token_search_results[token][doc_id]['title'])\n",
    "                    for token in query_tokens\n",
    "                    if token in token_search_results and doc_id in token_search_results[token]\n",
    "                ], []),\n",
    "                'abstract': sum([\n",
    "                    list(token_search_results[token][doc_id]['abstract'])\n",
    "                    for token in query_tokens\n",
    "                    if token in token_search_results and doc_id in token_search_results[token]\n",
    "                ], [])\n",
    "            }\n",
    "            for doc_id in doc_scores\n",
    "        }\n",
    "    else:\n",
    "        doc_highlights = {\n",
    "            doc_id: {\n",
    "                'title': [],\n",
    "                'abstract': [],\n",
    "            }\n",
    "            for doc_id in doc_scores\n",
    "        }\n",
    "    documents = [\n",
    "        SearchResult(\n",
    "            doc_id=doc_id,\n",
    "            score=score,\n",
    "            search_score=normalized_doc_scores[doc_id],\n",
    "            pagerank_score=normalized_pagerank_scores[doc_id],\n",
    "            title=highlight_text(corpus.data[corpus.data['id'] == doc_id]['title'].item(),\n",
    "                                 doc_highlights[doc_id]['title']),\n",
    "            abstract=highlight_text(corpus.data[corpus.data['id'] == doc_id]['abstract'].item(),\n",
    "                                    doc_highlights[doc_id]['abstract']),\n",
    "            related_topics=corpus.data[corpus.data['id'] == doc_id]['related_topics'].item(),\n",
    "        )\n",
    "        for i, (doc_id, score) in enumerate(sorted(combined_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "        if i < max_result_count or max_result_count == -1\n",
    "    ]\n",
    "    if print_result:\n",
    "        for i, doc in enumerate(documents):\n",
    "            print(f'# {i} - Document ID: {colored(doc.doc_id, \"green\")} - Score: {colored(doc.score, \"green\")}'.center(100, '-'))\n",
    "            print(f' Search Score: {colored(doc.search_score, \"green\")} - Pagerank Score: {colored(doc.pagerank_score, \"green\")} '.center(100, '-'))\n",
    "            print(f' {doc.title} '.center(100, '-'))\n",
    "            print(f' Related Topics: {\", \".join(doc.related_topics)} '.center(100, '-'))\n",
    "            print(f'Abstract'.center(100, '-'))\n",
    "            print(doc.abstract)\n",
    "            print('-' * 100)\n",
    "    return documents"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ss = Corpus(papers_by_professor, all_papers, stop_topk=20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# cache preprocess result for ai-bio\n",
    "_ = ss.non_stop_documents"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ss_trie = construct_positional_indexes(ss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('variable-byte:', store_index(ss_trie, \"index/ss/variable-byte\", \"variable-byte\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ss_trie = load_index(\"index/ss/variable-byte\", \"variable-byte\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ss_bigram_index = create_bigram_index(ss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_ = search(ss, ss_trie, ss_bigram_index,\n",
    "           'Attention in Convolutional neoral networks for interpretation and explanation',\n",
    "           20, method='okapi25', weight=0.2, highlight=True, print_result=True,\n",
    "           preference_by_professor={'Rohban': 10,\n",
    "                                    'Rabiee': 2,\n",
    "                                    'Soleymani': 4,\n",
    "                                    'Sharifi': 1,\n",
    "                                    'Kasaei': 1})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir=\"rtl\" style=\"text-align: justify\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>رتبه‌بندی نویسندگان (۲۵ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "    برای رتبه‌بندی نویسندگان، مفهوم ارجاع نویسندگان به یکدیگر مطرح می‌شود. زمانی که نویسنده A در مقاله خود به مقاله P که نویسنده B جزو نویسندگان آن مقاله یعنی مقاله P می‌باشد، ارجاع دهد، می‌گوییم که نویسنده A به نویسنده B ارجاع داده است. با توجه به این رابطه، می‌توان گراف ارجاعات بین نویسندگان را ایجاد و سپس با استفاده از الگوریتم HITS\n",
    "نویسندگان را رتبه‌بندی کرد. برای رتبه‌بندی نیاز است تا از شاخص‌های hub و authority استفاده کنیم.\n",
    "\n",
    "\n",
    "</font>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "\n",
    "def hit_algorithm(papers, n):\n",
    "    \"\"\"\n",
    "        Implementing the HITS algorithm to score authors based on their papers and co-authors.\n",
    "\n",
    "        Parameters\n",
    "        ---------------------------------------------------------------------------------------------------\n",
    "        papers: A list of paper dictionaries with the following keys:\n",
    "                \"id\": A unique ID for the paper\n",
    "                \"title\": The title of the paper\n",
    "                \"abstract\": The abstract of the paper\n",
    "                \"publication_year\": The year in which the paper was published\n",
    "                \"authors\": A list of the names of the authors of the paper\n",
    "                \"related_topics\": A list of IDs for related topics (optional)\n",
    "                \"citation_count\": The number of times the paper has been cited (optional)\n",
    "                \"reference_count\": The number of references in the paper (optional)\n",
    "                \"references\": A list of IDs for papers that are cited in the paper (optional)\n",
    "        n: An integer representing the number of top authors to return.\n",
    "\n",
    "        Returns\n",
    "        ---------------------------------------------------------------------------------------------------\n",
    "        List\n",
    "        list of the top n authors based on their hub scores.\n",
    "    \"\"\"\n",
    "    # Create a graph of authors and papers\n",
    "    g = nx.Graph()\n",
    "\n",
    "    for paper in papers:\n",
    "        paper_id = paper['id']\n",
    "        authors = paper['authors']\n",
    "\n",
    "        # Add paper node to the graph\n",
    "        g.add_node(f'paper:{paper_id}')\n",
    "\n",
    "        for author in authors:\n",
    "            # Add author node to the graph\n",
    "            g.add_node(f'author:{author}')\n",
    "\n",
    "            # Connect author node to paper node\n",
    "            g.add_edge(f'author:{author}', f'paper:{paper_id}')\n",
    "\n",
    "    # Run the HITS algorithm\n",
    "    hubs, authorities = nx.hits(g, max_iter=1000, tol=1e-08)\n",
    "\n",
    "    # Sort authors based on their hub scores\n",
    "    sorted_authors = sorted([a[0] for a in hubs.items() if a[0].startswith('author:')],\n",
    "                            key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return sorted_authors[:n]\n",
    "\n",
    "\n",
    "# call the hit_algorithm function\n",
    "top_authors = hit_algorithm(all_papers, 10)\n",
    "\n",
    "# print the top authors\n",
    "print(top_authors)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>سیستم پیشنهادگر (۲۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "\n",
    "در این بخش سعی می‌کنیم که یک سیستم پیشنهادگر مقالات بر اساس جست‌و‌جو‌ها یا علايق یک کاربر پیاده‌سازی کنیم، سیستم پیشنهاد دهنده‌ای که قصد داریم آن را ایجاد کنیم،‌ باید بتواند بر اساس لیستی از مقالاتی که کاربر قبلا آن‌ها را مطالعه کرده یا به آن‌ها علاقه داشته است، مقالات تازه انتشار یافته‌‌ی جدید را به کاربر پیشنهاد دهد.\n",
    "\n",
    "در فایل recommended_papers.json\n",
    "لیستی از کاربران قرار دارد که در فیلد positive_papers هر کاربر،\n",
    "تعداد ۵۰ مقاله از مقالاتی که کاربر به آن‌ها علاقه داشته است مشخص شده است. و همچینین در فیلد recommendedPapers هر کاربر تعداد ۱۰ مقاله به ترتیب اهمیت، از مقالات جدیدی که کاربر آن‌ها را پسندیده است قرار دارد.\n",
    "\n",
    "در این بخش هدف شما یادگیری سیستم پیشنهاد‌ دهنده بر اساس همین داده‌ها می‌باشد، و به عبارتی شما بایستی کاربر‌ها را به دو دسته آموزش و آزمایش تقسیم کنید، و بر اساس داده‌های آموزشی بتوانید مقالات جدید مورد پسند کاربرهای آزمایش را پیش‌بینی کنید. (بنابراین در این پیش‌بینی نمی‌توانید از فیلد recommendedPapers این کاربران استفاده کنید.)\n",
    "\n",
    "</font>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/papers/recommended_papers.json', 'r') as fp:\n",
    "    recommended_papers = json.load(fp)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_user = recommended_papers[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(recommended_papers)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(sample_user['positive_papers']), len(sample_user['recommendedPapers'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sample_user['positive_papers'][0]['paperId'])\n",
    "print(sample_user['positive_papers'][0]['title'])\n",
    "print(sample_user['positive_papers'][0]['abstract'])\n",
    "print(sample_user['positive_papers'][0]['fieldsOfStudy'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sample_user['recommendedPapers'][0]['paperId'])\n",
    "print(sample_user['recommendedPapers'][0]['title'])\n",
    "print(sample_user['recommendedPapers'][0]['abstract'])\n",
    "print(sample_user['recommendedPapers'][0]['fieldsOfStudy'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h2>\n",
    "    <b>روش Collaborative Filtering (۱۰ نمره)</b>\n",
    "    </h2>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "\n",
    "در این راهکار سعی می‌کنیم با استفاده از کاربران مشابه با یک کاربر، سلیقه‌ی او را حدس بزنیم و مقالاتی را که کاربران مشابه دیده‌اند را به کاربر نمایش دهیم.\n",
    "\n",
    "در این روش ابتدا باید $N$ کاربر که سلیقه‌ی مشابه با کاربر $x$ دارند را پیدا کنید، و با ترکیب لیست مقالات جدید مورد علاقه‌ی آن $N$ کاربر مشابه،\n",
    " ۱۰ مقاله‌ به کاربر $x$ پیشنهاد دهید.\n",
    "\n",
    "توجه داشته باشید که برای اینکه شباهت دو کاربر را پیدا کنید، باید cosine_similarity بین بردار زمینه‌های مورد علاقه‌ی دو کاربر استفاده کنید. این بردار از $M$ درایه تشکیل شده است، که $M$ تعداد زمینه‌های یکتاییست که در داده‌ها وجود دارد. و در این بردار درایه‌ی $j$ام\n",
    "نشان دهنده‌ی نسبت تعداد مقالات خوانده‌ی شده‌ کاربر در زمینه‌ی $j$ به تعداد کل مقاله‌های خوانده شده توسط او می‌باشد. (توجه کنید که هر مقاله می‌تواند چند زمینه داشته باشد و بنابراین حاصل جمع درایه‌های این بردار الزاما یک نمی‌باشد)\n",
    "\n",
    "</font>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [
    "fields = sorted(set(\n",
    "    field\n",
    "    for user in recommended_papers\n",
    "    for paper in user['positive_papers']\n",
    "    for field in paper['fieldsOfStudy'] or []\n",
    "))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(recommended_papers, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "class CollaborativeFiltering:\n",
    "\n",
    "    def __init__(self, data: dict, n = 10):\n",
    "        self.data = data\n",
    "        self.knn = NearestNeighbors(n_neighbors=n, metric='cosine')\n",
    "\n",
    "    @staticmethod\n",
    "    def create_user_field_matrix(user_positive_papers: List[List[Dict[str, Any]]]) -> pd.DataFrame:\n",
    "        user_fields = {\n",
    "            'user_index': list(range(len(user_positive_papers))),\n",
    "            **{field: [sum(field in (paper['fieldsOfStudy'] or []) for paper in positive_papers)\n",
    "                       for positive_papers in user_positive_papers]\n",
    "               for field in fields}\n",
    "        }\n",
    "        return pd.DataFrame(user_fields).set_index('user_index')\n",
    "\n",
    "    def fit(self):\n",
    "        user_field_matrix = self.create_user_field_matrix([user['positive_papers'] or [] for user in self.data])\n",
    "        self.knn.fit(user_field_matrix)\n",
    "        return self\n",
    "\n",
    "    def predict(self, user_positive_papers: List[Dict[str, Any]]):\n",
    "        user_field_vector = self.create_user_field_matrix([user_positive_papers])\n",
    "        distances, indices = self.knn.kneighbors(user_field_vector)\n",
    "\n",
    "        # get recommended papers from the k nearest neighbors\n",
    "        result = [\n",
    "            paper['paperId']\n",
    "            for similar_user_id in indices[0]\n",
    "            for paper in self.data[similar_user_id]['recommendedPapers']\n",
    "        ]\n",
    "        return result\n",
    "\n",
    "collaborative_filtering = CollaborativeFiltering(train_data).fit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h2>\n",
    "    <b>روش Content Based (۱۰ نمره)</b>\n",
    "    </h2>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "\n",
    "در این روش با استفاده از مقالات قبلی که کاربر آن‌ها را پسندیده است، به کاربر مقاله‌ی جدید پیشنهاد می‌دهیم.\n",
    "\n",
    "برای اینکار ابتدا تمام مقالات پیشنهاد شده برای تمام کاربرها را سر جمع کنید. (در واقع مدلی که پیاده‌سازی می‌کنید نباید بداند که به کدام کاربر چه مقالاتی پیشنهاد شده است)\n",
    "\n",
    "سپس بردار tf-idf برای تایتل هر یک از مقالات را ایجاد کنید، و میانگین بردار مقالات مورد علاقه‌ی هر فرد را با لیستی که از مقالات جدید سر جمع کردید مقایسه کنید و ۱۰ تا از شبیه‌ترین مقالات را خروجی دهید.\n",
    "\n",
    "</font>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "class ContentBasedRecommendation:\n",
    "\n",
    "    def __init__(self, all_recommended_papers: List[Dict[str, Any]]):\n",
    "        self.data = all_recommended_papers\n",
    "        self.tf_idf = TfidfVectorizer()\n",
    "        self.recommended_paper_vectors = None\n",
    "\n",
    "    def fit(self):\n",
    "        titles = [paper['title'] for paper in self.data]\n",
    "        self.recommended_paper_vectors = self.tf_idf.fit_transform(titles)\n",
    "        return self\n",
    "\n",
    "    def predict(self, user_positive_papers: List[Dict[str, Any]]):\n",
    "        titles = [paper['title'] for paper in user_positive_papers]\n",
    "        titles_vector = self.tf_idf.transform(titles).mean(axis=0)\n",
    "\n",
    "        # calculate the similarities between the user's positive papers and all the recommended papers\n",
    "        similarities: np.ndarray = np.asarray(titles_vector @ self.recommended_paper_vectors.T).flatten()\n",
    "        # get the top 10 most similar papers\n",
    "        result = [\n",
    "            self.data[i]['paperId']\n",
    "            for i in similarities.argsort()[-10:][::-1]\n",
    "        ]\n",
    "        return result\n",
    "\n",
    "content_based_recommendation = ContentBasedRecommendation([paper\n",
    "                                                           for user in recommended_papers\n",
    "                                                           for paper in (user['recommendedPapers'] or [])]).fit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h2>\n",
    "    <b>ارزیابی سیستم‌های پیشنهادگر</b>\n",
    "    </h2>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "\n",
    "در این بخش سیستم‌های پیشنهادگری را که پیاده کرده‌اید را با استفاده از معیار nDCG و با استفاده از دادگان واقعی از علایق کاربران نسبت به مقالات جدید ارزیابی کنید و نتایج حاصل از دو روش را با هم مقایسه کنید.\n",
    "\n",
    "</font>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "outputs": [],
   "source": [
    "def calculate_nDCG(ranked_list: List[str], relevant_list: List[str], k: int = 10):\n",
    "    \"\"\"\n",
    "    Calculates the nDCG (normalized discounted cumulative gain) score given a ranked list and a relevant list.\n",
    "\n",
    "    Parameters:\n",
    "    ranked_list (List[str]): Ranked list of item IDs.\n",
    "    relevant_list (List[str]): Relevant list of item IDs.\n",
    "    k (int): The truncation position for calculating nDCG@k. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    float: The nDCG score.\n",
    "    \"\"\"\n",
    "    # Truncate the ranked and relevant lists to the specified position\n",
    "    ranked_list = ranked_list[:k]\n",
    "    relevant_list = relevant_list[:k]\n",
    "\n",
    "    # Calculate the Discounted Cumulative Gain (DCG)\n",
    "    dcg = 0.0\n",
    "    for i, item_id in enumerate(ranked_list):\n",
    "        if item_id in relevant_list:\n",
    "            relevance = 1.0 / np.log2(i + 2)  # Assign a relevance of 1.0 if the item is relevant\n",
    "            dcg += relevance\n",
    "\n",
    "    # Calculate the Ideal DCG (IDCG)\n",
    "    IDCG = 0.0\n",
    "    for i in range(k):\n",
    "        relevance = 1.0 / np.log2(i + 2)  # Assign a relevance of 1.0 if the item is relevant\n",
    "        IDCG += relevance\n",
    "\n",
    "    # Calculate the nDCG score\n",
    "    nDCG = dcg / IDCG if IDCG > 0.0 else 0.0\n",
    "    return nDCG"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collaborative filtering nDCG: 0.36167419314249255\n",
      "Content-based nDCG: 0.1290581089990145\n"
     ]
    }
   ],
   "source": [
    "collaborative_filtering_ndcg = [\n",
    "    calculate_nDCG(collaborative_filtering.predict(user['positive_papers']), [p['paperId'] for p in user['recommendedPapers']])\n",
    "    for user in test_data\n",
    "]\n",
    "content_based_ndcg = [\n",
    "    calculate_nDCG(content_based_recommendation.predict(user['positive_papers']), [p['paperId'] for p in user['recommendedPapers']])\n",
    "    for user in test_data\n",
    "]\n",
    "\n",
    "print(f'Collaborative filtering nDCG: {np.mean(collaborative_filtering_ndcg)}')\n",
    "print(f'Content-based nDCG: {np.mean(content_based_ndcg)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div dir=\"rtl\" style=\"text-align: justify\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h1>\n",
    "    <b>رابط کاربری (تا ۱۰ نمره)</b>\n",
    "    </h1>\n",
    "</font>\n",
    "    <br>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در این بخش\n",
    " باید یک واسط کاربری ساده برای اجرای تعاملی بخش‌های مختلف سیستم که از فاز ۱ ساخته‌اید و همچنین مشاهده نتایج پیاده‌سازی کنید. در صورت پیاده سازی زیبا و بهتر رابط کاربری تا ده نمره نمره امتیازی نیز در نظر گرفته خواهد شد.\n",
    "</font>\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "08ac30a6a1fd2e576b33e03f7d61c3a285d7ee0582c2dd23dde6343ef303ebe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
