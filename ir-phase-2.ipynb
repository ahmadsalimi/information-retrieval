{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B Titr\" size=5>\n",
    "<p></p><p></p>\n",
    "بسمه تعالی\n",
    "<p></p>\n",
    "</font>\n",
    "<p></p>\n",
    "<font>\n",
    "<br>\n",
    "درس بازیابی پیشرفته اطلاعات\n",
    "<br>\n",
    "مدرس: دکتر سلیمانی\n",
    "</font>\n",
    "<p></p>\n",
    "<br>\n",
    "<font>\n",
    "<b>فاز دوم پروژه</b>\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "موعد تحویل: ۵ خرداد ۱۴۰۲ <br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font>\n",
    "دانشگاه صنعتی شریف\n",
    "<br>\n",
    "دانشکده مهندسی کامپیوتر\n",
    "<br>\n",
    "<br>\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4 >\n",
    "سامانه‌های مبتنی بر یادگیری ماشین در بخش‌های مختلف از روش‌های یادگیری ماشین استفاده می‌کنند. دسته‌بندی و\n",
    "خوشه‌بندی\n",
    "دو کار مورد نیاز این سامانه‌ها هستند. در این تمرین به پیاده‌سازی این کارها با روش‌های مختلف و پیاده‌سازی قابلیت‌های جدید برای سیستم بازیابی روی مقالات علمی می‌پردازیم. دادگان این فاز را می‌توانید از\n",
    "<a href=\"https://www.kaggle.com/datasets/spsayakpaul/arxiv-paper-abstracts?resource=download\">این لینک</a>\n",
    "دانلود کنید.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=6>\n",
    "    <h1>\n",
    "    <b>دسته‌بندی توسط NaiveBayes (۱۸ نمره)</b>\n",
    "    </h1>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در این قسمت شما به دسته‌بندی اسناد می‌پردازید. برای این منظور، ابتدا برای هر سند، بردار مربوط به تعداد کلمات کل را بسازید و سپس classifier NaiveBaise را خودتان از پایه پیاده‌سازی کرده و اسناد را دسته‌بندی کنید.\n",
    "در این تسک می‌توانید از کل دیتاست استفاده نکنید ولی باید دیتاست شما شامل هر سه کلاس موجود در دیتاست اصلی باشد.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <h2>\n",
    "    <b>دانلود دیتاست و آشنایی با آن</b>\n",
    "    </h2>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در این قسمت دیتاست زیر را در kaggle دریافت کرده و ویژگی‌های آن را مشاهده کنید.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./arxiv_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <h2>\n",
    "    <b>کاهش سایز دیتاست</b>\n",
    "    </h2>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "    در این قسمت می‌توانید سایز دیتاست را به دلخواه کم کرده تا NaiveBayes عملکرد بهتری بتواند از خودش نشان دهد.\n",
    "    کاهش سایز دیتاست می‌تواند باعث افزایش سرعت و حافظه‌ی مورد نیاز شود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <h2>\n",
    "    <b>پیش‌پردازش دیتاست</b>\n",
    "    </h2>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "    در این قسمت می‌توانید با استفاده از تابع زیر یا توابعی که در فاز 1 تعریف کردید به پیش‌پردازش دیتاست بپردازید.\n",
    "    داده‌ای که با آن در این بخش کار می‌کنید چند برچسبی می‌باشند. اما برای این بخش ما تنها برچسب اول از هر مقاله را در نظر می‌گیریم و به صورت تک برچسبی به داده‌ها نگاه می‌کنیم.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, minimum_length=1, stopword_removal=True, stopwords_domain=[], lower_case=True,\n",
    "                       punctuation_removal=True):\n",
    "    \n",
    "    normalized_tokens = word_tokenize(text)\n",
    "\n",
    "    if stopword_removal:\n",
    "        # Remove stopwords in English and also the given domain stopwords\n",
    "        stopwords = [x.lower() for x in nltk.corpus.stopwords.words('english')]\n",
    "        domain_stopwords = [x.lower() for x in stopwords_domain]\n",
    "        normalized_tokens = [word for word in normalized_tokens if word.lower() not in domain_stopwords + stopwords]\n",
    "\n",
    "    if punctuation_removal:\n",
    "        # Remove punctuations\n",
    "        normalized_tokens = [word for word in normalized_tokens if word not in string.punctuation]\n",
    "\n",
    "    if lower_case:\n",
    "        # Convert everything to lowercase and filter based on a min length\n",
    "        normalized_tokens = [word.lower() for word in normalized_tokens if len(word) > minimum_length]\n",
    "    else:\n",
    "        normalized_tokens = [word for word in normalized_tokens if len(word) > minimum_length]\n",
    "\n",
    "    return normalized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <h2>\n",
    "    <b>آماده‌سازی دیتای train و test</b>\n",
    "    </h2>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "    در این قسمت با ساخت ماتریس doc-word می‌توانید دیتای مورد نیاز برای NaiveBayes را آماده کنید. در این ماتریس هر سطر نشان‌دهنده آیدی داکیومنت و هر ستون نشان‌دهنده یک کلمه در کل vocabulary شما می‌باشد.\n",
    "   همچنین می‌توانید از CountVectorizer استفاده کنید. \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <h2>\n",
    "    <b>دسته‌بندی با استفاده از NaivaBayes</b>\n",
    "    </h2>\n",
    "    در این بخش کلاس زیر را تکمیل نمایید تا classifier NaiveBayes را <u><b>از پایه</b></u> پیاده‌سازی کنید.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes():\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.prior = None\n",
    "        self.word_counts = None\n",
    "        self.lk_word = None\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        '''\n",
    "        Fit the features and the labels\n",
    "        Calculate prior, word_counts and lk_word\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: np.ndarray\n",
    "            An m * n matrix - m is count \n",
    "            of docs and n is size of vocabulary\n",
    "\n",
    "        y: np.ndarray\n",
    "            The real class label for each doc\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "            Returns self as a classifier\n",
    "        '''\n",
    "        #TODO\n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: np.ndarray\n",
    "            An k * n matrix - k is count \n",
    "            of docs and n is vocabulary size\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Return the predicted class for each doc\n",
    "            with the highest probability (argmax)\n",
    "        '''\n",
    "        \n",
    "        #TODO\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use your classifier to fit on the training data\n",
    "# then try to predict classes of test data\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <h2>\n",
    "    <b>ارزیابی مدل train شده</b>\n",
    "    </h2>\n",
    "    در این بخش precision، recall، F1 score را در حالت macro و micro\n",
    "    و accuracy مدل\n",
    "    را با استفاده از sklearn به دست آورید.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "در بخش پایین roc curve مربوط به این دسته‌بندی non-binary را به دست آورده و به صورت مختصر تحلیل کنید.\n",
    "</div>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "در این بخش confusion matrix را بدون استفاده از sklearn و به کمک matplotlib و seaborn بکشید.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#TODO confusion matrix without importing confusion_matrix from sklearn.metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <h1>\n",
    "    <b>دسته بندی با شبکه های عصبی (۳۲ نمره)</b>\n",
    "    </h1>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     <br>\n",
    "    شما یک مجموعه داده از مقالات علمی دارید، هرکدام با یک چکیده و موضوع مربوطه نشان داده شده است. هدف ساختن یک مدل شبکه عصبی است که بتواند بر اساس چکیده مقاله علمی و عنوان آن، موضوع آن را پیش بینی کند.\n",
    "</font>\n",
    "</div>\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <h2>\n",
    "    <b>داده ها</b>\n",
    "    </h2>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     <br>\n",
    "    داده‌ای که با آن در این بخش کار می‌کنید چند برچسبی می‌باشند. اما برای این بخش ما تنها برچسب اول از هر مقاله را در نظر می‌گیریم و به صورت تک برچسبی به داده‌ها نگاه می‌کنیم.\n",
    "</font>\n",
    "</div>\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may not use all of these imports, but they are here to help you get started\n",
    "# you can add cells if you needed\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import fasttext\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from time import time\n",
    "from IPython.display import display"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     <br>\n",
    "در ابتدا دیتا را لود کنید. آن را تمیز کنید یا به عبارتی EDA مناسب در حدی که داده‌های null یا موارد این‌چنینی نداشته باشیم، بر روی آن انجام دهید.\n",
    " پیش پردازش‌های مورد نیاز که در فاز قبل با آن آشنا شدید را روی داده انجام دهید.\n",
    "    برای کتگوری هر مقاله برچسب اول آن را از دیتاست در نظر بگیرید.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv('./arxiv_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform data cleaning and eda (you can add cells here)\n",
    "# todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, minimum_length=1, stopword_removal=True, stopwords_domain=[], lower_case=True,\n",
    "                       punctuation_removal=True):\n",
    "    \"\"\"\n",
    "    preprocess text by removing stopwords, punctuations, and converting to lowercase, and also filter based on a min length\n",
    "    for stopwords use nltk.corpus.stopwords.words('english')\n",
    "    for punctuations use string.punctuation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str\n",
    "        text to be preprocessed\n",
    "    minimum_length: int\n",
    "        minimum length of the token\n",
    "    stopword_removal: bool\n",
    "        whether to remove stopwords\n",
    "    stopwords_domain: list\n",
    "        list of stopwords to be removed base on domain\n",
    "    lower_case: bool\n",
    "        whether to convert to lowercase\n",
    "    punctuation_removal: bool\n",
    "        whether to remove punctuations\n",
    "    \"\"\"\n",
    "    # todo\n",
    "    normalized_tokens = None\n",
    "    return normalized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     <br>\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess text\n",
    "df['preprocessed_abstract'] = df['summaries'].apply(lambda x: preprocess_text(x))\n",
    "df['preprocessed_title'] = df['titles'].apply(lambda x: preprocess_text(x))\n",
    "preprocessed_total = df['preprocessed_abstract'] + df['preprocessed_title']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     <br>\n",
    "در این قسمت از fasttext کمک می‌گیریم تا به یک embedding اولیه برای هر مقاله برسیم.\n",
    "    با استفاده از داده‌هایی که داریم یک مدل fasttext آموزش دهید که برای هر توکن یک امبدینگ ۱۰۰تایی بدهد.\n",
    "    در مرحله‌ی بعد میانگین وزن دار امبدینگ های fasttext\n",
    "        توکن‌های ورودی (چکیده + عنوان)\n",
    "    را بر اساس tfidif آن‌ها محاسبه کنید و به امبدینگ نهایی متن برسید.\n",
    "    <br>\n",
    "    در واقع به عبارت ساده‌تر بر اساس میانگین وزن‌دار که وزن‌های ما tfidf توکن‌ها می‌باشد به امبدینگ نهایی متن بر اساس fasttext می‌رسیم.\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastText:\n",
    "\n",
    "    def __init__(self, preprocessor=None, method='skipgram'):\n",
    "        self.method = method\n",
    "        self.model = None\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def train(self, texts):\n",
    "        \"\"\"\n",
    "        train the fasttext model and save it into self.model\n",
    "        Parameters\n",
    "        ----------\n",
    "        texts: list of list of str\n",
    "        \"\"\"\n",
    "        # todo\n",
    "\n",
    "    def get_query_embedding(self, query, tf_idf_vectorizer):\n",
    "        \"\"\"\n",
    "        get the embedding of the query. You can use the tf_idf_vectorizer to get the weights of the words in the query. preprocess the query using self.preprocessor if it is not None\n",
    "        Parameters\n",
    "        ----------\n",
    "        query: str\n",
    "        tf_idf_vectorizer: TfidfVectorizer\n",
    "        Returns embedding of the query\n",
    "        \"\"\"\n",
    "        # todo\n",
    "        query_embed = None\n",
    "        return query_embed\n",
    "\n",
    "    def save_FastText_model(self, path='FastText_model.bin'):\n",
    "        self.model.save_model(path)\n",
    "  \n",
    "    def load_FastText_model(self, path=\"FastText_model.bin\"):\n",
    "        self.model = fasttext.load_model(path)\n",
    "\n",
    "    def prepare(self, dataset, mode, save=False):\n",
    "        if mode == 'train':\n",
    "            self.train(dataset)\n",
    "        if mode == 'load':\n",
    "            self.load_FastText_model()\n",
    "        if save:\n",
    "            self.save_FastText_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastText_model = FastText(preprocessor=preprocess_text)\n",
    "FastText_model.prepare(preprocessed_total, mode='train', save=True)\n",
    "# FastText_model.prepare(preprocessed_total, mode='load')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'covid'\n",
    "\n",
    "FastText_model.model.get_nearest_neighbors(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TF_IDF:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "\n",
    "    def fit_vectorizer(self, data):\n",
    "        \"\"\"\n",
    "        fit the vectorizer on the data\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: list of list of str\n",
    "        \"\"\"\n",
    "        # todo\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_model = TF_IDF()\n",
    "TF_IDF_model.fit_vectorizer(preprocessed_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =  \"backpropagation is good\"\n",
    "TF_IDF_model.vectorizer.transform([' '.join(text)])\n",
    "FastText_model.get_query_embedding(text, TF_IDF_model.vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     <br>\n",
    "حال به کمک مدلی که نوشته‌اید دیتای ورودی شبکه‌ی عصبی را بسازید.\n",
    "    به عبارتی به ازای هر مقاله embedding مربوطه را بدست آورید.\n",
    "    همچنین برچسب‌ها را نیز به عدد تبدیل کنید تا برای شبکه‌ی عصبی قابل فهم باشد.\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "embeddings = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     <br>\n",
    "     در این قسمت dataset ورودی شبکه‌ی عصبی را بسازید.\n",
    "     همچنین دیتا‌ را به قسمت‌های train test validation تقسیم کنید.\n",
    "     80% داده برای آموزش\n",
    "     10% را برای  validation\n",
    "     و 10% را برای تست در نظر بگیرید.\n",
    "     در نهایت dataloaderهای مربوطه را بسازید.\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PapersDataSet(Dataset):\n",
    "    def __init__(self, embeddings: list, labels: list):\n",
    "        # todo\n",
    "        pass\n",
    "        \n",
    "    def __len__(self):\n",
    "        # todo\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # todo\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo calculate the labels\n",
    "labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo split the data into train, val, test and create dataloaders\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     <br>\n",
    "حال دراین قسمت مدل شبکه‌ی عصبی خود را تعریف کنید.\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierModel(nn.Module):\n",
    "    def __init__(self, in_features=100, num_classes=3):\n",
    "        # todo\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # todo\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     <br>\n",
    "در این قسمت به کمک داده‌ی آموزش و validation مدل خود را آموزش دهید.\n",
    "اطلاعات مورد نیاز نظیر lossهای train و validation را ذخیره کنید تا در ادامه به کمک نمودار یادگیری مدل خود را ارزیابی کنید.\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ClassifierModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo define the loss function and the optimizer. We suggest using Adam optimizer\n",
    "criterion = None\n",
    "learning_rate = None\n",
    "optimizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model: nn.Module, criterion: nn.Module, dataloader: torch.utils.data.DataLoader, test_mode=False):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the given dataloader. used for validation and test\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: nn.Module\n",
    "    criterion: nn.Module\n",
    "    dataloader: torch.utils.data.DataLoader\n",
    "    test_mode: bool\n",
    "        If True, the function will print 'Test' instead of 'Validation'\n",
    "    Returns\n",
    "    -------\n",
    "    eval_loss: float\n",
    "        The loss on the given dataloader\n",
    "    predicted_labels: list\n",
    "        The predicted labels\n",
    "    true_labels: list\n",
    "        The true labels\n",
    "    f1_score_macro: float\n",
    "        The f1 score on the given dataloader\n",
    "    \"\"\"\n",
    "    # todo\n",
    "    eval_loss = None\n",
    "    predicted_labels = None\n",
    "    true_labels = None\n",
    "    f1_score_macro = None\n",
    "    return eval_loss, predicted_labels, true_labels, f1_score_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train the model for num_epochs epochs\n",
    "epoch_true and epoch_all are used to calculate the accuracy.\n",
    "epoch_true is the number of correct predictions and epoch_all is the total number of predictions in the epoch\n",
    "\"\"\"\n",
    "num_epochs = 50\n",
    "\n",
    "train_loss_arr, val_loss_arr = [], []\n",
    "f1_macro_scores = []\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time()\n",
    "\n",
    "    train_loss, val_loss = 0, 0\n",
    "    epoch_all = 0\n",
    "    epoch_true = 0\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    with tqdm(enumerate(train_loader), total=len(train_loader)) as pbar:\n",
    "        for i, (x, label) in pbar:\n",
    "            # todo\n",
    "            pass\n",
    "  \n",
    "    model.eval()\n",
    "    \n",
    "    val_loss, predicted_labels, true_labels, f1_score_macro = eval_epoch(model, criterion, val_loader)\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    train_loss_arr.append(train_loss)\n",
    "    val_loss_arr.append(val_loss)\n",
    "    f1_macro_scores.append(f1_score_macro)\n",
    "    \n",
    "    end_time = time()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1} finished in {end_time - start_time:.2f}s')\n",
    "\n",
    "    print(f\"[Epoch {epoch + 1}]\\t\"\n",
    "        f\"Train Loss: {train_loss:.4f}\\t\"\n",
    "        f\"Validation Loss: {val_loss:.4f}\\t F1 score macro: {f1_score_macro}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     <br>\n",
    "نمودار loss بر حسب epoch را برای داده‌های train و validation رسم کنید.\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: plot the train and validation loss by epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     <br>\n",
    "نمودار f1 score بر حسب epoch را برای داده‌های train و validation رسم کنید.\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: plot the f1 score by epoch "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     <br>\n",
    "حال داده‌ی test را به مدلتان دهید و بر اساس خروجی‌ها موارد زیر را به دست آورید.\n",
    "    <br>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>accuracy</li>\n",
    "        <li>Loss</li>\n",
    "        <li>F1-Macro</li>\n",
    "        <li>F1-Micro</li>\n",
    "        <li>Confusion matrix</li>\n",
    "        <li>Macro-average precision</li>\n",
    "        <li>Macro-average recall</li>\n",
    "        <li>Micro-average precision</li>\n",
    "        <li>Micro-average recall</li>\n",
    "    </ul>\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: evaluate the model on the test set\n",
    "# print the required metrics\n",
    "model.eval()\n",
    "eval_loss, predicted_labels, true_labels, f1_score_macro = eval_epoch(model, criterion, test_loader, test_mode=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     <br>\n",
    "در نهایت در این قسمت تابعی بنویسید که یک متن را به عنوان ورودی بگیرد و بر اساس مدلتان برچسب‌ مناسب را به آن بدهد.\n",
    "    <br>\n",
    "    <br>\n",
    "    </font>\n",
    "</div>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    \"\"\"\n",
    "    predict the class code of a given query\n",
    "    :param x: the query\n",
    "    :return: the class of the query in the form of real strings.\n",
    "    \"\"\"\n",
    "    # todo\n",
    "    predicted_class = None\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('hardware and computerc architecture is good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"nn_fasttext_model.pt\"\n",
    "\n",
    "# Save\n",
    "torch.save(model, PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <h1>\n",
    "    <b>دسته بندی با استفاده از مدل های زبانی (۴۰ نمره)</b>\n",
    "    </h1>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     <br>\n",
    "در این قسمت قرار است تا به کمک language modelهای از پیش آموزش شده \n",
    "و fine-tune کردن آن‌ها طبقه‌بند خودمان را درست کنیم.\n",
    "</font>\n",
    "</div>\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     <br>\n",
    "در ابتدا دیتا را لود کنید. آن را تمیز کنید یا به عبارتی EDA مناسب در حدی که داده‌های null یا موارد این‌چنینی نداشته باشیم، بر روی آن انجام دهید.\n",
    " پیش پردازش‌های مورد نیاز که در فاز قبل با آن آشنا شدید را روی داده انجام دهید.\n",
    "    برای کتگوری هر مقاله برچسب اول آن را از دیتاست در نظر بگیرید.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv('./arxiv_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform data cleaning and eda (you can add cells here)\n",
    "# todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, minimum_length=1, stopword_removal=True, stopwords_domain=[], lower_case=True,\n",
    "                       punctuation_removal=True):\n",
    "    \"\"\"\n",
    "    preprocess text by removing stopwords, punctuations, and converting to lowercase, and also filter based on a min length\n",
    "    for stopwords use nltk.corpus.stopwords.words('english')\n",
    "    for punctuations use string.punctuation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str\n",
    "        text to be preprocessed\n",
    "    minimum_length: int\n",
    "        minimum length of the token\n",
    "    stopword_removal: bool\n",
    "        whether to remove stopwords\n",
    "    stopwords_domain: list\n",
    "        list of stopwords to be removed base on domain\n",
    "    lower_case: bool\n",
    "        whether to convert to lowercase\n",
    "    punctuation_removal: bool\n",
    "        whether to remove punctuations\n",
    "    \"\"\"\n",
    "    # todo\n",
    "    normalized_tokens = None\n",
    "    return normalized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess text\n",
    "df['preprocessed_abstract'] = df['summaries'].apply(lambda x: preprocess_text(x))\n",
    "df['preprocessed_title'] = df['titles'].apply(lambda x: preprocess_text(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     <br>\n",
    "     در این قسمت برچسب‌ها و متن‌ها را برای مدل آماده کنید.\n",
    "     برای متن هم عنوان و هم چکیده را در نظر بگیرید و این دو را به هم بچسبانید.\n",
    "     به صورت رندم، ۱۰۰۰۰ داده را برای تمرین انتخاب کنید.\n",
    "     همچنین جدا‌سازی داده‌ها را انجام دهید.\n",
    "     در این بخش کافی است تا داده‌ها را به دو دسته‌ی آموزش و تست تقسیم کنید. 80% داده‌ها برای آموزش و 20% را برای تست قرار دهید.\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "# calculate labels\n",
    "# calulcate final texts\n",
    "# select 10000 data randomly\n",
    "# split data into train and test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "random_state = 1\n",
    "X_train, X_test, y_train, y_test = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer based classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     <br>\n",
    "مدلی که در این قسمت استفاده می‌کنیم مدل معروف bert می‌باشد.\n",
    "    برای این کار از کتابخانه transformers استفاده می‌کنیم.\n",
    "    مدل و tokenizer مربوطه را لود کنید.\n",
    "    <br><b><u>\n",
    "    به کمک داده‌های قسمت قبل و به کمک \n",
    "    Trainer\n",
    "    مربوط به کتاب‌خانه‌ی transformers عملیات fine-tune را انجام دهید.\n",
    "    <br>\n",
    "    همچنین یکبار وزن‌های مدل برت را فریز کنید و فقط دسته‌بند مربوطه را آموزش دهید.\n",
    "    </u></b><br>\n",
    "    مطالعه‌ی بیشتر کتاب‌خانه‌ی transformers بسیار توصیه می‌شود.\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# todo \n",
    "# you can add more cells here if you need\n",
    "model = None\n",
    "model_2 = None\n",
    "tokenizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "train_encodings = None\n",
    "test_encodings = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        pass\n",
    "\n",
    "train_dataset = CustomDataset(train_encodings, y_train)\n",
    "test_dataset = CustomDataset(test_encodings, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "# fine tune bert and train model\n",
    "training_args = None\n",
    "\n",
    "trainer = None\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "# freeze bert weights and then train model_2\n",
    "training_args = None\n",
    "\n",
    "trainer = None\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     <br>\n",
    "حال داده‌ی test را به هر دو مدل بدهید و بر اساس خروجی‌ها موارد زیر را به دست آورید.\n",
    "    <br>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>accuracy</li>\n",
    "        <li>F1-Macro</li>\n",
    "        <li>F1-Micro</li>\n",
    "        <li>Confusion matrix</li>\n",
    "        <li>Macro-average precision</li>\n",
    "        <li>Macro-average recall</li>\n",
    "        <li>Micro-average precision</li>\n",
    "        <li>Micro-average recall</li>\n",
    "    </ul>\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can add more cells here if you need\n",
    "# todo\n",
    "y_pred_transformers = []\n",
    "y_pred_transformers_2 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     <br>\n",
    "     نتایج به دست آمده برای دو مدل آموزش داده شده را مقایسه کنید و سپس نتایج مدل بهتر را با خروجی‌های fasttext که در قسمت قبل استخراج کردید نیز مقایسه کنید و تحلیل‌های خود را بنویسید.\n",
    "    <br>\n",
    "    <br>\n",
    "    </font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=3>\n",
    "پاسخ خود را در این قسمت بنویسید\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=3>\n",
    "     <br>\n",
    "در نهایت در این قسمت تابعی بنویسید که یک متن را به عنوان ورودی بگیرد و بر اساس مدلتان برچسب‌ مناسب را به آن بدهد.\n",
    "    <br>\n",
    "    <br>\n",
    "    </font>\n",
    "</div>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, model):\n",
    "    \"\"\"\n",
    "    predict the class code of a given query\n",
    "    :param x: the query\n",
    "    :return: the class of the query in the form of real strings.\n",
    "    \"\"\"\n",
    "    # todo\n",
    "    predicted_class = None\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('hardware is very good.', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is highly recommended to save your final models\n",
    "# todo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <h2>\n",
    "    <b>جستجو و بازیابی اسناد در دسته‌های مختلف</b>\n",
    "    </h2>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=4>\n",
    "در این بخش می‌خواهیم به تابع search مربوط به فاز قبل قابلیتی اضافه کنیم که بر اساس آن بتوان جستجو و بازیابی پرسمان را به یک دسته خاص محدود کرد و یا مشخص کرد که جستجو بر روی تمام اسناد انجام شود. همانند فاز قبل:\n",
    "<br>\n",
    " پرسمانی که از کاربر می‌گیرید را در مجموعه اسناد نمایه شده جست و جو کنید. توجه داشته باشید که جست و جویی که انجام می‌دهید هم باید در عنوان مقاله و هم در چکیده آن انجام شود. در نهایت، اسناد باید به ترتیب امتیاز نهایی‌شان برگردانده شوند. امتیاز نهایی هر سند نیز از جمع وزن‌دار امتیاز جستجو در عنوان و جستجو در چکیده مقاله به دست می‌آید.\n",
    "<br>\n",
    "ورودی‌های تابع search در این فاز، همانند فاز قبل است؛ تنها ورودی category به آن اضافه شده است. در صورتی که ورودی category برابر all باشد، جستجو در تمامی اسناد انجام می‌شود. در صورتی که category برابر نام یک دسته باشد، جستجو تنها در دسته گفته شده انجام می‌شود.\n",
    "<br>\n",
    "(برای این قسمت از کدهایی که در فاز قبل زده‌اید استفاده کنید و قابلیت گفته شده را به آن اضافه کنید.)\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(title_query: str, abstract_query: str, max_result_count: int, method: str = 'ltn-lnn',\\\n",
    "           weight: float = 0.5, print=False, category = 'all'):\n",
    "    \"\"\"\n",
    "        Finds relevant documents to query\n",
    "        \n",
    "        Parameters\n",
    "        ---------------------------------------------------------------------------------------------------\n",
    "        max_result_count: Return top 'max_result_count' docs which have the highest scores. \n",
    "                          notice that if max_result_count = -1, then you have to return all docs\n",
    "        \n",
    "        mode: 'detailed' for searching in title and text separately.\n",
    "              'overall' for all words, and weighted by where the word apears on.\n",
    "        \n",
    "        method: 'ltn-lnn' or 'ltc-lnc' or 'okapi25'\n",
    "\n",
    "        category: 'all' for searching in all documents. \n",
    "                  'category_name' for searching in a specific category with category_name label.\n",
    "\n",
    "        Returns\n",
    "        ----------------------------------------------------------------------------------------------------\n",
    "        list\n",
    "        Retreived documents with snippet\n",
    "    \"\"\"\n",
    "    # TODO: retun top 'max_result_count' documents for your searched query"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=6>\n",
    "    <h1>\n",
    "    <b>خوشه‌بندی (۴۰ نمره)</b>\n",
    "    </h1>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در این قسمت شما به خوشه‌بندی اسناد و استفاده از آن خوشه‌ها برای اهداف مختلف میپردازید. برای این منظور، ابتدا برای هر سند، بردار جاسازی (Embedding) تولید کرده و با استفاده از دو الگوریتم kmeans و خوشه‌بندی سلسله‌مراتبی (Hierarchical Clustering)، خوشه‌ها را ایجاد می‌کنید.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <h2>\n",
    "    <b>استخراج بردار جاسازی اسناد</b>\n",
    "    </h2>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "اگرچه امکان ایجاد بردار جاسازی برای هر سند با استفاده از روش ‌Bag of Words نیز وجود دارد، اما برای اینکه احتمالا بتوانید به نتابج بهتری در خوشه‌بندی برسید، در این قسمت از مدل‌های زبانی برمبنای مبدل‌ها (Transformers) برای استخراج این بردارها استفاده می‌کنید.\n",
    "برای این منظور می‌توانید از\n",
    "<a href=\"https://huggingface.co/\">HuggingFace</a>\n",
    "  و مدل‌های متعددی که در آن وجود دارد استفاده کنید.\n",
    "  <br>\n",
    "  در تابع extract_embedding پیاده‌سازی مورد نیاز را انجام دهید. این تابع لیستی از اسناد را به عنوان ورودی دریافت میکند و در خروجی لیستی از بردار‌های جاسازی متناظر با هر عنصر در ورودی را برمی‌گرداند.\n",
    "  <br>\n",
    "برای محاسبه بردار جاسازی هر سند، روش های مختلفی وجود دارد. استفاده از بردار جاسازی توکن CLS در مدل ‌BERT، میانگین‌گیری (وزن‌دار یا بدون وزن) از بردارهای جاسازی هر کلمه ورودی و ... از روش‌های موجود هستند که می‌توانید به دلخواه خود انتخاب کنید.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ؟ points\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def extract_embedding(doc_list : List):\n",
    "    \"\"\"Extracts embedding vector for each document in doc_list\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_list : List\n",
    "        A list of documents\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of embedding vectors\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Extract a vector for each input document using a transformer-based language model \n",
    "    pass\n",
    "docs_embedding = extract_embedding([str, str, str, ...]) # return [[NUM, NUM, NUM, ...], [NUM, NUM, NUM, ...], [NUM, NUM, NUM, ...]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <h2>\n",
    "    <b>کاهش بعد بردارها برای رسم نمودار دوبعدی</b>\n",
    "    </h2>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "برای اینکه بتوانید در مراحل بعدی، نتایج خوشه‌بندی را مشاهده کنید، در این قسمت به پیاده‌سازی تابع کاهش بعد بردارهای جاسازی با استفاده از روش T-SNE می‌پردازید.\n",
    "برای اینکار تابع convert_to_2d_tsne را پیاده‌سازی می‌کنید که لیستی از بردارهای جاسازی را به عنوان ورودی دریافت می‌کند و در خروجی، لیستی از بردارهای جاسازی کاهش بعد داده شده به دو بعد را تولید می‌کند. برای پیاده سازی این تابع می‌توانید از کتابخانه‌های آماده استفاده کنید.\n",
    "<br>\n",
    "توجه کنید که از بردارهای خروجی این قسمت <u>صرفا برای رسم نمودار</u> استفاده می‌کنید و تمامی مراحلی که در ادامه طی می‌کنید (به جز رسم نمودار)، باید با استفاده از بردارهای کاهش بعد داده <u>نشده</u> انجام شوند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ؟ points\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def convert_to_2d_tsne(emb_vecs : List):\n",
    "    \"\"\"Converts each raw embedding vector to 2d vector \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    emb_vecs : List\n",
    "        A list of vectors\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of 2d vectors\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Convert each input vector to 2d vector \n",
    "    pass\n",
    "docs_embedding_2d = convert_to_2d_tsne([[NUM, NUM, NUM, ...], [NUM, NUM, NUM, ...], [NUM, NUM, NUM, ...]]) # return [[NUM, NUM], [NUM, NUM], [NUM, NUM], ...]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <h2>\n",
    "    <b>رسم نمودار</b>\n",
    "    </h2>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در این قسمت، تابع plot_docs را پیاده‌سازی می‌کنید که لیستی از بردارهای کاهش بعد داده شده و لیستی از شماره‌ خوشه‌‌های هربردار (برای رنگ‌آمیزی نقاط) را به عنوان ورودی دریافت کرده و نمودار دوبعدی ای را رسم می‌کند که در آن به هر خوشه یک رنگ مجزا اختصاص داده شده است و هر سند به عنوان یک نقطه نشان داده می‌شود که به رنگ خوشه‌ای است که به آن نسبت داده شده است. \n",
    "</font>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ؟ points\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def plot_docs(emb_vecs_2d: List, labels : List):\n",
    "    \"\"\"Draws a 2d plot of input vectors\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    emb_2d_vecs : List\n",
    "        A list of 2d vectors\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Draw a 2d plot from input vectors and color each cluster with specific color.\n",
    "    pass\n",
    "\n",
    "plot_docs([[NUM, NUM], [NUM, NUM], [NUM, NUM], ...])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <h2>\n",
    "    <b>خوشه‌بندی مستندها</b>\n",
    "    </h2>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در این قسمت شما به خوشه‌بندی اسناد با استفاده از بردارهای جاسازی مستخرج از مدل زبانی با دو روش Kmeans و خوشه‌بندی سلسله‌مراتبی می‌پردازید.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h3>\n",
    "    <b>روش K-means</b>\n",
    "    </h3>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در این قسمت، شما ابتدا الگوریتم خوشه‌بندی K-means را \n",
    "<u><b>از پایه</b></u>\n",
    " پیاده‌سازی کرده و سپس با استفاده از آن، خوشه‌های اسناد را ایجاد می‌کنید. الگوریتم را با استفاده از چند مقدار مختلف تعداد خوشه‌ها (k) اجرا کنید. در هربار اجرا، با استفاده از تعدادی از اسناد موجود در هر خوشه، موضوع آن خوشه را تعیین کرده و خوشه‌بندی حاصله را با استفاده از بردار‌های دوبعدی قسمت قبل، رسم کنید. با اینکار، پیاده‌سازی خود و همچنین کارایی این الگوریتم در خوشه‌بندی اسناد و قرار دادن اسناد مشابه در خوشه‌های یکسان را بررسی کنید.\n",
    "<br>\n",
    " نمودار silhouette score برای مقدار‌های محتلف k را رسم کرده و silhouette analysis برای انتخاب k مناسب انجام دهید. \n",
    " همچنین با استفاده از داده‌های دارای برچسب، مقدار purity به ازای k را رسم کرده و مقدار purity برای k نهایی را گزارش کنید.\n",
    "<br>\n",
    "پیاده‌سازی خود را در تابع cluster_kmeans قرار دهید. این تابع یک لیست از بردارهای جاسازی دریافت کرده و در خروجی، مختصات مرکز هر خوشه را به همراه لیستی از شماره خوشه‌های متناظر با هر بردار جاسازی تولید می‌کند.\n",
    "توجه کنید که الگوریتم می‌بایست از پایه پیاده‌سازی شود و امکان استفاده از پیاده‌سازی‌های آماده آن وجود ندارد.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ؟ points\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def cluster_kmeans(emb_vecs : List, n_clusters : int):\n",
    "    \"\"\"Clusters input vectors using K-means method\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    emb_vecs : List\n",
    "        A list of vectors\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Two lists: 1) A list containing cluster centers 2) A list containing cluster index for each input vector\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement K-means method \n",
    "    pass\n",
    "\n",
    "cluster_centers, cluster_kmeans = cluster_kmeans([[NUM, NUM, NUM, ...], [NUM, NUM, NUM, ...], [NUM, NUM, NUM, ...]]) # return [NUM, NUM, NUM, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "#TODO: silhouette analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: plot purity for different value of k"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=4>\n",
    "    <h3>\n",
    "    <b>روش خوشه‌بندی سلسله‌مراتبی (Hierarchical Clustering)</b>\n",
    "    </h3>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "تکنیک خوشه‌بندی سلسله مراتبی یکی از تکنیک‌های خوشه‌بندی در یادگیری ماشین است. در این قسمت شما می‌توانید از لایببری scipy یا هر لایبرری دیگری در پایتون استفاده کنید تا داده‌ها را به صورت سلسله‌مراتبی خوشه‌بندی کنید. سپس می‌توانید خوشه‌ها را با matplotlib مشاهده کنید.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as shc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <h2>\n",
    "    <b>خوشه‌بندی خروجی‌های سیستم جستجو فاز ۱</b>\n",
    "    </h2>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در این قسمت شما به افزودن قابلیت خوشه‌بندی در سیستم جستجویی که در فاز ۱ درس توسعه داده‌اید می‌پردازید. برای این منظور، توابعی که تابحال پیاده‌سازی کرده‌اید را به نحوی به سامانه جستجوی فاز ۱ خود اضافه می‌کنید که اسناد خروجی سامانه را خوشه‌بندی کرده و در نمایش خروجی،‌ اسنادی که در یک خوشه هستند را مشخص کند.\n",
    "می‌توانید با فراخوانی توابعی که در فاز ۱ پیاده‌سازی کرده‌اید و ایجاد تغییر در آن‌ها در سلول زیر، این‌کار را انجام دهید. نحوه پیاده‌سازی این بخش به عهده خودتان است.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <h2>\n",
    "    <b>جست‌و‌جوی اسناد مشابه با یک سند</b>\n",
    "    </h2>\n",
    "    <p></p>\n",
    "    <p></p>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "در این قسمت، شما به پیاده‌سازی تابع find_similar_docs می‌پردازید که در آن با دریافت اندیس یک سند در مجموعه دادگان، تعداد اسناد مشابه مورد نظر و لیست بردارهای جاسازی، اندیس اسناد مشابه با آن در مجموعه دادگان را در خروجی تولید می‌کند. برای این منظور، به تعداد num_of_similar_docs تا از نزدیک‌ترین بردارها را به بردار مدنظر پیدا کرده و اندیس آن‌ها را به عنوان خروجی برمیگردانید. اسناد مشابه باید به ترتیب شباهت (مقدار شباهت بردار جاسازی آن‌ها به بردار جاسازی سند ورودی) مرتب شده باشند. انتخاب معیار شباهت برعهده خودتان است.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ؟ points\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def find_similar_docs(input_doc_index : int,\n",
    "                      num_of_similar_docs: int,\n",
    "                      emb_vecs : List):\n",
    "    \"\"\"Finds similar documents to input in dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_doc_index: int\n",
    "        Index of input document vector in emb_vecs list to search for specific paper\n",
    "\n",
    "    num_of_similar_docs:\n",
    "        Number of similar documents to return \n",
    "\n",
    "    emb_vecs : List\n",
    "        A list of vectors corresponding to documents\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List\n",
    "        A list of similar document indexes to input document\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement a function to find similar documents to input document.\n",
    "    pass\n",
    "\n",
    "find_similar_docs(int, int, [[NUM, NUM, NUM, ...], [NUM, NUM, NUM, ...], [NUM, NUM, NUM, ...]])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
